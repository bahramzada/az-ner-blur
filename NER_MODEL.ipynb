{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bahramzada/az-ner-blur/blob/main/NER_MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at-GfBXHA3hk",
        "outputId": "1e078778-83ac-405c-9787-1f229f0b4ce6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NÃ¼munÉ™ cÃ¼mlÉ™lÉ™r:\n",
            "1. Bu gÃ¼n 72-MY-215 nÃ¶mrÉ™sini qeyd etdim.\n",
            "2. NÉ™qliyyatÄ±n hÉ™rÉ™kÉ™tini izlÉ™mÉ™k Ã¼Ã§Ã¼n 58-OH-587 nÃ¶mrÉ™sindÉ™n istifadÉ™ edildi.\n",
            "3. Avtomobil 68-CB-083 nÃ¶mrÉ™si qara maÅŸÄ±nda yazÄ±lÄ±b.\n",
            "4. 55-RO-777 nÃ¶mrÉ™li avtomobilin sahibi kimdir?\n",
            "5. TÉ™hlÃ¼kÉ™sizlik É™mÉ™kdaÅŸÄ± 21-NJ-764 nÃ¶mrÉ™sini soruÅŸdu.\n",
            "6. Bu gÃ¼n 16-GV-210 nÃ¶mrÉ™sini qeyd etdim.\n",
            "7. 42-ZP-581 nÃ¶mrÉ™li maÅŸÄ±n Ã§ox sÃ¼rÉ™tlÉ™ gedirdi.\n",
            "8. 31-GX-041 nÃ¶mrÉ™li maÅŸÄ±n Ã§ox bÃ¶yÃ¼kdÃ¼r.\n",
            "9. Avtomobil nÃ¶mrÉ™si 77-OG-934 qeydiyyatdan keÃ§di.\n",
            "10. TÉ™dbirdÉ™ iÅŸtirak edÉ™n hÉ™r bir avtomobilin nÃ¶mrÉ™si, o cÃ¼mlÉ™dÉ™n 46-NE-732 qeyd olundu.\n",
            "\n",
            "==================================================\n",
            "CÉ™mi 30000 cÃ¼mlÉ™ yaradÄ±ldÄ±.\n",
            "30000 cÃ¼mlÉ™ car_plate_dataset.txt faylÄ±nda saxlanÄ±ldÄ±.\n",
            "CSV formatÄ±nda da car_plate_dataset.csv faylÄ±nda saxlanÄ±ldÄ±.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import string\n",
        "import csv\n",
        "import re\n",
        "\n",
        "# EN: Rayon (region) codes list for Azerbaijani car plates\n",
        "# AZ: AzÉ™rbaycan avtomobil nÃ¶mrÉ™lÉ™ri Ã¼Ã§Ã¼n rayon kodlarÄ± siyahÄ±sÄ±\n",
        "first_digit = [\n",
        "    \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\",\n",
        "    \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"38\", \"39\",\n",
        "    \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \"55\", \"56\", \"57\", \"58\",\n",
        "    \"59\", \"60\", \"61\", \"62\", \"63\", \"64\", \"65\", \"66\", \"67\", \"68\", \"69\", \"70\", \"71\", \"72\", \"77\", \"85\", \"90\", \"99\"\n",
        "]\n",
        "\n",
        "# EN: Variable for rayon codes used in car plate generation\n",
        "# AZ: Avtomobil nÃ¶mrÉ™si yaratmaq Ã¼Ã§Ã¼n rayon kodlarÄ± dÉ™yiÅŸÉ™ni\n",
        "RAYON_CODES = first_digit\n",
        "\n",
        "def generate_car_plate():\n",
        "    \"\"\"\n",
        "    EN: Generates a car plate in the format RayonCode-XX-XXX (e.g., 01-XX-001)\n",
        "    AZ: Rayon kodu-XX-XXX formatÄ±nda avtomobil nÃ¶mrÉ™si yaradÄ±r (mÉ™sÉ™lÉ™n, 01-XX-001)\n",
        "    \"\"\"\n",
        "    # EN: Select a rayon code\n",
        "    # AZ: Rayon kodunu seÃ§\n",
        "    first_digits = random.choice(RAYON_CODES)\n",
        "\n",
        "    # EN: Generate two random uppercase letters\n",
        "    # AZ: TÉ™sadÃ¼fi iki bÃ¶yÃ¼k hÉ™rf yarat\n",
        "    letters = ''.join(random.choices(string.ascii_uppercase, k=2))\n",
        "\n",
        "    # EN: Generate the last three digits (001â€“999), always three digits\n",
        "    # AZ: Son Ã¼Ã§ rÉ™qÉ™mi (001â€“999), hÉ™miÅŸÉ™ Ã¼Ã§rÉ™qÉ™mli kimi yarat\n",
        "    last_digits = f\"{random.randint(1, 999):03d}\"\n",
        "\n",
        "    # EN: Return the car plate string\n",
        "    # AZ: Avtomobil nÃ¶mrÉ™sini qaytar\n",
        "    return f\"{first_digits}-{letters}-{last_digits}\"\n",
        "\n",
        "def generate_sentences_with_car_plates(num_sentences=1000):\n",
        "    \"\"\"\n",
        "    EN: Generates sentences containing car plates\n",
        "    AZ: Avtomobil nÃ¶mrÉ™lÉ™ri olan cÃ¼mlÉ™lÉ™r yaradÄ±r\n",
        "    \"\"\"\n",
        "    # EN: Various sentence templates for generating sentences with car plates\n",
        "    # AZ: Avtomobil nÃ¶mrÉ™si ilÉ™ cÃ¼mlÉ™ yaratmaq Ã¼Ã§Ã¼n mÃ¼xtÉ™lif ÅŸablonlar\n",
        "    sentence_templates = [\n",
        "        \"Avtomobil nÃ¶mrÉ™si {} olan maÅŸÄ±n yolda gedirdi.\",\n",
        "        \"{} nÃ¶mrÉ™li avtomobil sÃ¼rÉ™tlÉ™ keÃ§di.\",\n",
        "        \"MÉ™n {} nÃ¶mrÉ™sini gÃ¶rdÃ¼m.\",\n",
        "        \"{} nÃ¶mrÉ™li maÅŸÄ±n dayanacaqda idi.\",\n",
        "        \"Polis {} nÃ¶mrÉ™li avtomobili dayandÄ±rdÄ±.\",\n",
        "        \"Bu gÃ¼n {} nÃ¶mrÉ™sini qeyd etdim.\",\n",
        "        \"{} nÃ¶mrÉ™li avtomobil qÄ±rmÄ±zÄ± iÅŸÄ±qda dayandÄ±.\",\n",
        "        \"QonÅŸumun avtomobil nÃ¶mrÉ™si {}dÄ±r.\",\n",
        "        \"{} nÃ¶mrÉ™li maÅŸÄ±n Ã§ox sÃ¼rÉ™tlÉ™ gedirdi.\",\n",
        "        \"Avtomobil {} nÃ¶mrÉ™si ilÉ™ qeydiyyatdan keÃ§ib.\",\n",
        "        \"MÉ™n {} nÃ¶mrÉ™li avtomobili tanÄ±yÄ±ram.\",\n",
        "        \"{} nÃ¶mrÉ™sindÉ™ olan maÅŸÄ±n aÄŸ rÉ™ngdÉ™dir.\",\n",
        "        \"DÃ¼nÉ™n {} nÃ¶mrÉ™li avtomobili gÃ¶rdÃ¼m.\",\n",
        "        \"Bu {} nÃ¶mrÉ™li maÅŸÄ±n kimÉ™ mÉ™xsusdur?\",\n",
        "        \"{} nÃ¶mrÉ™li avtomobil yeni alÄ±nÄ±b.\",\n",
        "        \"Parklama yerindÉ™ {} nÃ¶mrÉ™si var idi.\",\n",
        "        \"{} nÃ¶mrÉ™li maÅŸÄ±n tÉ™mirÉ™ ehtiyacÄ± var.\",\n",
        "        \"Avtomobil nÃ¶mrÉ™si {} olan sÃ¼rÃ¼cÃ¼ tÉ™crÃ¼bÉ™lidir.\",\n",
        "        \"MÉ™nim dostumun avtomobil nÃ¶mrÉ™si {}dÄ±r.\",\n",
        "        \"{} nÃ¶mrÉ™li avtomobil bazarda satÄ±lÄ±r.\",\n",
        "        \"Bu sÉ™hÉ™r {} nÃ¶mrÉ™sini yolda gÃ¶rdÃ¼m.\",\n",
        "        \"{} nÃ¶mrÉ™li maÅŸÄ±n Ã§ox bÃ¶yÃ¼kdÃ¼r.\",\n",
        "        \"Avtomobil {} nÃ¶mrÉ™si qara maÅŸÄ±nda yazÄ±lÄ±b.\",\n",
        "        \"MÉ™n {} nÃ¶mrÉ™sini unutmuÅŸdum.\",\n",
        "        \"{} nÃ¶mrÉ™li avtomobil hÉ™ftÉ™sonu istifadÉ™ olunur.\",\n",
        "        \"Bu {} avtomobil nÃ¶mrÉ™si Ã§ox maraqlÄ±dÄ±r.\",\n",
        "        \"{} nÃ¶mrÉ™li maÅŸÄ±n É™la vÉ™ziyyÉ™tdÉ™dir.\",\n",
        "        \"Avtomobil nÃ¶mrÉ™si {} yaddaÅŸÄ±mda qalÄ±b.\",\n",
        "        \"DÃ¼nÉ™n axÅŸam {} nÃ¶mrÉ™li avtomobil gÉ™ldi.\",\n",
        "        \"{} nÃ¶mrÉ™sini polisÉ™ bildirdim.\",\n",
        "        \"HadisÉ™ yerindÉ™n {} nÃ¶mrÉ™li avtomobil uzaqlaÅŸdÄ±.\",\n",
        "        \"ÅÉ™hÉ™r kameralarÄ± {} nÃ¶mrÉ™li maÅŸÄ±nÄ± qeydÉ™ aldÄ±.\",\n",
        "        \"{} nÃ¶mrÉ™si ilÉ™ icarÉ™yÉ™ gÃ¶tÃ¼rÃ¼lÉ™n avtomobil qaytarÄ±ldÄ±.\",\n",
        "        \"TÉ™hlÃ¼kÉ™sizlik É™mÉ™kdaÅŸÄ± {} nÃ¶mrÉ™sini soruÅŸdu.\",\n",
        "        \"Bu avtomobilin nÃ¶mrÉ™si {} olaraq qeyd edilib.\",\n",
        "        \"Avtomobilin texniki baxÄ±ÅŸÄ± {} nÃ¶mrÉ™sinÉ™ uyÄŸundur.\",\n",
        "        \"{} nÃ¶mrÉ™li maÅŸÄ±n yol kÉ™narÄ±nda saxlanÄ±lÄ±b.\",\n",
        "        \"MÉ™lumat bazasÄ±nda {} nÃ¶mrÉ™si ilÉ™ axtarÄ±ÅŸ aparÄ±ldÄ±.\",\n",
        "        \"O, Ã¶z avtomobilinin nÃ¶mrÉ™sini, {} nÃ¶mrÉ™sini xatÄ±rladÄ±.\",\n",
        "        \"{} nÃ¶mrÉ™li avtomobilin sahibi kimdir?\",\n",
        "        \"KÃ¶mÉ™k Ã¼Ã§Ã¼n {} nÃ¶mrÉ™li maÅŸÄ±n Ã§aÄŸÄ±rÄ±ldÄ±.\",\n",
        "        \"Avtomobil nÃ¶mrÉ™si {} qeydiyyatdan keÃ§di.\",\n",
        "        \"NÉ™qliyyatÄ±n hÉ™rÉ™kÉ™tini izlÉ™mÉ™k Ã¼Ã§Ã¼n {} nÃ¶mrÉ™sindÉ™n istifadÉ™ edildi.\",\n",
        "        \"GÃ¶mrÃ¼kdÉ™ {} nÃ¶mrÉ™li avtomobil yoxlanÄ±ldÄ±.\",\n",
        "        \"Bu qÉ™za ilÉ™ baÄŸlÄ± {} nÃ¶mrÉ™li avtomobilin adÄ± hallanÄ±r.\",\n",
        "        \"{} nÃ¶mrÉ™li maÅŸÄ±nÄ±n tÉ™kÉ™rlÉ™ri dÉ™yiÅŸdirildi.\",\n",
        "        \"Avtomobilin nÃ¶mrÉ™si {} vÉ™ rÉ™ngi aÄŸdÄ±r.\",\n",
        "        \"Bu avtomobilin nÃ¶mrÉ™si {} olaraq dÉ™yiÅŸdirilÉ™cÉ™k.\",\n",
        "        \"TÉ™dbirdÉ™ iÅŸtirak edÉ™n hÉ™r bir avtomobilin nÃ¶mrÉ™si, o cÃ¼mlÉ™dÉ™n {} qeyd olundu.\"\n",
        "    ]\n",
        "    sentences = []\n",
        "\n",
        "    # EN: Generate the requested number of sentences\n",
        "    # AZ: Ä°stÉ™nilÉ™n sayda cÃ¼mlÉ™ yarat\n",
        "    for _ in range(num_sentences):\n",
        "        # EN: Select a random template\n",
        "        # AZ: TÉ™sadÃ¼fi ÅŸablon seÃ§\n",
        "        template = random.choice(sentence_templates)\n",
        "        # EN: Generate car plate\n",
        "        # AZ: Avtomobil nÃ¶mrÉ™sini yarat\n",
        "        car_plate = generate_car_plate()\n",
        "        # EN: Insert car plate into sentence template\n",
        "        # AZ: Avtomobil nÃ¶mrÉ™sini cÃ¼mlÉ™ ÅŸablonuna yerlÉ™ÅŸdir\n",
        "        sentence = template.format(car_plate)\n",
        "        sentences.append(sentence)\n",
        "\n",
        "    return sentences\n",
        "\n",
        "def save_dataset(sentences, filename=\"car_plate_dataset.txt\"):\n",
        "    \"\"\"\n",
        "    EN: Saves the dataset to a text file\n",
        "    AZ: Dataset-i mÉ™tn faylÄ±na saxlayÄ±r\n",
        "    \"\"\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        for sentence in sentences:\n",
        "            f.write(sentence + '\\n')\n",
        "    # EN: Print how many sentences were saved\n",
        "    # AZ: NeÃ§É™ cÃ¼mlÉ™ saxlanÄ±ldÄ±ÄŸÄ±nÄ± Ã§ap et\n",
        "    print(f\"{len(sentences)} cÃ¼mlÉ™ {filename} faylÄ±nda saxlanÄ±ldÄ±.\")\n",
        "\n",
        "def save_dataset_csv(sentences, filename=\"car_plate_dataset.csv\"):\n",
        "    \"\"\"\n",
        "    EN: Saves the dataset to a CSV file (with columns: sentence & car_plate)\n",
        "    AZ: Dataset-i CSV formatÄ±nda saxlayÄ±r (sÃ¼tunlar: cÃ¼mlÉ™ & avtomobil nÃ¶mrÉ™si)\n",
        "    \"\"\"\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['sentence', 'car_plate'])\n",
        "\n",
        "        for sentence in sentences:\n",
        "            # EN: Find car plate in the sentence using regex\n",
        "            # AZ: CÃ¼mlÉ™dÉ™ avtomobil nÃ¶mrÉ™sini regex ilÉ™ tap\n",
        "            car_plate_pattern = r'\\d{2}-[A-Z]{2}-\\d{3}'\n",
        "            match = re.search(car_plate_pattern, sentence)\n",
        "            if match:\n",
        "                car_plate = match.group()\n",
        "                writer.writerow([sentence, car_plate])\n",
        "\n",
        "    # EN: Print confirmation about CSV saving\n",
        "    # AZ: CSV saxlanÄ±lmasÄ± barÉ™dÉ™ tÉ™sdiq Ã§ap et\n",
        "    print(f\"CSV formatÄ±nda da {filename} faylÄ±nda saxlanÄ±ldÄ±.\")\n",
        "\n",
        "def main():\n",
        "    # EN: Generate 30,000 sentences with car plates\n",
        "    # AZ: 30,000 avtomobil nÃ¶mrÉ™li cÃ¼mlÉ™ yarat\n",
        "    sentences = generate_sentences_with_car_plates(30000)\n",
        "\n",
        "    # EN: Display the first 10 sample sentences\n",
        "    # AZ: Ä°lk 10 nÃ¼munÉ™ cÃ¼mlÉ™ni gÃ¶stÉ™r\n",
        "    print(\"NÃ¼munÉ™ cÃ¼mlÉ™lÉ™r:\")\n",
        "    for i, sentence in enumerate(sentences[:10], 1):\n",
        "        print(f\"{i}. {sentence}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"CÉ™mi {len(sentences)} cÃ¼mlÉ™ yaradÄ±ldÄ±.\")\n",
        "\n",
        "    # EN: Save sentences to text file\n",
        "    # AZ: CÃ¼mlÉ™lÉ™ri mÉ™tn faylÄ±na saxla\n",
        "    save_dataset(sentences)\n",
        "\n",
        "    # EN: Save sentences also as CSV\n",
        "    # AZ: CÃ¼mlÉ™lÉ™ri hÉ™m dÉ™ CSV formatÄ±nda saxla\n",
        "    save_dataset_csv(sentences)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2EDZy3ZA-qp",
        "outputId": "b5bcda17-d271-4cbf-8313-d9eca16414a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV fayl adÄ±nÄ± daxil edin (mÉ™s: sentences.csv): \n",
            "30000 cÃ¼mlÉ™ annotate edildi vÉ™ /content/car_plate_dataset_annotated.json faylÄ±nda saxlanÄ±ldÄ±.\n",
            "\n",
            "NÃ¼munÉ™ 5 annotated cÃ¼mlÉ™:\n",
            "================================================================================\n",
            "1. Bu gÃ¼n 72-MY-215 nÃ¶mrÉ™sini qeyd etdim.\n",
            "   -> TapÄ±lan: '72-MY-215' (mÃ¶vqe: 7-16)\n",
            "----------------------------------------\n",
            "2. NÉ™qliyyatÄ±n hÉ™rÉ™kÉ™tini izlÉ™mÉ™k Ã¼Ã§Ã¼n 58-OH-587 nÃ¶mrÉ™sindÉ™n istifadÉ™ edildi.\n",
            "   -> TapÄ±lan: '58-OH-587' (mÃ¶vqe: 36-45)\n",
            "----------------------------------------\n",
            "3. Avtomobil 68-CB-083 nÃ¶mrÉ™si qara maÅŸÄ±nda yazÄ±lÄ±b.\n",
            "   -> TapÄ±lan: '68-CB-083' (mÃ¶vqe: 10-19)\n",
            "----------------------------------------\n",
            "4. 55-RO-777 nÃ¶mrÉ™li avtomobilin sahibi kimdir?\n",
            "   -> TapÄ±lan: '55-RO-777' (mÃ¶vqe: 0-9)\n",
            "----------------------------------------\n",
            "5. TÉ™hlÃ¼kÉ™sizlik É™mÉ™kdaÅŸÄ± 21-NJ-764 nÃ¶mrÉ™sini soruÅŸdu.\n",
            "   -> TapÄ±lan: '21-NJ-764' (mÃ¶vqe: 23-32)\n",
            "----------------------------------------\n",
            "\n",
            "âœ… UÄŸurla tamamlandÄ±!\n",
            "ğŸ“ Input: /content/car_plate_dataset.csv\n",
            "ğŸ“ Output: /content/car_plate_dataset_annotated.json\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import json\n",
        "import re\n",
        "\n",
        "def find_car_plate_positions(text):\n",
        "    \"\"\"\n",
        "    EN: Finds the positions of car plates in the text\n",
        "    AZ: MÉ™tndÉ™ avtomobil nÃ¶mrÉ™lÉ™rinin mÃ¶vqelÉ™rini tapÄ±r\n",
        "    \"\"\"\n",
        "    car_plate_pattern = r'\\d{2}-[A-Z]{2}-\\d{3}'\n",
        "    entities = []\n",
        "\n",
        "    # EN: Look for all matches of the car plate pattern in the text\n",
        "    # AZ: MÉ™tndÉ™ avtomobil nÃ¶mrÉ™si ÅŸablonuna uyÄŸun bÃ¼tÃ¼n uyÄŸunluqlarÄ± tap\n",
        "    for match in re.finditer(car_plate_pattern, text):\n",
        "        start_pos = match.start()\n",
        "        end_pos = match.end()\n",
        "        # EN: Add the entity as [start, end, \"CAR_PLATE\"]\n",
        "        # AZ: Entitiyi [start, end, \"CAR_PLATE\"] formatÄ±nda É™lavÉ™ et\n",
        "        entities.append([start_pos, end_pos, \"CAR_PLATE\"])\n",
        "\n",
        "    return entities\n",
        "\n",
        "def annotate_csv_file(input_csv_file, output_json_file):\n",
        "    \"\"\"\n",
        "    EN: Reads a CSV file and annotates it in JSON format\n",
        "    AZ: CSV faylÄ±nÄ± oxuyub JSON formatÄ±nda annotate edir\n",
        "    \"\"\"\n",
        "    annotated_data = []\n",
        "\n",
        "    # EN: Read the CSV file\n",
        "    # AZ: CSV faylÄ±nÄ± oxu\n",
        "    with open(input_csv_file, 'r', encoding='utf-8') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "\n",
        "        # EN: Skip header if present, but also process if it's not a header\n",
        "        # AZ: Header-i atla (É™gÉ™r varsa), amma header deyilsÉ™, onu da emal et\n",
        "        try:\n",
        "            first_row = next(reader)\n",
        "            # EN: If the first row is not a header, process it as a sentence\n",
        "            # AZ: ÆgÉ™r ilk sÉ™tir header deyilsÉ™, onu da cÃ¼mlÉ™ kimi emal et\n",
        "            if not (first_row[0].lower() in ['sentence', 'text', 'cÃ¼mle']):\n",
        "                sentence = first_row[0]\n",
        "                entities = find_car_plate_positions(sentence)\n",
        "                annotation = {\"entities\": entities}\n",
        "                annotated_data.append([sentence, annotation])\n",
        "        except StopIteration:\n",
        "            print(\"CSV faylÄ± boÅŸdur!\")\n",
        "            return\n",
        "\n",
        "        # EN: Process the remaining rows\n",
        "        # AZ: Qalan sÉ™tirlÉ™ri emal et\n",
        "        for row in reader:\n",
        "            if row:  # EN: Skip empty rows | AZ: BoÅŸ sÉ™tirlÉ™ri atla\n",
        "                sentence = row[0]  # EN: Take the first column as sentence | AZ: Ä°lk sÃ¼tunu cÃ¼mlÉ™ kimi gÃ¶tÃ¼r\n",
        "                entities = find_car_plate_positions(sentence)\n",
        "                annotation = {\"entities\": entities}\n",
        "                annotated_data.append([sentence, annotation])\n",
        "\n",
        "    # EN: Save the annotated data as JSON\n",
        "    # AZ: Annotate edilmiÅŸ mÉ™lumatÄ± JSON faylÄ±nda saxla\n",
        "    with open(output_json_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(annotated_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"{len(annotated_data)} cÃ¼mlÉ™ annotate edildi vÉ™ {output_json_file} faylÄ±nda saxlanÄ±ldÄ±.\")\n",
        "\n",
        "    return annotated_data\n",
        "\n",
        "def print_sample_results(data, num_samples=5):\n",
        "    \"\"\"\n",
        "    EN: Print sample annotated results\n",
        "    AZ: NÃ¼munÉ™ annotate edilmiÅŸ nÉ™ticÉ™lÉ™ri gÃ¶stÉ™r\n",
        "    \"\"\"\n",
        "    print(f\"\\nNÃ¼munÉ™ {min(num_samples, len(data))} annotated cÃ¼mlÉ™:\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for i, (sentence, annotation) in enumerate(data[:num_samples]):\n",
        "        print(f\"{i+1}. {sentence}\")\n",
        "        if annotation['entities']:\n",
        "            for entity in annotation['entities']:\n",
        "                start, end, label = entity\n",
        "                car_plate = sentence[start:end]\n",
        "                # EN: Print found car plate and its position\n",
        "                # AZ: TapÄ±lan avtomobil nÃ¶mrÉ™sini vÉ™ mÃ¶vqeyini gÃ¶stÉ™r\n",
        "                print(f\"   -> TapÄ±lan: '{car_plate}' (mÃ¶vqe: {start}-{end})\")\n",
        "        else:\n",
        "            print(\"   -> Avtomobil nÃ¶mrÉ™si tapÄ±lmadÄ±\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "def main():\n",
        "    # EN: Ask for CSV file name from user\n",
        "    # AZ: Ä°stifadÉ™Ã§idÉ™n CSV fayl adÄ±nÄ± soruÅŸ\n",
        "    input_csv = input(\"CSV fayl adÄ±nÄ± daxil edin (mÉ™s: sentences.csv): \").strip()\n",
        "    if not input_csv:\n",
        "        input_csv = \"/content/car_plate_dataset.csv\"\n",
        "\n",
        "    # EN: Set the output JSON file name\n",
        "    # AZ: Output JSON fayl adÄ±nÄ± tÉ™yin et\n",
        "    output_json = input_csv.replace('.csv', '_annotated.json')\n",
        "\n",
        "    try:\n",
        "        # EN: Annotate the CSV file\n",
        "        # AZ: CSV-ni annotate et\n",
        "        data = annotate_csv_file(input_csv, output_json)\n",
        "\n",
        "        # EN: Print sample results\n",
        "        # AZ: NÃ¼munÉ™ nÉ™ticÉ™lÉ™ri gÃ¶stÉ™r\n",
        "        print_sample_results(data)\n",
        "\n",
        "        print(f\"\\nâœ… UÄŸurla tamamlandÄ±!\")\n",
        "        print(f\"ğŸ“ Input: {input_csv}\")\n",
        "        print(f\"ğŸ“ Output: {output_json}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ XÆTA: '{input_csv}' faylÄ± tapÄ±lmadÄ±!\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ XÆTA: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtfB9WCPBJvK"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def random_aze_id():\n",
        "    \"\"\"\n",
        "    EN: Generates a random Azerbaijani ID in the format AZE + 9 digits\n",
        "    AZ: TÉ™sadÃ¼fi AzÉ™rbaycan ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™si nÃ¶mrÉ™si yaradÄ±r (AZE + 9 rÉ™qÉ™m)\n",
        "    \"\"\"\n",
        "    digits = ''.join([str(random.randint(0, 9)) for _ in range(9)])\n",
        "    return \"AZE\" + digits\n",
        "\n",
        "def random_aa_id():\n",
        "    \"\"\"\n",
        "    EN: Generates a random AA ID in the format AA + 7 digits\n",
        "    AZ: TÉ™sadÃ¼fi AA ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™si nÃ¶mrÉ™si yaradÄ±r (AA + 7 rÉ™qÉ™m)\n",
        "    \"\"\"\n",
        "    digits = ''.join([str(random.randint(0, 9)) for _ in range(7)])\n",
        "    return \"AA\" + digits\n",
        "\n",
        "# EN: Sentence templates with ID placeholders\n",
        "# AZ: ÅÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si Ã¼Ã§Ã¼n cÃ¼mlÉ™ ÅŸablonlarÄ±\n",
        "templates = [\n",
        "    \"MÉ™nim ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™m {}-dir.\",\n",
        "    \"ÅÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si {}-dir.\",\n",
        "    \"VÉ™siqÉ™ nÃ¶mrÉ™m {}-dir.\",\n",
        "    \"ZÉ™hmÉ™t olmasa, ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™nizi qeyd edin: {}\",\n",
        "    \"AdÄ±: Bahram Zada, VÉ™siqÉ™ nÃ¶mrÉ™si: {}\",\n",
        "    \"ÅÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si: {}\",\n",
        "    \"MÉ™nim {} ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™mdir.\",\n",
        "    \"ÅÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™sini yoxlamaq Ã¼Ã§Ã¼n {} daxil edin.\",\n",
        "    \"VÉ™siqÉ™ nÃ¶mrÉ™si olmadan qeydiyyat mÃ¼mkÃ¼n deyil: {}\",\n",
        "    \"Sizin ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™niz {}-dÃ¼r?\",\n",
        "    \"Qeydiyyat Ã¼Ã§Ã¼n ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si tÉ™lÉ™b olunur: {}\",\n",
        "    \"Ä°stifadÉ™Ã§inin ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si: {}\",\n",
        "    \"AÅŸaÄŸÄ±da gÃ¶stÉ™rilÉ™n vÉ™siqÉ™ nÃ¶mrÉ™sini yoxlayÄ±n: {}\",\n",
        "    \"SistemÉ™ giriÅŸ Ã¼Ã§Ã¼n {} vÉ™siqÉ™ nÃ¶mrÉ™sini daxil edin.\",\n",
        "    \"SÉ™nÉ™d mÉ™lumatlarÄ±: ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si - {}\",\n",
        "    \"{} ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™sini tÉ™sdiqlÉ™yin.\",\n",
        "    \"YuxarÄ±da qeyd olunan {} vÉ™siqÉ™ nÃ¶mrÉ™sidir.\",\n",
        "    \"ÆlavÉ™ mÉ™lumat Ã¼Ã§Ã¼n {} ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™sini istifadÉ™ edin.\",\n",
        "    \"ÅÉ™xsiyyÉ™tinizi tÉ™sdiqlÉ™mÉ™k Ã¼Ã§Ã¼n {} nÃ¶mrÉ™sini yazÄ±n.\",\n",
        "    \"ÆgÉ™r ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™niz {}-dirsÉ™, davam edin.\",\n",
        "    \"Qeydiyyat zamanÄ± istifadÉ™ etdiyiniz vÉ™siqÉ™ nÃ¶mrÉ™si: {}\",\n",
        "    \"ÅÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™niz bir daha tÉ™sdiqlÉ™nir: {}\",\n",
        "    \"Formada yazÄ±lan ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si {}-dir.\",\n",
        "    \"SizdÉ™n tÉ™lÉ™b olunan ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si: {}\",\n",
        "    \"AÅŸaÄŸÄ±da gÃ¶stÉ™rilÉ™n {} nÃ¶mrÉ™si sizin vÉ™siqÉ™nizdir.\",\n",
        "    \"ProfilinizdÉ™ qeyd olunan ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si: {}\",\n",
        "    \"ÅÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si olmadan qeydiyyat mÃ¼mkÃ¼n deyil, nÃ¶mrÉ™niz: {}\",\n",
        "    \"SistemÉ™ daxil olmaq Ã¼Ã§Ã¼n ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™nizi {} daxil edin.\",\n",
        "    \"Qeydiyyat formasÄ±nda {} ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™sini yazÄ±n.\",\n",
        "    \"MÉ™lumat bazasÄ±nda saxlanÄ±lan vÉ™siqÉ™ nÃ¶mrÉ™si: {}\",\n",
        "    \"MÃ¼raciÉ™t Ã¼Ã§Ã¼n {} ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si vacibdir.\",\n",
        "    \"YalnÄ±z {} nÃ¶mrÉ™si olan ÅŸÉ™xs xidmÉ™tdÉ™n yararlana bilÉ™r.\",\n",
        "    \"{} ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si ilÉ™ É™laqÉ™li sÉ™nÉ™dlÉ™r qÉ™bul edildi.\",\n",
        "    \"Ã–dÉ™niÅŸi etmÉ™k Ã¼Ã§Ã¼n {} ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™nizi gÃ¶stÉ™rin.\",\n",
        "    \"ÆrizÉ™yÉ™ {} vÉ™siqÉ™ nÃ¶mrÉ™si ilÉ™ mÃ¼raciÉ™t edÉ™ bilÉ™rsiniz.\",\n",
        "    \"HÉ™r hansÄ± bir dÉ™yiÅŸiklik Ã¼Ã§Ã¼n {} nÃ¶mrÉ™si tÉ™lÉ™b olunur.\",\n",
        "    \"{} nÃ¶mrÉ™si yoxlandÄ± vÉ™ tÉ™sdiq edildi.\",\n",
        "    \"Bu hesabÄ±n sahibi {} ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™sinÉ™ malikdir.\",\n",
        "    \"VÉ™siqÉ™nin Ã¼zÉ™rindÉ™ {} nÃ¶mrÉ™si qeyd olunub.\",\n",
        "    \"{} nÃ¶mrÉ™si ilÉ™ baÄŸlÄ± bÃ¼tÃ¼n mÉ™lumatlar doÄŸrudur.\",\n",
        "    \"GiriÅŸ Ã¼Ã§Ã¼n {} ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™sini yenidÉ™n daxil edin.\",\n",
        "    \"LÃ¼tfÉ™n, {} nÃ¶mrÉ™sini É™lavÉ™ edin.\",\n",
        "    \"Bu ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™si nÃ¶mrÉ™si, {} yeni verilmiÅŸdir.\",\n",
        "    \"SÄ±ÄŸorta É™mÉ™liyyatÄ± {} nÃ¶mrÉ™sinÉ™ É™saslanÄ±r.\",\n",
        "    \"TÉ™sdiqlÉ™mÉ™ kodu {} vÉ™siqÉ™ nÃ¶mrÉ™nizÉ™ gÃ¶ndÉ™rildi.\",\n",
        "    \"{} vÉ™siqÉ™ nÃ¶mrÉ™si mÉ™lumat bazasÄ±na daxil edilmiÅŸdir.\",\n",
        "    \"XidmÉ™t haqqÄ±nÄ± Ã¶dÉ™mÉ™k Ã¼Ã§Ã¼n {} nÃ¶mrÉ™si lazÄ±mdÄ±r.\",\n",
        "    \"SistemdÉ™ {} nÃ¶mrÉ™si ilÉ™ baÄŸlÄ± heÃ§ bir qeyd tapÄ±lmadÄ±.\",\n",
        "    \"Ã–dÉ™niÅŸin tÉ™sdiqi Ã¼Ã§Ã¼n {} nÃ¶mrÉ™sini gÃ¶stÉ™rin.\",\n",
        "    \"{} nÃ¶mrÉ™si ilÉ™ baÄŸlÄ± bÃ¼tÃ¼n mÉ™lumatlar qorunur.\"\n",
        "]\n",
        "\n",
        "def generate_sentences(n=1000, aze_ratio=0.7):\n",
        "    \"\"\"\n",
        "    EN: Generates sentences with random ID numbers (AZE or AA format)\n",
        "    AZ: TÉ™sadÃ¼fi ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™lÉ™ri ilÉ™ cÃ¼mlÉ™lÉ™r yaradÄ±r (AZE vÉ™ ya AA formatÄ±)\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    for _ in range(n):\n",
        "        template = random.choice(templates)\n",
        "        # EN: 70% AZE format, 30% AA format\n",
        "        # AZ: 70% AZE formatÄ±, 30% AA formatÄ±\n",
        "        if random.random() < aze_ratio:\n",
        "            id_num = random_aze_id()\n",
        "        else:\n",
        "            id_num = random_aa_id()\n",
        "        sentence = template.format(id_num)\n",
        "        sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # EN: Generate 30,000 sentences and save to file\n",
        "    # AZ: 30,000 cÃ¼mlÉ™ yaradÄ±n vÉ™ fayla yazÄ±n\n",
        "    sentences = generate_sentences(30000)\n",
        "    with open(\"vesiqe_numune.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for sentence in sentences:\n",
        "            f.write(sentence + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCnDDFcfBXub"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "# EN: Regex patterns for ID numbers\n",
        "# AZ: ÅÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™lÉ™ri Ã¼Ã§Ã¼n regex pattern-lÉ™r\n",
        "pattern_aze = r'AZE\\d{9}'         # EN: AZE + 9 digits | AZ: AZE + 9 rÉ™qÉ™m\n",
        "pattern_aa = r'AA\\d{7}'           # EN: AA + 7 digits  | AZ: AA + 7 rÉ™qÉ™m\n",
        "\n",
        "label = \"ID_NUMBER\"  # EN: Entity label for ID numbers | AZ: ÅÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si Ã¼Ã§Ã¼n entiti label\n",
        "\n",
        "def label_sentences(sentences):\n",
        "    \"\"\"\n",
        "    EN: Labels AZE and AA ID numbers in sentences with their positions\n",
        "    AZ: CÃ¼mlÉ™lÉ™rdÉ™ AZE vÉ™ AA ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™lÉ™rinin mÃ¶vqelÉ™rini annotate edir\n",
        "    \"\"\"\n",
        "    labeled_data = []\n",
        "    for sentence in sentences:\n",
        "        entities = []\n",
        "        # EN: For AZE IDs\n",
        "        # AZ: AZE nÃ¶mrÉ™lÉ™ri Ã¼Ã§Ã¼n\n",
        "        for match in re.finditer(pattern_aze, sentence):\n",
        "            start, end = match.start(), match.end()\n",
        "            entities.append((start, end, label))\n",
        "        # EN: For AA IDs\n",
        "        # AZ: AA nÃ¶mrÉ™lÉ™ri Ã¼Ã§Ã¼n\n",
        "        for match in re.finditer(pattern_aa, sentence):\n",
        "            start, end = match.start(), match.end()\n",
        "            entities.append((start, end, label))\n",
        "        # EN: If any entity is found, add to the labeled data\n",
        "        # AZ: ÆgÉ™r entitilÉ™r tapÄ±lÄ±bsa, annotate edilmiÅŸ verilÉ™nlÉ™rÉ™ É™lavÉ™ et\n",
        "        if entities:\n",
        "            labeled_data.append((sentence, {\"entities\": entities}))\n",
        "    return labeled_data\n",
        "\n",
        "# EN: Read sentences from file\n",
        "# AZ: Fayldan cÃ¼mlÉ™lÉ™ri oxu\n",
        "with open(\"vesiqe_numune.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    sentences = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# EN: Label sentences with ID numbers\n",
        "# AZ: CÃ¼mlÉ™lÉ™ri ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™lÉ™ri ilÉ™ annotate et\n",
        "labeled_data = label_sentences(sentences)\n",
        "\n",
        "# EN: Write labeled data to annotation format JSON\n",
        "# AZ: Annotasiya formatlÄ± JSON-a yaz\n",
        "with open(\"vesiqe_annotated.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(labeled_data, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGdSwxQ8Bqog"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def random_fin():\n",
        "    \"\"\"\n",
        "    EN: Generates a random FIN code (7 characters, excluding 'I' and 'O')\n",
        "    AZ: TÉ™sadÃ¼fi FÄ°N kodu yaradÄ±r (7 simvol, 'I' vÉ™ 'O' istisna olmaqla)\n",
        "    \"\"\"\n",
        "    # EN: Allowed letters are from English alphabet except 'I' and 'O'\n",
        "    # AZ: Ä°cazÉ™li hÉ™rflÉ™r Ä°ngilis É™lifbasÄ±ndan, amma 'I' vÉ™ 'O' olmamalÄ±dÄ±r\n",
        "    letters = [chr(c) for c in range(ord('A'), ord('Z')+1) if chr(c) not in ['I', 'O']]\n",
        "    digits = [str(d) for d in range(10)]\n",
        "    allowed = letters + digits\n",
        "    # EN: FIN code is 7 characters, randomly chosen from allowed characters\n",
        "    # AZ: FÄ°N kodu icazÉ™li simvollardan tÉ™sadÃ¼fi seÃ§ilmiÅŸ 7 simvoldan ibarÉ™tdir\n",
        "    fin = ''.join(random.choice(allowed) for _ in range(7))\n",
        "    return fin\n",
        "\n",
        "# EN: Sentence templates with FIN placeholder\n",
        "# AZ: FÄ°N kodu Ã¼Ã§Ã¼n cÃ¼mlÉ™ ÅŸablonlarÄ±\n",
        "fin_templates = [\n",
        "    \"MÉ™nim FÄ°N kodum {}-dur.\",\n",
        "    \"FÄ°N kodu: {}\",\n",
        "    \"ZÉ™hmÉ™t olmasa, FÄ°N kodunuzu daxil edin: {}\",\n",
        "    \"Sizin FÄ°N kodunuz {}-dir?\",\n",
        "    \"Qeydiyyat Ã¼Ã§Ã¼n tÉ™lÉ™b olunan FÄ°N kodu: {}\",\n",
        "    \"Formada yazÄ±lan FÄ°N kodu {}-dir.\",\n",
        "    \"ÆlavÉ™ mÉ™lumat Ã¼Ã§Ã¼n FÄ°N kodu: {}\",\n",
        "    \"ProfilinizdÉ™ qeyd olunan FÄ°N kodu: {}\",\n",
        "    \"FÄ°N kodunuzu yoxlamaq Ã¼Ã§Ã¼n {} daxil edin.\",\n",
        "    \"FÄ°N kodu olmadan qeydiyyat mÃ¼mkÃ¼n deyil: {}\",\n",
        "    \"SistemdÉ™ qeydiyyatdan keÃ§mÉ™k Ã¼Ã§Ã¼n FÄ°N kodunuzu daxil edin: {}\",\n",
        "    \"Sizin ÅŸÉ™xsiyyÉ™tinizi tÉ™sdiqlÉ™yÉ™n FÄ°N kod: {}\",\n",
        "    \"FÄ°N kodunuz bir daha tÉ™sdiqlÉ™nir: {}\",\n",
        "    \"AÅŸaÄŸÄ±da gÃ¶stÉ™rilÉ™n FÄ°N kodunu yoxlayÄ±n: {}\",\n",
        "    \"SÉ™nÉ™d mÉ™lumatlarÄ±: FÄ°N kodu - {}\",\n",
        "    \"Qeydiyyat formasÄ±nda {} FÄ°N kodunu yazÄ±n.\",\n",
        "    \"MÉ™lumat bazasÄ±nda saxlanÄ±lan FÄ°N kodu: {}\",\n",
        "    \"SizdÉ™n tÉ™lÉ™b olunan FÄ°N kodu: {}\",\n",
        "    \"FÄ°N kodu olmadan É™mÉ™liyyat davam etmir: {}\",\n",
        "    \"SistemÉ™ giriÅŸ Ã¼Ã§Ã¼n {} FÄ°N kodunu daxil edin.\",\n",
        "    \"SÉ™nÉ™din Ã¼zÉ™rindÉ™ yazÄ±lmÄ±ÅŸ FÄ°N kodu {}-dir.\",\n",
        "    \"FÄ°N kodunuzu tÉ™sdiqlÉ™yin: {}\",\n",
        "    \"FÄ°N kodu sahÉ™sinÉ™ {} yazÄ±n.\",\n",
        "    \"ÅÉ™xsiyyÉ™ti tÉ™sdiqlÉ™mÉ™k Ã¼Ã§Ã¼n {} FÄ°N kodunu daxil edin.\",\n",
        "    \"Sizin Ã¼Ã§Ã¼n yaradÄ±lmÄ±ÅŸ FÄ°N kodu: {}\",\n",
        "    \"SÉ™nÉ™dÉ™ É™lavÉ™ olunan FÄ°N kodu: {}\",\n",
        "    \"FÄ°N kodu tÉ™lÉ™b olunduqda {} tÉ™qdim edin.\",\n",
        "    \"SistemdÉ™ mÃ¶vcud FÄ°N kodu: {}\",\n",
        "    \"FÄ°N kodunu dÉ™yiÅŸmÉ™k Ã¼Ã§Ã¼n kÃ¶hnÉ™ kod: {}\",\n",
        "    \"ÆgÉ™r FÄ°N kodunuz {}-dirsÉ™, davam edin.\",\n",
        "    \"FÄ°N kodunuzu unutmusunuzsa, yeni kod alÄ±n: {}\",\n",
        "    \"TÉ™sdiqlÉ™nmiÅŸ FÄ°N kodu: {}\",\n",
        "    \"Yeni qeydiyyat Ã¼Ã§Ã¼n FÄ°N kodu: {}\",\n",
        "    \"FÄ°N kodu olmadan qeydiyyat mÃ¼mkÃ¼n deyil, kodunuz: {}\",\n",
        "    \"AÅŸaÄŸÄ±da gÃ¶stÉ™rilÉ™n {} kodu sizin FÄ°N kodunuzdur.\",\n",
        "    \"FÄ°N kodu ilÉ™ baÄŸlÄ± sualÄ±nÄ±z varsa, kod: {}\",\n",
        "    \"MÉ™lumat formasÄ±nda FÄ°N kodu: {}\",\n",
        "    \"SistemÉ™ daxil olmaq Ã¼Ã§Ã¼n FÄ°N kodunuzu {} yazÄ±n.\",\n",
        "    \"FÄ°N kodunuzun dÃ¼zgÃ¼nlÃ¼yÃ¼nÃ¼ yoxlayÄ±n: {}\",\n",
        "    \"ÅÉ™xsiyyÉ™t vÉ™siqÉ™si vÉ™ FÄ°N kodu: {}\",\n",
        "    \"FÄ°N kodu olmadan sÉ™nÉ™d qÉ™bul edilmir: {}\",\n",
        "    \"FÄ°N kodu qutusuna {} yazÄ±n.\",\n",
        "    \"FÄ°N kodu sÉ™hvdirsÉ™, yenisini daxil edin: {}\",\n",
        "    \"HÉ™r hansÄ± É™mÉ™liyyat Ã¼Ã§Ã¼n FÄ°N kodu {} mÃ¼tlÉ™qdir.\",\n",
        "    \"MÉ™lumatlarÄ±nÄ±zÄ±n tÉ™hlÃ¼kÉ™sizliyi Ã¼Ã§Ã¼n FÄ°N kodunuzu {} ilÉ™ tÉ™sdiqlÉ™yin.\",\n",
        "    \"Daxil etdiyiniz FÄ°N kodu {} mÉ™lumatlarÄ±mÄ±zla uyÄŸun gÉ™lmir.\",\n",
        "    \"Bank xidmÉ™tlÉ™rindÉ™n istifadÉ™ Ã¼Ã§Ã¼n FÄ°N kodunuz: {}\",\n",
        "    \"ÅÉ™xsi mÉ™lumatlarÄ±nÄ±zÄ± yoxlamaq Ã¼Ã§Ã¼n {} FÄ°N kodunuzu daxil edin.\",\n",
        "    \"FÄ°N kodu {} ilÉ™ baÄŸlÄ± bÃ¼tÃ¼n sÉ™nÉ™dlÉ™r qÉ™bul edildi.\",\n",
        "    \"Ä°stifadÉ™Ã§inin FÄ°N kodu {} olaraq qeyd olundu.\",\n",
        "    \"Bu sÉ™nÉ™dÉ™ uyÄŸun FÄ°N kodu {}-dir.\",\n",
        "    \"Yeni bank kartÄ± almaq Ã¼Ã§Ã¼n {} FÄ°N kodu tÉ™lÉ™b olunur.\",\n",
        "    \"FÄ°N kodu {} olan ÅŸÉ™xs sistemÉ™ daxil oldu.\",\n",
        "    \"SistemdÉ™ {} FÄ°N kodu ilÉ™ axtarÄ±ÅŸ aparÄ±ldÄ±.\",\n",
        "    \"{} FÄ°N kodu ilÉ™ Ã¶dÉ™niÅŸ uÄŸurla tamamlandÄ±.\",\n",
        "    \"Vergi Ã¶dÉ™niÅŸi Ã¼Ã§Ã¼n FÄ°N kodunuzu {} yazÄ±n.\",\n",
        "    \"MaliyyÉ™ É™mÉ™liyyatlarÄ±nÄ± yoxlamaq Ã¼Ã§Ã¼n {} FÄ°N kodunu tÉ™qdim edin.\",\n",
        "]\n",
        "\n",
        "def generate_fin_sentences(n=500):\n",
        "    \"\"\"\n",
        "    EN: Generates sentences with random FIN codes\n",
        "    AZ: TÉ™sadÃ¼fi FÄ°N kodlarÄ± ilÉ™ cÃ¼mlÉ™lÉ™r yaradÄ±r\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    for _ in range(n):\n",
        "        template = random.choice(fin_templates)\n",
        "        fin_code = random_fin()\n",
        "        sentence = template.format(fin_code)\n",
        "        sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # EN: Generate 30,000 FIN sentences and save to file\n",
        "    # AZ: 30,000 FÄ°N kodlu cÃ¼mlÉ™ yaradÄ±b fayla yazÄ±n\n",
        "    sentences = generate_fin_sentences(30000)\n",
        "    with open(\"fin_numune.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for sentence in sentences:\n",
        "            f.write(sentence + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpU3QhkBCFw2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "# EN: FIN code pattern - 7 characters, uppercase letters (excluding \"I\" and \"O\") and digits\n",
        "# AZ: FÄ°N kodu pattern-i - 7 simvol, bÃ¶yÃ¼k hÉ™rf vÉ™ rÉ™qÉ™m, \"I\" vÉ™ \"O\" istisna\n",
        "# EN: Pattern: [A-HJ-NP-Z0-9]{7} (A-Z, except I and O, and 0-9)\n",
        "# AZ: Pattern: [A-HJ-NP-Z0-9]{7} (A-Z, amma I vÉ™ O olmadan, 0-9)\n",
        "pattern_fin = r'\\b[A-HJ-NP-Z0-9]{7}\\b'\n",
        "label = \"FIN_CODE\"  # EN: Entity label for FIN code | AZ: FÄ°N kod Ã¼Ã§Ã¼n entiti label\n",
        "\n",
        "def label_sentences(sentences):\n",
        "    \"\"\"\n",
        "    EN: Finds and labels FIN codes in sentences with their positions\n",
        "    AZ: CÃ¼mlÉ™lÉ™rdÉ™ FÄ°N kodlarÄ±nÄ±n mÃ¶vqelÉ™rini tapÄ±r vÉ™ annotate edir\n",
        "    \"\"\"\n",
        "    labeled_data = []\n",
        "    for sentence in sentences:\n",
        "        entities = []\n",
        "        # EN: Find all FIN code matches in the sentence\n",
        "        # AZ: CÃ¼mlÉ™dÉ™ bÃ¼tÃ¼n FÄ°N kod uyÄŸunluqlarÄ±nÄ± tap\n",
        "        for match in re.finditer(pattern_fin, sentence):\n",
        "            start, end = match.start(), match.end()\n",
        "            entities.append((start, end, label))\n",
        "        # EN: If any FIN code is found, add to labeled data\n",
        "        # AZ: ÆgÉ™r FÄ°N kod tapÄ±lÄ±bsa, annotate edilmiÅŸ verilÉ™nlÉ™rÉ™ É™lavÉ™ et\n",
        "        if entities:\n",
        "            labeled_data.append((sentence, {\"entities\": entities}))\n",
        "    return labeled_data\n",
        "\n",
        "# EN: Read sentences from file\n",
        "# AZ: Fayldan cÃ¼mlÉ™lÉ™ri oxu\n",
        "with open(\"fin_numune.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    sentences = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# EN: Label sentences with FIN codes\n",
        "# AZ: CÃ¼mlÉ™lÉ™ri FÄ°N kodlarÄ± ilÉ™ annotate et\n",
        "labeled_data = label_sentences(sentences)\n",
        "\n",
        "# EN: Write annotation data to JSON file\n",
        "# AZ: Annotasiya mÉ™lumatÄ±nÄ± JSON-a yaz\n",
        "with open(\"fin_annotated.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(labeled_data, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5S1PMjSB33A"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/sample_data /content/car_plate_dataset.csv /content/car_plate_dataset.txt /content/fin_numune.txt /content/vesiqe_numune.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0G6br3dB30X",
        "outputId": "f48eab11-41fb-43a6-bb25-8da0ff7c12bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… BÃ¼tÃ¼n kitabxanalar yÃ¼klÉ™ndi\n",
            "ğŸ”¥ PyTorch versiyasÄ±: 2.8.0+cu126\n",
            "ğŸš€ CUDA mÃ¶vcuddur: True\n",
            "ğŸ’» Ä°stifadÉ™ edilÉ™cÉ™k device: cuda\n"
          ]
        }
      ],
      "source": [
        "# EN: Imports\n",
        "# AZ: Import-lar\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from typing import List, Tuple, Dict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForTokenClassification\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # EN: Ignore warnings | AZ: Warning-larÄ± gizlÉ™t\n",
        "\n",
        "print(\"âœ… BÃ¼tÃ¼n kitabxanalar yÃ¼klÉ™ndi\")  # EN: All libraries loaded | AZ: BÃ¼tÃ¼n kitabxanalar yÃ¼klÉ™ndi\n",
        "print(f\"ğŸ”¥ PyTorch versiyasÄ±: {torch.__version__}\")  # EN: PyTorch version | AZ: PyTorch versiyasÄ±\n",
        "print(f\"ğŸš€ CUDA mÃ¶vcuddur: {torch.cuda.is_available()}\")  # EN: CUDA available | AZ: CUDA mÃ¶vcuddur\n",
        "\n",
        "# EN: Set device (GPU if available, otherwise CPU)\n",
        "# AZ: Device ayarla (É™gÉ™r GPU varsa, GPU, yoxdursa CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ğŸ’» Ä°stifadÉ™ edilÉ™cÉ™k device: {device}\")  # EN: Device to be used | AZ: Ä°stifadÉ™ edilÉ™cÉ™k device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U67rVqykB3xe",
        "outputId": "1c539176-2ab7-49f6-dd5b-5c2dbe140ab2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“‹ Konfiqurasiya:\n",
            "Model: bert-base-multilingual-cased\n",
            "Max Length: 128\n",
            "Batch Size: 16\n",
            "Epochs: 3\n",
            "Learning Rate: 2e-05\n",
            "Labels: ['O', 'B-PLATE', 'I-PLATE', 'B-FIN', 'I-FIN', 'B-ID', 'I-ID']\n"
          ]
        }
      ],
      "source": [
        "# EN: Model and training configuration\n",
        "# AZ: Model vÉ™ training konfiqurasiyasÄ±\n",
        "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 3\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "# EN: Label system - based on your dataset's entities\n",
        "# AZ: Label sistemi - datasetinizdÉ™n Ã§Ä±xan nÉ™ticÉ™lÉ™rÉ™ gÃ¶rÉ™\n",
        "LABELS = [\n",
        "    'O',           # EN: Outside | AZ: KÉ™nar (etiketlÉ™nmiÅŸ entitiy olmayan hissÉ™)\n",
        "    'B-PLATE',     # EN: Beginning of Car Plate | AZ: Avtomobil nÃ¶mrÉ™sinin baÅŸlanÄŸÄ±cÄ±\n",
        "    'I-PLATE',     # EN: Inside Car Plate | AZ: Avtomobil nÃ¶mrÉ™sinin iÃ§i (davamÄ±)\n",
        "    'B-FIN',       # EN: Beginning of FIN Code | AZ: FÄ°N kodunun baÅŸlanÄŸÄ±cÄ±\n",
        "    'I-FIN',       # EN: Inside FIN Code | AZ: FÄ°N kodunun iÃ§i (davamÄ±)\n",
        "    'B-ID',        # EN: Beginning of ID Number | AZ: ÅÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™sinin baÅŸlanÄŸÄ±cÄ±\n",
        "    'I-ID'         # EN: Inside ID Number | AZ: ÅÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™sinin iÃ§i (davamÄ±)\n",
        "]\n",
        "\n",
        "# EN: Label mapping (label to id and id to label dictionaries)\n",
        "# AZ: Label mapping (etiket-id vÉ™ id-etiket dictionary-lÉ™ri)\n",
        "label2id = {label: i for i, label in enumerate(LABELS)}\n",
        "id2label = {i: label for i, label in enumerate(LABELS)}\n",
        "\n",
        "print(\"ğŸ“‹ Konfiqurasiya:\")  # EN: Configuration | AZ: Konfiqurasiya\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Max Length: {MAX_LEN}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"Labels: {LABELS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPZHwhf6B3vW",
        "outputId": "a0bd8eea-9211-4c2d-82f7-bb4d9df8e2f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“ Entity Format QaydalarÄ±:\n",
            "ğŸ”¹ FIN kod: 7 simvol (hÉ™rf+rÉ™qÉ™m) - mÉ™sÉ™lÉ™n: AZEDF12\n",
            "ğŸ”¹ ID/AA: 9 simvol (AA + 7 rÉ™qÉ™m) - mÉ™sÉ™lÉ™n: AA1234567\n",
            "ğŸ”¹ ID/AZE: 12 simvol (AZE + 9 rÉ™qÉ™m) - mÉ™sÉ™lÉ™n: AZE123456789\n",
            "ğŸ”¹ Avtomobil: XX-YY-ZZZ formatÄ± - mÉ™sÉ™lÉ™n: 90-AB-123\n"
          ]
        }
      ],
      "source": [
        "def validate_entity_format(text: str, entity_type: str) -> bool:\n",
        "    \"\"\"\n",
        "    EN: Validates the format of an entity based on its type\n",
        "    AZ: Entity-nin formatÄ±nÄ± doÄŸrulayÄ±r\n",
        "\n",
        "    EN: Format rules:\n",
        "    AZ: Format qaydalarÄ±:\n",
        "    - FIN code: 7 characters (letters + digits)\n",
        "      EN: Example: AZEDF12\n",
        "    - ID/AA: AA + 7 digits (9 characters)\n",
        "      EN: Example: AA1234567\n",
        "    - ID/AZE: AZE + 9 digits (12 characters)\n",
        "      EN: Example: AZE123456789\n",
        "    - Car Plate: XX-YY-ZZZ format\n",
        "      EN: Example: 90-AB-123\n",
        "    \"\"\"\n",
        "    if entity_type == 'FIN':\n",
        "        # EN: FIN code must be 7 characters (letters or digits)\n",
        "        # AZ: FIN kod 7 simvol olmalÄ±dÄ±r (hÉ™rf vÉ™ rÉ™qÉ™m qarÄ±ÅŸÄ±ÄŸÄ±)\n",
        "        return len(text) == 7 and re.match(r'^[A-Z0-9]{7}$', text)\n",
        "\n",
        "    elif entity_type == 'ID':\n",
        "        # EN: Serial number rules\n",
        "        # AZ: Seriya nÃ¶mrÉ™si qaydalarÄ±\n",
        "        if text.startswith('AA'):\n",
        "            # EN: If starts with AA, must be 9 characters (AA + 7 digits)\n",
        "            # AZ: AA ilÉ™ baÅŸlayÄ±rsa 9 simvol (AA + 7 rÉ™qÉ™m)\n",
        "            return len(text) == 9 and re.match(r'^AA\\d{7}$', text)\n",
        "        elif text.startswith('AZE'):\n",
        "            # EN: If starts with AZE, must be 12 characters (AZE + 9 digits)\n",
        "            # AZ: AZE ilÉ™ baÅŸlayÄ±rsa 12 simvol (AZE + 9 rÉ™qÉ™m)\n",
        "            return len(text) == 12 and re.match(r'^AZE\\d{9}$', text)\n",
        "        else:\n",
        "            # EN: For other serials, general rule (2-3 letters + 6-9 digits)\n",
        "            # AZ: DigÉ™r seriya nÃ¶mrÉ™lÉ™ri Ã¼Ã§Ã¼n Ã¼mumi qayda\n",
        "            return 8 <= len(text) <= 12 and re.match(r'^[A-Z]{2,3}\\d{6,9}$', text)\n",
        "\n",
        "    elif entity_type == 'PLATE':\n",
        "        # EN: Car plate should be in XX-YY-ZZZ format\n",
        "        # AZ: Avtomobil nÃ¶mrÉ™si XX-YY-ZZZ formatÄ±nda olmalÄ±dÄ±r\n",
        "        return len(text) == 9 and re.match(r'^\\d{2}-[A-Z]{2}-\\d{3}$', text)\n",
        "\n",
        "    return False\n",
        "\n",
        "def show_validation_rules():\n",
        "    \"\"\"\n",
        "    EN: Displays the format validation rules for entities\n",
        "    AZ: Format qaydalarÄ±nÄ± gÃ¶stÉ™rir\n",
        "    \"\"\"\n",
        "    print(\"ğŸ“ Entity Format QaydalarÄ±:\")\n",
        "    print(\"ğŸ”¹ FIN kod: 7 simvol (hÉ™rf+rÉ™qÉ™m) - mÉ™sÉ™lÉ™n: AZEDF12\")\n",
        "    print(\"ğŸ”¹ ID/AA: 9 simvol (AA + 7 rÉ™qÉ™m) - mÉ™sÉ™lÉ™n: AA1234567\")\n",
        "    print(\"ğŸ”¹ ID/AZE: 12 simvol (AZE + 9 rÉ™qÉ™m) - mÉ™sÉ™lÉ™n: AZE123456789\")\n",
        "    print(\"ğŸ”¹ Avtomobil: XX-YY-ZZZ formatÄ± - mÉ™sÉ™lÉ™n: 90-AB-123\")\n",
        "\n",
        "show_validation_rules()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3S-cTAOB3sp",
        "outputId": "391ca304-8eef-4290-dd1a-19d7f0542399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Dataset yÃ¼klÉ™mÉ™ funksiyalarÄ± hazÄ±rlandÄ±\n"
          ]
        }
      ],
      "source": [
        "def convert_spacy_to_bio(text: str, entities: list) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    EN: Converts spaCy format entities to BIO format labels\n",
        "    AZ: spaCy formatÄ±ndan BIO formatÄ±na Ã§evirmÉ™\n",
        "    \"\"\"\n",
        "    tokens = text.split()\n",
        "    labels = ['O'] * len(tokens)\n",
        "\n",
        "    # EN: Calculate token positions (character indices in text)\n",
        "    # AZ: Token-larÄ±n mÃ¶vqelÉ™rini hesablayÄ±rÄ±q\n",
        "    token_positions = []\n",
        "    current_pos = 0\n",
        "\n",
        "    for token in tokens:\n",
        "        start_pos = text.find(token, current_pos)\n",
        "        end_pos = start_pos + len(token)\n",
        "        token_positions.append((start_pos, end_pos))\n",
        "        current_pos = end_pos\n",
        "\n",
        "    # EN: Convert entities to BIO format\n",
        "    # AZ: Entity-lÉ™ri BIO formatÄ±na Ã§eviririk\n",
        "    for start_char, end_char, entity_type in entities:\n",
        "        # EN: Map entity type to BIO label\n",
        "        # AZ: Entity tipini bizim formatÄ±mÄ±za uyÄŸunlaÅŸdÄ±rÄ±rÄ±q\n",
        "        if entity_type == \"CAR_PLATE\":\n",
        "            bio_label = \"PLATE\"\n",
        "        elif entity_type == \"FIN_CODE\":\n",
        "            bio_label = \"FIN\"\n",
        "        elif entity_type == \"ID_NUMBER\":\n",
        "            bio_label = \"ID\"\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        # EN: Find tokens that overlap with entity span\n",
        "        # AZ: HansÄ± token-larÄ±n entity-yÉ™ aid olduÄŸunu tapÄ±rÄ±q\n",
        "        entity_tokens = []\n",
        "        for i, (token_start, token_end) in enumerate(token_positions):\n",
        "            if token_start < end_char and token_end > start_char:\n",
        "                entity_tokens.append(i)\n",
        "\n",
        "        # EN: Set BIO labels for entity tokens\n",
        "        # AZ: BIO etiketlÉ™ri tÉ™yin edirik\n",
        "        for i, token_idx in enumerate(entity_tokens):\n",
        "            if i == 0:\n",
        "                labels[token_idx] = f\"B-{bio_label}\"\n",
        "            else:\n",
        "                labels[token_idx] = f\"I-{bio_label}\"\n",
        "\n",
        "    return tokens, labels\n",
        "\n",
        "def load_spacy_json_from_path(file_path: str):\n",
        "    \"\"\"\n",
        "    EN: Loads a spaCy-format JSON dataset from the given path\n",
        "    AZ: VerilÉ™n fayl yolundan spaCy formatÄ±nda JSON dataseti yÃ¼klÉ™yir\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        texts = []\n",
        "        labels = []\n",
        "\n",
        "        for item in data:\n",
        "            text = item[0]  # EN: Sentence | AZ: CÃ¼mlÉ™\n",
        "            annotations = item[1]  # EN: Annotation | AZ: Annotasiyalar\n",
        "            entities = annotations.get('entities', [])\n",
        "\n",
        "            tokens, bio_labels = convert_spacy_to_bio(text, entities)\n",
        "            texts.append(tokens)\n",
        "            labels.append(bio_labels)\n",
        "\n",
        "        print(f\"âœ… {file_path} uÄŸurla yÃ¼klÉ™ndi: {len(texts)} nÃ¼munÉ™\")\n",
        "\n",
        "        return texts, labels\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ Fayl tapÄ±lmadÄ±: {file_path}\")\n",
        "        return [], []\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ XÉ™ta baÅŸ verdi {file_path}: {e}\")\n",
        "        return [], []\n",
        "\n",
        "def load_datasets_from_paths(file_paths: List[str]):\n",
        "    \"\"\"\n",
        "    EN: Loads multiple spaCy-format datasets from given file paths\n",
        "    AZ: VerilÉ™n fayl yollarÄ±ndan datasetlÉ™ri yÃ¼klÉ™yir\n",
        "    \"\"\"\n",
        "    datasets = []\n",
        "\n",
        "    for i, file_path in enumerate(file_paths, 1):\n",
        "        print(f\"\\nğŸ“ Dataset {i} yÃ¼klÉ™nir: {file_path}\")\n",
        "        texts, labels = load_spacy_json_from_path(file_path)\n",
        "\n",
        "        if texts:\n",
        "            datasets.append((texts, labels))\n",
        "            print(f\"âœ… Dataset {i} yÃ¼klÉ™ndi: {len(texts)} nÃ¼munÉ™\")\n",
        "        else:\n",
        "            print(f\"âŒ Dataset {i} yÃ¼klÉ™nmÉ™di\")\n",
        "\n",
        "    return datasets\n",
        "\n",
        "def combine_datasets(datasets: List[Tuple[List[List[str]], List[List[str]]]]):\n",
        "    \"\"\"\n",
        "    EN: Combines multiple token/label datasets into one\n",
        "    AZ: DatasetlÉ™ri birlÉ™ÅŸdirir\n",
        "    \"\"\"\n",
        "    all_texts = []\n",
        "    all_labels = []\n",
        "\n",
        "    for i, (texts, labels) in enumerate(datasets, 1):\n",
        "        print(f\"Dataset {i}: {len(texts)} nÃ¼munÉ™\")\n",
        "        all_texts.extend(texts)\n",
        "        all_labels.extend(labels)\n",
        "\n",
        "    print(f\"ğŸ“Š Ãœmumi: {len(all_texts)} nÃ¼munÉ™\")\n",
        "    return all_texts, all_labels\n",
        "\n",
        "print(\"âœ… Dataset yÃ¼klÉ™mÉ™ funksiyalarÄ± hazÄ±rlandÄ±\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szCW2dIbB3qS",
        "outputId": "3ed09163-fc28-4dc1-af3f-73517af8a6d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Dataset analiz funksiyalarÄ± hazÄ±rlandÄ±\n"
          ]
        }
      ],
      "source": [
        "def analyze_dataset(texts: List[List[str]], labels: List[List[str]]):\n",
        "    \"\"\"\n",
        "    EN: Analyzes the dataset and prints statistics\n",
        "    AZ: Dataset analizi vÉ™ statistika\n",
        "    \"\"\"\n",
        "    label_counts = {}\n",
        "    total_tokens = 0\n",
        "    entity_examples = {}\n",
        "\n",
        "    for text_tokens, label_seq in zip(texts, labels):\n",
        "        total_tokens += len(label_seq)\n",
        "\n",
        "        # EN: Collect entity examples\n",
        "        # AZ: Entity nÃ¼munÉ™lÉ™rini topla\n",
        "        i = 0\n",
        "        while i < len(text_tokens):\n",
        "            label = label_seq[i] if i < len(label_seq) else 'O'\n",
        "\n",
        "            if label.startswith('B-'):\n",
        "                entity_type = label[2:]\n",
        "                entity_tokens = [text_tokens[i]]\n",
        "                j = i + 1\n",
        "\n",
        "                while (j < len(text_tokens) and\n",
        "                       j < len(label_seq) and\n",
        "                       label_seq[j] == f'I-{entity_type}'):\n",
        "                    entity_tokens.append(text_tokens[j])\n",
        "                    j += 1\n",
        "\n",
        "                entity_text = ''.join(entity_tokens)\n",
        "                if entity_type not in entity_examples:\n",
        "                    entity_examples[entity_type] = []\n",
        "                if len(entity_examples[entity_type]) < 3:\n",
        "                    entity_examples[entity_type].append(entity_text)\n",
        "                i = j\n",
        "            else:\n",
        "                i += 1\n",
        "\n",
        "        # EN: Count label frequencies\n",
        "        # AZ: Label sayÄ±nÄ± hesabla\n",
        "        for label in label_seq:\n",
        "            label_counts[label] = label_counts.get(label, 0) + 1\n",
        "\n",
        "    print(\"\\nğŸ“ˆ Dataset StatistikasÄ±:\")\n",
        "    print(f\"Ãœmumi cÃ¼mlÉ™ sayÄ±: {len(texts)}\")\n",
        "    print(f\"Ãœmumi token sayÄ±: {total_tokens}\")\n",
        "    print(\"\\nğŸ·ï¸ Label paylanmasÄ±:\")\n",
        "\n",
        "    for label, count in sorted(label_counts.items()):\n",
        "        percentage = (count / total_tokens) * 100\n",
        "        print(f\"{label}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    print(\"\\nğŸ“ Entity nÃ¼munÉ™lÉ™ri:\")\n",
        "    for entity_type, examples in entity_examples.items():\n",
        "        print(f\"{entity_type}: {', '.join(examples[:3])}\")\n",
        "\n",
        "def validate_dataset_entities(texts: List[List[str]], labels: List[List[str]]):\n",
        "    \"\"\"\n",
        "    EN: Validates entities in the dataset using format rules\n",
        "    AZ: Dataset-dÉ™ olan entity-lÉ™ri validation qaydalarÄ±na gÃ¶rÉ™ yoxlayÄ±r\n",
        "    \"\"\"\n",
        "    valid_entities = {'FIN': 0, 'ID': 0, 'PLATE': 0}\n",
        "    invalid_entities = {'FIN': 0, 'ID': 0, 'PLATE': 0}\n",
        "\n",
        "    for text_tokens, label_seq in zip(texts, labels):\n",
        "        i = 0\n",
        "        while i < len(text_tokens):\n",
        "            label = label_seq[i] if i < len(label_seq) else 'O'\n",
        "\n",
        "            if label.startswith('B-'):\n",
        "                entity_type = label[2:]\n",
        "                entity_tokens = [text_tokens[i]]\n",
        "                j = i + 1\n",
        "\n",
        "                while (j < len(text_tokens) and\n",
        "                       j < len(label_seq) and\n",
        "                       label_seq[j] == f'I-{entity_type}'):\n",
        "                    entity_tokens.append(text_tokens[j])\n",
        "                    j += 1\n",
        "\n",
        "                entity_text = ''.join(entity_tokens)\n",
        "\n",
        "                if validate_entity_format(entity_text, entity_type):\n",
        "                    valid_entities[entity_type] += 1\n",
        "                else:\n",
        "                    invalid_entities[entity_type] += 1\n",
        "\n",
        "                i = j\n",
        "            else:\n",
        "                i += 1\n",
        "\n",
        "    print(\"\\nâœ… Validation NÉ™ticÉ™lÉ™ri:\")\n",
        "    for entity_type in valid_entities:\n",
        "        total = valid_entities[entity_type] + invalid_entities[entity_type]\n",
        "        if total > 0:\n",
        "            valid_percent = (valid_entities[entity_type] / total) * 100\n",
        "            print(f\"{entity_type}: {valid_entities[entity_type]}/{total} valid ({valid_percent:.1f}%)\")\n",
        "\n",
        "print(\"âœ… Dataset analiz funksiyalarÄ± hazÄ±rlandÄ±\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH9-8GDDB3W0",
        "outputId": "fafcd822-0483-466b-9bde-336395c09dc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“‚ DatasetlÉ™r yÃ¼klÉ™nir...\n",
            "Fayl yollarÄ±:\n",
            "  1. /content/car_plate_dataset_annotated.json\n",
            "  2. /content/fin_annotated.json\n",
            "  3. /content/vesiqe_annotated.json\n",
            "\n",
            "ğŸ“ Dataset 1 yÃ¼klÉ™nir: /content/car_plate_dataset_annotated.json\n",
            "âœ… /content/car_plate_dataset_annotated.json uÄŸurla yÃ¼klÉ™ndi: 30000 nÃ¼munÉ™\n",
            "âœ… Dataset 1 yÃ¼klÉ™ndi: 30000 nÃ¼munÉ™\n",
            "\n",
            "ğŸ“ Dataset 2 yÃ¼klÉ™nir: /content/fin_annotated.json\n",
            "âœ… /content/fin_annotated.json uÄŸurla yÃ¼klÉ™ndi: 30000 nÃ¼munÉ™\n",
            "âœ… Dataset 2 yÃ¼klÉ™ndi: 30000 nÃ¼munÉ™\n",
            "\n",
            "ğŸ“ Dataset 3 yÃ¼klÉ™nir: /content/vesiqe_annotated.json\n",
            "âœ… /content/vesiqe_annotated.json uÄŸurla yÃ¼klÉ™ndi: 30000 nÃ¼munÉ™\n",
            "âœ… Dataset 3 yÃ¼klÉ™ndi: 30000 nÃ¼munÉ™\n",
            "Dataset 1: 30000 nÃ¼munÉ™\n",
            "Dataset 2: 30000 nÃ¼munÉ™\n",
            "Dataset 3: 30000 nÃ¼munÉ™\n",
            "ğŸ“Š Ãœmumi: 90000 nÃ¼munÉ™\n",
            "\n",
            "ğŸ“ˆ Dataset StatistikasÄ±:\n",
            "Ãœmumi cÃ¼mlÉ™ sayÄ±: 90000\n",
            "Ãœmumi token sayÄ±: 576351\n",
            "\n",
            "ğŸ·ï¸ Label paylanmasÄ±:\n",
            "B-FIN: 30000 (5.2%)\n",
            "B-ID: 30000 (5.2%)\n",
            "B-PLATE: 30000 (5.2%)\n",
            "O: 486351 (84.4%)\n",
            "\n",
            "ğŸ“ Entity nÃ¼munÉ™lÉ™ri:\n",
            "PLATE: 72-MY-215, 58-OH-587, 68-CB-083\n",
            "FIN: B5LC8EA, 7Y8TGYS, LYV28M9\n",
            "ID: AZE123928547, AZE457505033-dir., AZE643900779\n",
            "\n",
            "âœ… Validation NÉ™ticÉ™lÉ™ri:\n",
            "FIN: 26817/30000 valid (89.4%)\n",
            "ID: 26399/30000 valid (88.0%)\n",
            "PLATE: 28734/30000 valid (95.8%)\n",
            "\n",
            "ğŸ“‹ Data Split:\n",
            "Train: 72000 nÃ¼munÉ™\n",
            "Validation: 18000 nÃ¼munÉ™\n",
            "âœ… DatasetlÉ™r uÄŸurla hazÄ±rlandÄ±!\n"
          ]
        }
      ],
      "source": [
        "# EN: File paths for datasets\n",
        "# AZ: Fayl YollarÄ±\n",
        "dataset_paths = [\n",
        "    \"/content/car_plate_dataset_annotated.json\",    # EN: 1st dataset | AZ: 1-ci dataset\n",
        "    \"/content/fin_annotated.json\",                  # EN: 2nd dataset | AZ: 2-ci dataset\n",
        "    \"/content/vesiqe_annotated.json\"                # EN: 3rd dataset | AZ: 3-cÃ¼ dataset\n",
        "]\n",
        "\n",
        "print(\"ğŸ“‚ DatasetlÉ™r yÃ¼klÉ™nir...\")  # EN: Loading datasets...\n",
        "print(\"Fayl yollarÄ±:\")              # EN: File paths:\n",
        "for i, path in enumerate(dataset_paths, 1):\n",
        "    print(f\"  {i}. {path}\")\n",
        "\n",
        "# EN: Load datasets from paths\n",
        "# AZ: DatasetlÉ™ri yÃ¼klÉ™\n",
        "datasets = load_datasets_from_paths(dataset_paths)\n",
        "\n",
        "if datasets:\n",
        "    # EN: Combine datasets into one\n",
        "    # AZ: DatasetlÉ™ri birlÉ™ÅŸdir\n",
        "    all_texts, all_labels = combine_datasets(datasets)\n",
        "\n",
        "    # EN: Analyze dataset statistics\n",
        "    # AZ: Dataset analizini et\n",
        "    analyze_dataset(all_texts, all_labels)\n",
        "\n",
        "    # EN: Entity validation check\n",
        "    # AZ: Validation yoxlamasÄ±\n",
        "    validate_dataset_entities(all_texts, all_labels)\n",
        "\n",
        "    # EN: Train-validation split (80% train, 20% validation)\n",
        "    # AZ: Train-validation split (80% train, 20% validation)\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "        all_texts, all_labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"\\nğŸ“‹ Data Split:\")\n",
        "    print(f\"Train: {len(train_texts)} nÃ¼munÉ™\")\n",
        "    print(f\"Validation: {len(val_texts)} nÃ¼munÉ™\")\n",
        "\n",
        "    print(\"âœ… DatasetlÉ™r uÄŸurla hazÄ±rlandÄ±!\")\n",
        "else:\n",
        "    print(\"âŒ HeÃ§ bir dataset yÃ¼klÉ™nmÉ™di!\")\n",
        "    print(\"Fayl yollarÄ±nÄ± yoxlayÄ±n vÉ™ yenidÉ™n cÉ™hd edin.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181,
          "referenced_widgets": [
            "7451b09e487c4481916e1f6c7abbc33a",
            "7278f1d7dfcc48cab48ea5b001d483c5",
            "f7bf98e4d6a6428093a033fa97e2c849",
            "4d10d376136648b7ba9b2ad5c4d32a1e",
            "77b3ffc1214c4627b8626f726fa41a59",
            "0e0b28a282dc49f5a563d7ab521dd801",
            "7f78fcc60886479da88c422bbc0b8ebc",
            "e130ec4e73aa45598a5a009fdc887887",
            "e64757ddaf8b4bf2ae0f6c5f745ade23",
            "caf67030608e4dc99a3a4900ef9241b0",
            "37dac18db95f447b95320205457ba558",
            "8c830504c9e54be8baab8bf66f041af3",
            "46763577fa8c4794bb03e2aa7de8f5d0",
            "655f4bc334af49e2b52c11021e23604c",
            "67c13fbb127f4d2c94ff40f6af9bc909",
            "847a476b689d40af98170a06eee892a3",
            "312caf705e1f47c39fc5b07c894ad63a",
            "e0a5bb0cc08a47f4bf3e8154c425d890",
            "22d0f7121be84fa9b0e832a8e3c4c7d4",
            "2a0edfbf54634e078d19c46a1b820b3e",
            "d5c63abd168a4109bc6a756f72999f30",
            "861963f762cb488ba70148fb8bc78ba4",
            "1231e82cfc7d46ff8b493cf78d2093f5",
            "e180afe3e08f4bbc840e653a1ad110c7",
            "274856a9889343dea99fa8b10844ca63",
            "737985314d1446c5b5243b30038451bd",
            "9c4375d247234b59af4dac64b6c7b0ee",
            "7333a454b12e4202853f8568ac11cd5d",
            "d8781ae189bf4ba18fe1635b59667bcf",
            "81c8e7e2e3c14dfba3fac7c31c93a0f2",
            "ddf642453cbd480ca3a3a21e70360bb9",
            "037f4ce829924fdf9c7885f81c25b382",
            "e9d9cc787a1648c6bc07cc18958272ca",
            "44dc96d1215349d999fe19aed90fd1c1",
            "17d348b63b4c44fa93d62fb58d178b36",
            "2b54f79fc7f14ab0a8234e05f0171dd4",
            "46eb81caa4434b58b9e9337306bfc80a",
            "6ce9db9351fe421d979fffd75aa17fe6",
            "8909ac2efc1243c5a50400ade497a322",
            "25cb880c61b140558493812f183c39cf",
            "9344f8c4324e467d8513e7125c521999",
            "0fea1788deb44212bce939ec154765ba",
            "f96cb7e1b0cc4ea5858fe112742305e5",
            "5794e1b8601745e88a1427466832bb49"
          ]
        },
        "id": "Cu0s4LX5ExMa",
        "outputId": "d62e4ab3-4d00-4051-fbbb-144ca5f40202"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7451b09e487c4481916e1f6c7abbc33a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c830504c9e54be8baab8bf66f041af3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1231e82cfc7d46ff8b493cf78d2093f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44dc96d1215349d999fe19aed90fd1c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Tokenizer yÃ¼klÉ™ndi: bert-base-multilingual-cased\n",
            "âœ… NERDataset sinifi hazÄ±rlandÄ±\n"
          ]
        }
      ],
      "source": [
        "# EN: Load tokenizer and define NERDataset class\n",
        "# AZ: Tokenizer vÉ™ model yÃ¼klÉ™mÉ™\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "print(f\"âœ… Tokenizer yÃ¼klÉ™ndi: {MODEL_NAME}\")  # EN: Tokenizer loaded\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    \"\"\"\n",
        "    EN: Custom PyTorch Dataset for NER tasks with BIO labels\n",
        "    AZ: BIO etiketli NER tapÅŸÄ±rÄ±qlarÄ± Ã¼Ã§Ã¼n PyTorch Dataset sinifi\n",
        "    \"\"\"\n",
        "    def __init__(self, texts: List[List[str]], labels: List[List[str]], tokenizer, max_len: int):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        labels = self.labels[idx]\n",
        "\n",
        "        # EN: Tokenize the text (split into words)\n",
        "        # AZ: TokenlÉ™ÅŸdirmÉ™ (sÃ¶zlÉ™r Ã¼zrÉ™ split)\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            is_split_into_words=True,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # EN: Align labels with tokens (using word_ids)\n",
        "        # AZ: Label-larÄ± token-lara uyÄŸunlaÅŸdÄ±rma\n",
        "        word_ids = encoding.word_ids()\n",
        "        label_ids = []\n",
        "\n",
        "        for word_id in word_ids:\n",
        "            if word_id is None:\n",
        "                label_ids.append(-100)  # EN: Special tokens (CLS, SEP, PAD etc.)\n",
        "            elif word_id < len(labels):\n",
        "                label_ids.append(label2id[labels[word_id]])\n",
        "            else:\n",
        "                label_ids.append(label2id['O'])\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label_ids, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "print(\"âœ… NERDataset sinifi hazÄ±rlandÄ±\")  # EN: NERDataset class is ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247,
          "referenced_widgets": [
            "2d5b7ffb094a455993681e950a647494",
            "81112e276508416f8a2ffe6b827ced56",
            "655914e0e4154ad49400c8ada1abf629",
            "bf8adce54c9d4ca69fad5daf553fd139",
            "91923a9511b44236a10d3ea92b6d392c",
            "f93ddb1432874d79880e8e77ed9a0d1b",
            "141b0b48bbdf4d15ac171b055176a780",
            "6b0a5a20a9224dc786b04134a33e96dd",
            "14e5291f852a415bac0822e2318c8730",
            "daaca05878734626ae9514ede263158f",
            "f27b01f137894612907ee7f21566b221"
          ]
        },
        "id": "2gE4EeNSE8Nj",
        "outputId": "c561a418-70ab-4d86-bab8-40f7d4a241fb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d5b7ffb094a455993681e950a647494",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Model yÃ¼klÉ™ndi: bert-base-multilingual-cased\n",
            "ğŸ“Š Label sayÄ±: 7\n",
            "Max steps: 300\n",
            "Warmup steps: 50\n",
            "Logging every: 25 steps\n",
            "Evaluation every: 50 steps\n",
            "Save every: 50 steps\n",
            "âœ… Training konfiqurasiyasÄ± hazÄ±rlandÄ±\n"
          ]
        }
      ],
      "source": [
        "# EN: Load model and set training configuration\n",
        "# AZ: Model yÃ¼klÉ™ vÉ™ training konfiqurasiyasÄ±\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(LABELS),\n",
        "    label2id=label2id,\n",
        "    id2label=id2label\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "print(f\"âœ… Model yÃ¼klÉ™ndi: {MODEL_NAME}\")        # EN: Model loaded\n",
        "print(f\"ğŸ“Š Label sayÄ±: {len(LABELS)}\")          # EN: Number of labels\n",
        "\n",
        "# EN: Metrics calculation function for Trainer\n",
        "# AZ: Metrics hesablama funksiyasÄ±\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    EN: Computes metrics during training\n",
        "    AZ: Training zamanÄ± metriklÉ™r hesablamaq Ã¼Ã§Ã¼n\n",
        "    \"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # EN: Remove labels with -100 (special tokens)\n",
        "    # AZ: -100 olan labellarÄ± Ã§Ä±xar (special tokens)\n",
        "    true_predictions = [\n",
        "        [id2label[p] for p, l in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [id2label[l] for p, l in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    # EN: Flatten lists\n",
        "    # AZ: Flatten etmÉ™k\n",
        "    flat_true_labels = [label for sublist in true_labels for label in sublist]\n",
        "    flat_predictions = [pred for sublist in true_predictions for pred in sublist]\n",
        "\n",
        "    # EN: Calculate metrics\n",
        "    # AZ: Metrics hesablamaq\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        flat_true_labels, flat_predictions, average='weighted', zero_division=0\n",
        "    )\n",
        "    accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# EN: Training arguments for 300 steps\n",
        "# AZ: Training argumentlÉ™ri - 300 step Ã¼Ã§Ã¼n\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    max_steps=300,                    # EN: 300 steps limit | AZ: 300 step mÉ™hdudiyyÉ™ti\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    warmup_steps=50,                  # EN: Less warmup (1/6 of 300) | AZ: Daha az warmup (300 step-in 1/6-sÄ±)\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=25,                 # EN: More frequent logging | AZ: Daha tez-tez log (300/12)\n",
        "    eval_strategy=\"steps\",      # EN: Use evaluation_strategy if needed\n",
        "    eval_steps=50,                    # EN: More frequent eval | AZ: Daha tez-tez eval (300/6)\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,                    # EN: More frequent save | AZ: Daha tez-tez save\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    report_to=None,\n",
        "    learning_rate=LEARNING_RATE,\n",
        ")\n",
        "\n",
        "print(f\"Max steps: 300\")\n",
        "print(f\"Warmup steps: 50\")\n",
        "print(f\"Logging every: 25 steps\")\n",
        "print(f\"Evaluation every: 50 steps\")\n",
        "print(f\"Save every: 50 steps\")\n",
        "\n",
        "print(\"âœ… Training konfiqurasiyasÄ± hazÄ±rlandÄ±\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 944
        },
        "id": "M01IrWO9FBXp",
        "outputId": "b7b6fb06-12d3-420c-e7bd-88660ad1ba45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Train dataset: 72000 nÃ¼munÉ™\n",
            "âœ… Validation dataset: 18000 nÃ¼munÉ™\n",
            "âœ… Trainer hazÄ±rlandÄ±\n",
            "\n",
            "ğŸš€ Training baÅŸlayÄ±r...\n",
            "ğŸ“Š Total training samples: 72000\n",
            "ğŸ“Š Total validation samples: 18000\n",
            "ğŸ”¥ Device: cuda\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbahramzada\u001b[0m (\u001b[33mbahramzada-unec-business-school\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "creating run (0.0s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250915_111703-6nq9rfy8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bahramzada-unec-business-school/huggingface/runs/6nq9rfy8' target=\"_blank\">colorful-sound-25</a></strong> to <a href='https://wandb.ai/bahramzada-unec-business-school/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/bahramzada-unec-business-school/huggingface' target=\"_blank\">https://wandb.ai/bahramzada-unec-business-school/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/bahramzada-unec-business-school/huggingface/runs/6nq9rfy8' target=\"_blank\">https://wandb.ai/bahramzada-unec-business-school/huggingface/runs/6nq9rfy8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [300/300 21:26, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.165500</td>\n",
              "      <td>0.015308</td>\n",
              "      <td>0.998266</td>\n",
              "      <td>0.998264</td>\n",
              "      <td>0.998270</td>\n",
              "      <td>0.998266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.004300</td>\n",
              "      <td>0.000867</td>\n",
              "      <td>0.999983</td>\n",
              "      <td>0.999983</td>\n",
              "      <td>0.999983</td>\n",
              "      <td>0.999983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.000642</td>\n",
              "      <td>0.999966</td>\n",
              "      <td>0.999966</td>\n",
              "      <td>0.999966</td>\n",
              "      <td>0.999966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.000385</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000326</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Training tamamlandÄ±!\n",
            "\n",
            "ğŸ“Š Model qiymÉ™tlÉ™ndirilir...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1125/1125 02:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¯ Evaluation nÉ™ticÉ™lÉ™ri:\n",
            "eval_loss: 0.0004\n",
            "eval_accuracy: 1.0000\n",
            "eval_f1: 1.0000\n",
            "eval_precision: 1.0000\n",
            "eval_recall: 1.0000\n",
            "eval_runtime: 125.3216\n",
            "eval_samples_per_second: 143.6300\n",
            "eval_steps_per_second: 8.9770\n",
            "epoch: 0.0667\n",
            "âœ… Model ./best_model qovluÄŸunda saxlanÄ±ldÄ±\n"
          ]
        }
      ],
      "source": [
        "# EN: Create dataset objects and start training if train_texts are loaded\n",
        "# AZ: Dataset obyektlÉ™rini yarat vÉ™ train baÅŸlat, É™gÉ™r train_texts mÃ¶vcuddursa\n",
        "\n",
        "if 'train_texts' in locals() and len(train_texts) > 0:\n",
        "    # EN: Create train and validation NERDataset objects\n",
        "    # AZ: Train vÉ™ validation NERDataset obyektlÉ™ri yarat\n",
        "    train_dataset = NERDataset(\n",
        "        texts=train_texts,\n",
        "        labels=train_labels,\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=MAX_LEN\n",
        "    )\n",
        "\n",
        "    val_dataset = NERDataset(\n",
        "        texts=val_texts,\n",
        "        labels=val_labels,\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=MAX_LEN\n",
        "    )\n",
        "\n",
        "    # EN: Data collator for token classification\n",
        "    # AZ: Token classification Ã¼Ã§Ã¼n data collator\n",
        "    data_collator = DataCollatorForTokenClassification(\n",
        "        tokenizer=tokenizer,\n",
        "        padding=True,\n",
        "        max_length=MAX_LEN,\n",
        "        pad_to_multiple_of=None,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # EN: Create Trainer object\n",
        "    # AZ: Trainer obyektini yarat\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    print(f\"âœ… Train dataset: {len(train_dataset)} nÃ¼munÉ™\")          # EN: Train dataset samples\n",
        "    print(f\"âœ… Validation dataset: {len(val_dataset)} nÃ¼munÉ™\")      # EN: Validation dataset samples\n",
        "    print(\"âœ… Trainer hazÄ±rlandÄ±\")                                  # EN: Trainer is ready\n",
        "\n",
        "    # EN: Start training process\n",
        "    # AZ: Training baÅŸlat\n",
        "    print(\"\\nğŸš€ Training baÅŸlayÄ±r...\")\n",
        "    print(f\"ğŸ“Š Total training samples: {len(train_dataset)}\")\n",
        "    print(f\"ğŸ“Š Total validation samples: {len(val_dataset)}\")\n",
        "    print(f\"ğŸ”¥ Device: {device}\")\n",
        "\n",
        "    try:\n",
        "        trainer.train()\n",
        "        print(\"âœ… Training tamamlandÄ±!\")                             # EN: Training completed\n",
        "\n",
        "        # EN: Evaluate the model\n",
        "        # AZ: Model qiymÉ™tlÉ™ndirmÉ™\n",
        "        print(\"\\nğŸ“Š Model qiymÉ™tlÉ™ndirilir...\")\n",
        "        eval_results = trainer.evaluate()\n",
        "\n",
        "        print(\"ğŸ¯ Evaluation nÉ™ticÉ™lÉ™ri:\")                          # EN: Evaluation results\n",
        "        for key, value in eval_results.items():\n",
        "            if isinstance(value, float):\n",
        "                print(f\"{key}: {value:.4f}\")\n",
        "            else:\n",
        "                print(f\"{key}: {value}\")\n",
        "\n",
        "        # EN: Save best model and tokenizer\n",
        "        # AZ: En yaxÅŸÄ± modeli saxla\n",
        "        trainer.save_model(\"./best_model\")\n",
        "        tokenizer.save_pretrained(\"./best_model\")\n",
        "        print(\"âœ… Model ./best_model qovluÄŸunda saxlanÄ±ldÄ±\")        # EN: Model saved in ./best_model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Training zamanÄ± xÉ™ta: {e}\")                      # EN: Error during training\n",
        "\n",
        "else:\n",
        "    print(\"âŒ DatasetlÉ™r hazÄ±rlanmadÄ±, training edilÉ™ bilmÉ™z!\")     # EN: Datasets not ready, training cannot be performed\n",
        "    print(\"ÆvvÉ™lki hissÉ™lÉ™rdÉ™ datasetlÉ™ri dÃ¼zgÃ¼n yÃ¼klÉ™yin.\")        # EN: Make sure datasets are loaded in previous steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpeNh_TqFNw-",
        "outputId": "f4ca39a9-1a30-403d-d9c8-d60c32865b11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… TÉ™kmillÉ™ÅŸdirilmiÅŸ inference funksiyalarÄ± hazÄ±rlandÄ±\n"
          ]
        }
      ],
      "source": [
        "def predict_entities_with_model(text: str, model, tokenizer, device):\n",
        "    \"\"\"\n",
        "    EN: Predicts entities in text using model\n",
        "    AZ: Modeli istifadÉ™ edÉ™rÉ™k entity-lÉ™ri predict edir\n",
        "    \"\"\"\n",
        "    tokens = text.split()\n",
        "\n",
        "    # EN: Encode with tokenizer\n",
        "    # AZ: Tokenizer ilÉ™ encode et\n",
        "    inputs = tokenizer(\n",
        "        tokens,\n",
        "        is_split_into_words=True,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=MAX_LEN\n",
        "    )\n",
        "\n",
        "    # EN: Send to GPU if available\n",
        "    # AZ: GPU-ya gÃ¶ndÉ™r\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # EN: Model prediction\n",
        "    # AZ: Model prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=2)\n",
        "\n",
        "    # EN: Get word IDs for mapping tokens\n",
        "    # AZ: Word IDs ilÉ™ token-word mapping\n",
        "    word_ids = inputs.word_ids() if hasattr(inputs, 'word_ids') else None\n",
        "    predicted_labels = []\n",
        "\n",
        "    if word_ids is None:\n",
        "        # EN: Manual mapping if word_ids not available\n",
        "        # AZ: Manual word mapping\n",
        "        tokenized = tokenizer.tokenize(' '.join(tokens))\n",
        "        pred_idx = 1  # EN: Start after CLS token\n",
        "\n",
        "        for token in tokens:\n",
        "            if pred_idx < len(predictions[0]):\n",
        "                label_id = predictions[0][pred_idx].item()\n",
        "                if label_id < len(id2label):\n",
        "                    predicted_labels.append(id2label[label_id])\n",
        "                else:\n",
        "                    predicted_labels.append('O')\n",
        "\n",
        "                # EN: Count subwords for token\n",
        "                # AZ: Token neÃ§É™ subword-É™ bÃ¶lÃ¼nÃ¼b onu hesabla\n",
        "                token_subwords = tokenizer.tokenize(token)\n",
        "                pred_idx += len(token_subwords)\n",
        "            else:\n",
        "                predicted_labels.append('O')\n",
        "    else:\n",
        "        # EN: Use word_ids mapping\n",
        "        # AZ: Word IDs istifadÉ™ et\n",
        "        for i in range(len(tokens)):\n",
        "            word_positions = [j for j, wid in enumerate(word_ids[0]) if wid == i]\n",
        "            if word_positions:\n",
        "                label_id = predictions[0][word_positions[0]].item()\n",
        "                if label_id < len(id2label):\n",
        "                    predicted_labels.append(id2label[label_id])\n",
        "                else:\n",
        "                    predicted_labels.append('O')\n",
        "            else:\n",
        "                predicted_labels.append('O')\n",
        "\n",
        "    # EN: Fix label count to match tokens\n",
        "    # AZ: Label sayÄ±nÄ± dÃ¼zÉ™lt\n",
        "    while len(predicted_labels) < len(tokens):\n",
        "        predicted_labels.append('O')\n",
        "    predicted_labels = predicted_labels[:len(tokens)]\n",
        "\n",
        "    return tokens, predicted_labels\n",
        "\n",
        "def enhanced_predict_with_validation(text: str, model, tokenizer, device):\n",
        "    \"\"\"\n",
        "    EN: Entity prediction with format validation\n",
        "    AZ: Validation qaydalarÄ± ilÉ™ tÉ™kmillÉ™ÅŸdirilmiÅŸ entity prediction\n",
        "    \"\"\"\n",
        "    tokens, predicted_labels = predict_entities_with_model(text, model, tokenizer, device)\n",
        "\n",
        "    validated_labels = []\n",
        "    i = 0\n",
        "\n",
        "    while i < len(tokens):\n",
        "        current_label = predicted_labels[i] if i < len(predicted_labels) else 'O'\n",
        "\n",
        "        if current_label.startswith('B-'):\n",
        "            entity_type = current_label[2:]\n",
        "            entity_tokens = [tokens[i]]\n",
        "\n",
        "            # EN: Gather full entity text\n",
        "            # AZ: Entity-nin tam mÉ™tnini topla\n",
        "            j = i + 1\n",
        "            while (j < len(tokens) and\n",
        "                   j < len(predicted_labels) and\n",
        "                   predicted_labels[j] == f'I-{entity_type}'):\n",
        "                entity_tokens.append(tokens[j])\n",
        "                j += 1\n",
        "\n",
        "            entity_text = ''.join(entity_tokens)\n",
        "\n",
        "            # EN: Validate format\n",
        "            # AZ: Format validation\n",
        "            if validate_entity_format(entity_text, entity_type):\n",
        "                # EN: Valid entity, keep labels\n",
        "                # AZ: Valid entity - saxla\n",
        "                validated_labels.append(current_label)\n",
        "                for k in range(i + 1, j):\n",
        "                    if k < len(predicted_labels):\n",
        "                        validated_labels.append(predicted_labels[k])\n",
        "                    else:\n",
        "                        validated_labels.append('O')\n",
        "            else:\n",
        "                # EN: Invalid entity, set to 'O'\n",
        "                # AZ: Invalid entity - O etiketi ver\n",
        "                for k in range(i, j):\n",
        "                    validated_labels.append('O')\n",
        "\n",
        "            i = j\n",
        "        else:\n",
        "            validated_labels.append(current_label)\n",
        "            i += 1\n",
        "\n",
        "    # EN: Fix label count to match tokens\n",
        "    # AZ: Label sayÄ±nÄ± dÃ¼zÉ™lt\n",
        "    while len(validated_labels) < len(tokens):\n",
        "        validated_labels.append('O')\n",
        "    validated_labels = validated_labels[:len(tokens)]\n",
        "\n",
        "    return tokens, validated_labels\n",
        "\n",
        "def blur_with_model_and_validation(text: str, model, tokenizer, device):\n",
        "    \"\"\"\n",
        "    EN: Blur entities using model & validation\n",
        "    AZ: Model + validation ilÉ™ blurring edir\n",
        "    \"\"\"\n",
        "    tokens, validated_labels = enhanced_predict_with_validation(text, model, tokenizer, device)\n",
        "\n",
        "    blurred_tokens = []\n",
        "    entities_found = []\n",
        "    i = 0\n",
        "\n",
        "    while i < len(tokens):\n",
        "        current_label = validated_labels[i] if i < len(validated_labels) else 'O'\n",
        "\n",
        "        if current_label.startswith('B-'):\n",
        "            entity_type = current_label[2:]\n",
        "            entity_tokens = [tokens[i]]\n",
        "            entity_start = i\n",
        "\n",
        "            # EN: Find entity continuation (I- labels)\n",
        "            # AZ: I- etiketli davamÄ±nÄ± tap\n",
        "            j = i + 1\n",
        "            while (j < len(tokens) and\n",
        "                   j < len(validated_labels) and\n",
        "                   validated_labels[j] == f'I-{entity_type}'):\n",
        "                entity_tokens.append(tokens[j])\n",
        "                j += 1\n",
        "\n",
        "            entity_text = ''.join(entity_tokens)\n",
        "\n",
        "            entities_found.append({\n",
        "                'text': entity_text,\n",
        "                'type': entity_type,\n",
        "                'start': entity_start,\n",
        "                'end': j-1,\n",
        "                'tokens': entity_tokens\n",
        "            })\n",
        "\n",
        "            blurred_tokens.append('[BLURRED]')\n",
        "            i = j\n",
        "        else:\n",
        "            blurred_tokens.append(tokens[i])\n",
        "            i += 1\n",
        "\n",
        "    return ' '.join(blurred_tokens), entities_found\n",
        "\n",
        "print(\"âœ… TÉ™kmillÉ™ÅŸdirilmiÅŸ inference funksiyalarÄ± hazÄ±rlandÄ±\")  # EN: Enhanced inference functions ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QI70afjkFZ7d",
        "outputId": "6644391d-91e6-4d66-d73c-fa21e02ac66d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¯ Model test edilir...\n",
            "ğŸ§ª Model + Validation Test...\n",
            "\n",
            "ğŸ“ Test 1:\n",
            "Orijinal: MÉ™nim fin kodum AZEDF12 olan kartÄ±m var\n",
            "Blurred:  MÉ™nim fin kodum [BLURRED] olan kartÄ±m var\n",
            "âœ… TapÄ±lan valid entity-lÉ™r:\n",
            "  âœ“ Valid - 'AZEDF12' (FIN)\n",
            "----------------------------------------------------------------------\n",
            "ğŸ“ Test 2:\n",
            "Orijinal: FIN kod AB12345 mÃ¶vcuddur\n",
            "Blurred:  FIN kod [BLURRED] mÃ¶vcuddur\n",
            "âœ… TapÄ±lan valid entity-lÉ™r:\n",
            "  âœ“ Valid - 'AB12345' (FIN)\n",
            "----------------------------------------------------------------------\n",
            "ğŸ“ Test 3:\n",
            "Orijinal: Bu 90-AB-123 nÃ¶mrÉ™li avtomobil dostumundur\n",
            "Blurred:  Bu [BLURRED] nÃ¶mrÉ™li avtomobil dostumundur\n",
            "âœ… TapÄ±lan valid entity-lÉ™r:\n",
            "  âœ“ Valid - '90-AB-123' (PLATE)\n",
            "----------------------------------------------------------------------\n",
            "ğŸ“ Test 4:\n",
            "Orijinal: ÅÉ™xsiyyÉ™t vÉ™siqÉ™ AA1234567 dir\n",
            "Blurred:  ÅÉ™xsiyyÉ™t vÉ™siqÉ™ [BLURRED] dir\n",
            "âœ… TapÄ±lan valid entity-lÉ™r:\n",
            "  âœ“ Valid - 'AA1234567' (ID)\n",
            "----------------------------------------------------------------------\n",
            "ğŸ“ Test 5:\n",
            "Orijinal: SÉ™nÉ™d nÃ¶mrÉ™si AZE123456789 tÉ™qdim edilmÉ™lidir\n",
            "Blurred:  SÉ™nÉ™d nÃ¶mrÉ™si [BLURRED] tÉ™qdim edilmÉ™lidir\n",
            "âœ… TapÄ±lan valid entity-lÉ™r:\n",
            "  âœ“ Valid - 'AZE123456789' (ID)\n",
            "----------------------------------------------------------------------\n",
            "ğŸ“ Test 6:\n",
            "Orijinal: SÉ™hv format AA12345 vÉ™ ya AZE12345 var\n",
            "Blurred:  SÉ™hv format AA12345 vÉ™ ya AZE12345 var\n",
            "âŒ HeÃ§ bir valid entity tapÄ±lmadÄ±\n",
            "----------------------------------------------------------------------\n",
            "ğŸ“ Test 7:\n",
            "Orijinal: Bu adi cÃ¼mlÉ™ dir heÃ§ nÉ™ yoxdur\n",
            "Blurred:  Bu adi cÃ¼mlÉ™ dir heÃ§ nÉ™ yoxdur\n",
            "âŒ HeÃ§ bir valid entity tapÄ±lmadÄ±\n",
            "----------------------------------------------------------------------\n",
            "ğŸ“ Test 8:\n",
            "Orijinal: FÄ°N MM4NS3L vÉ™ maÅŸÄ±n 77-KM-596 qeydiyyatÄ± var\n",
            "Blurred:  FÄ°N [BLURRED] vÉ™ maÅŸÄ±n [BLURRED] qeydiyyatÄ± var\n",
            "âœ… TapÄ±lan valid entity-lÉ™r:\n",
            "  âœ“ Valid - 'MM4NS3L' (FIN)\n",
            "  âœ“ Valid - '77-KM-596' (PLATE)\n",
            "----------------------------------------------------------------------\n",
            "ğŸ“ Test 9:\n",
            "Orijinal: Qeydiyyat prosesindÉ™ ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™si nÃ¶mrÉ™si AZE123456789 vÉ™ FÄ°N kod MM4NS3L tÉ™qdim edilmÉ™lidir. Avtomobilin nÃ¶mrÉ™si 77-KM-596 vÉ™ 10-AB-123 ilÉ™ baÄŸlÄ± mÉ™lumatlar da sistemÉ™ daxil edilmÉ™lidir. ÆlavÉ™ olaraq, yeni qeydiyyatda tÉ™lÉ™b olunan ikinci ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™si nÃ¶mrÉ™si AZE987654321 vÉ™ FÄ°N kodu QA9WT2K dÉ™ yoxlanÄ±lacaq. Qeyd: hÉ™r hansÄ± bir kod sÉ™hv daxil edilÉ™rsÉ™, sistem xÉ™bÉ™rdarlÄ±q edÉ™cÉ™k.\n",
            "Blurred:  Qeydiyyat prosesindÉ™ ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™si nÃ¶mrÉ™si [BLURRED] vÉ™ FÄ°N kod [BLURRED] tÉ™qdim edilmÉ™lidir. Avtomobilin nÃ¶mrÉ™si [BLURRED] vÉ™ [BLURRED] ilÉ™ baÄŸlÄ± mÉ™lumatlar da sistemÉ™ daxil edilmÉ™lidir. ÆlavÉ™ olaraq, yeni qeydiyyatda tÉ™lÉ™b olunan ikinci ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™si nÃ¶mrÉ™si [BLURRED] vÉ™ FÄ°N kodu [BLURRED] dÉ™ yoxlanÄ±lacaq. Qeyd: hÉ™r hansÄ± bir kod sÉ™hv daxil edilÉ™rsÉ™, sistem xÉ™bÉ™rdarlÄ±q edÉ™cÉ™k.\n",
            "âœ… TapÄ±lan valid entity-lÉ™r:\n",
            "  âœ“ Valid - 'AZE123456789' (ID)\n",
            "  âœ“ Valid - 'MM4NS3L' (FIN)\n",
            "  âœ“ Valid - '77-KM-596' (PLATE)\n",
            "  âœ“ Valid - '10-AB-123' (PLATE)\n",
            "  âœ“ Valid - 'AZE987654321' (ID)\n",
            "  âœ“ Valid - 'QA9WT2K' (FIN)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "======================================================================\n",
            "Ä°nteraktiv sistemÉ™ keÃ§mÉ™k istÉ™yirsinizmi? (y/n)\n",
            "ğŸ¤– AzÉ™rbaycan NER Blur Sistemi\n",
            "Model + Validation qaydalarÄ± istifadÉ™ edilir\n",
            "Format qaydalarÄ±:\n",
            "  â€¢ FIN: 7 simvol (hÉ™rf+rÉ™qÉ™m)\n",
            "  â€¢ ID/AA: 9 simvol (AA + 7 rÉ™qÉ™m)\n",
            "  â€¢ ID/AZE: 12 simvol (AZE + 9 rÉ™qÉ™m)\n",
            "  â€¢ Avtomobil: XX-YY-ZZZ\n",
            "\n",
            "Ã‡Ä±xmaq Ã¼Ã§Ã¼n 'exit' yazÄ±n\n",
            "\n",
            "ğŸ“ MÉ™tn daxil edin: exit\n",
            "ğŸ‘‹ GÃ¶rÃ¼ÅŸÉ™nÉ™dÉ™k!\n"
          ]
        }
      ],
      "source": [
        "def test_enhanced_model():\n",
        "    \"\"\"\n",
        "    EN: Tests the model with validation rules on various sentences\n",
        "    AZ: Validation qaydalarÄ± ilÉ™ modeli test edir\n",
        "    \"\"\"\n",
        "    test_sentences = [\n",
        "        \"MÉ™nim fin kodum AZEDF12 olan kartÄ±m var\",          # Valid FIN (7 simvol)\n",
        "        \"FIN kod AB12345 mÃ¶vcuddur\",                        # Valid FIN (7 simvol)\n",
        "        \"Bu 90-AB-123 nÃ¶mrÉ™li avtomobil dostumundur\",       # Valid PLATE\n",
        "        \"ÅÉ™xsiyyÉ™t vÉ™siqÉ™ AA1234567 dir\",                   # Valid ID (AA + 7 rÉ™qÉ™m)\n",
        "        \"SÉ™nÉ™d nÃ¶mrÉ™si AZE123456789 tÉ™qdim edilmÉ™lidir\",    # Valid ID (AZE + 9 rÉ™qÉ™m)\n",
        "        \"SÉ™hv format AA12345 vÉ™ ya AZE12345 var\",           # Invalid formats\n",
        "        \"Bu adi cÃ¼mlÉ™ dir heÃ§ nÉ™ yoxdur\",                   # HeÃ§ nÉ™ yox\n",
        "        \"FÄ°N MM4NS3L vÉ™ maÅŸÄ±n 77-KM-596 qeydiyyatÄ± var\",    # Valid entities\n",
        "        'Qeydiyyat prosesindÉ™ ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™si nÃ¶mrÉ™si AZE123456789 vÉ™ FÄ°N kod MM4NS3L tÉ™qdim edilmÉ™lidir. Avtomobilin nÃ¶mrÉ™si 77-KM-596 vÉ™ 10-AB-123 ilÉ™ baÄŸlÄ± mÉ™lumatlar da sistemÉ™ daxil edilmÉ™lidir. ÆlavÉ™ olaraq, yeni qeydiyyatda tÉ™lÉ™b olunan ikinci ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™si nÃ¶mrÉ™si AZE987654321 vÉ™ FÄ°N kodu QA9WT2K dÉ™ yoxlanÄ±lacaq. Qeyd: hÉ™r hansÄ± bir kod sÉ™hv daxil edilÉ™rsÉ™, sistem xÉ™bÉ™rdarlÄ±q edÉ™cÉ™k.' #custom\n",
        "    ]\n",
        "\n",
        "    print(\"ğŸ§ª Model + Validation Test...\\n\")\n",
        "\n",
        "    for i, sentence in enumerate(test_sentences, 1):\n",
        "        print(f\"ğŸ“ Test {i}:\")\n",
        "        print(f\"Orijinal: {sentence}\")\n",
        "\n",
        "        try:\n",
        "            blurred_text, entities = blur_with_model_and_validation(sentence, model, tokenizer, device)\n",
        "            print(f\"Blurred:  {blurred_text}\")\n",
        "\n",
        "            if entities:\n",
        "                print(\"âœ… TapÄ±lan valid entity-lÉ™r:\")\n",
        "                for entity in entities:\n",
        "                    validation_result = \"âœ“ Valid\" if validate_entity_format(entity['text'], entity['type']) else \"âœ— Invalid\"\n",
        "                    print(f\"  {validation_result} - '{entity['text']}' ({entity['type']})\")\n",
        "            else:\n",
        "                print(\"âŒ HeÃ§ bir valid entity tapÄ±lmadÄ±\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ XÉ™ta: {e}\")\n",
        "\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "def detailed_analysis_demo(text: str):\n",
        "    \"\"\"\n",
        "    EN: Shows detailed token-level analysis for a given sentence\n",
        "    AZ: Detailed token-level analiz\n",
        "    \"\"\"\n",
        "    print(f\"\\nğŸ” Detailed Analiz: '{text}'\")\n",
        "\n",
        "    try:\n",
        "        tokens, model_labels = predict_entities_with_model(text, model, tokenizer, device)\n",
        "        _, validated_labels = enhanced_predict_with_validation(text, model, tokenizer, device)\n",
        "\n",
        "        print(\"\\nToken-by-token analiz:\")\n",
        "        print(\"Token\".ljust(15) + \"Model\".ljust(10) + \"Validated\".ljust(12) + \"Valid?\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for i, token in enumerate(tokens):\n",
        "            model_label = model_labels[i] if i < len(model_labels) else 'O'\n",
        "            validated_label = validated_labels[i] if i < len(validated_labels) else 'O'\n",
        "\n",
        "            # EN: Check entity validation status\n",
        "            # AZ: Entity-nin validation statusunu yoxla\n",
        "            if model_label.startswith('B-'):\n",
        "                entity_type = model_label[2:]\n",
        "                # EN: Get full entity text\n",
        "                # AZ: Entity-nin tam mÉ™tnini tap\n",
        "                entity_tokens = [token]\n",
        "                j = i + 1\n",
        "                while (j < len(tokens) and\n",
        "                       j < len(model_labels) and\n",
        "                       model_labels[j] == f'I-{entity_type}'):\n",
        "                    entity_tokens.append(tokens[j])\n",
        "                    j += 1\n",
        "                entity_text = ''.join(entity_tokens)\n",
        "                is_valid = validate_entity_format(entity_text, entity_type)\n",
        "                valid_status = \"âœ“\" if is_valid else \"âœ—\"\n",
        "            else:\n",
        "                valid_status = \"-\"\n",
        "\n",
        "            print(f\"{token:<15} {model_label:<10} {validated_label:<12} {valid_status}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Analiz xÉ™tasÄ±: {e}\")\n",
        "\n",
        "def interactive_blur_system():\n",
        "    \"\"\"\n",
        "    EN: Interactive NER blur system with validation rules\n",
        "    AZ: Ä°nteraktiv blurring sistemi\n",
        "    \"\"\"\n",
        "    print(\"ğŸ¤– AzÉ™rbaycan NER Blur Sistemi\")\n",
        "    print(\"Model + Validation qaydalarÄ± istifadÉ™ edilir\")\n",
        "    print(\"Format qaydalarÄ±:\")\n",
        "    print(\"  â€¢ FIN: 7 simvol (hÉ™rf+rÉ™qÉ™m)\")\n",
        "    print(\"  â€¢ ID/AA: 9 simvol (AA + 7 rÉ™qÉ™m)\")\n",
        "    print(\"  â€¢ ID/AZE: 12 simvol (AZE + 9 rÉ™qÉ™m)\")\n",
        "    print(\"  â€¢ Avtomobil: XX-YY-ZZZ\")\n",
        "    print(\"\\nÃ‡Ä±xmaq Ã¼Ã§Ã¼n 'exit' yazÄ±n\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"ğŸ“ MÉ™tn daxil edin: \")\n",
        "\n",
        "        if user_input.lower() in ['exit', 'Ã§Ä±x', 'quit', 'q']:\n",
        "            print(\"ğŸ‘‹ GÃ¶rÃ¼ÅŸÉ™nÉ™dÉ™k!\")\n",
        "            break\n",
        "\n",
        "        if user_input.strip():\n",
        "            try:\n",
        "                blurred_text, entities = blur_with_model_and_validation(user_input, model, tokenizer, device)\n",
        "                print(f\"ğŸ”’ Blurred: {blurred_text}\")\n",
        "\n",
        "                if entities:\n",
        "                    print(\"ğŸ“ TapÄ±lan entity-lÉ™r:\")\n",
        "                    for entity in entities:\n",
        "                        print(f\"  â€¢ '{entity['text']}' â†’ {entity['type']}\")\n",
        "                else:\n",
        "                    print(\"ğŸ“ HeÃ§ bir entity tapÄ±lmadÄ±\")\n",
        "\n",
        "                # EN: Show detailed analysis\n",
        "                # AZ: Detailed analizi gÃ¶stÉ™r\n",
        "                detailed_analysis_demo(user_input)\n",
        "                print()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ XÉ™ta: {e}\\n\")\n",
        "        else:\n",
        "            print(\"âŒ BoÅŸ mÉ™tn daxil etdiniz\\n\")\n",
        "\n",
        "# EN: If model is trained and available, run demo\n",
        "# AZ: ÆgÉ™r model Ã¶yrÉ™dilib vÉ™ mÃ¶vcuddursa test et\n",
        "if 'model' in locals():\n",
        "    print(\"ğŸ¯ Model test edilir...\")\n",
        "    test_enhanced_model()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Ä°nteraktiv sistemÉ™ keÃ§mÉ™k istÉ™yirsinizmi? (y/n)\")\n",
        "    interactive_blur_system()\n",
        "else:\n",
        "    print(\"âŒ Model É™vvÉ™lcÉ™ Ã¶yrÉ™dilmÉ™lidir!\")  # EN: Model must be trained first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oPeoA71FU8P",
        "outputId": "e4c0499e-16ad-4941-9feb-3c4c9b206c65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/best_model/ (stored 0%)\n",
            "  adding: content/best_model/model.safetensors (deflated 7%)\n",
            "  adding: content/best_model/config.json (deflated 55%)\n",
            "  adding: content/best_model/training_args.bin (deflated 53%)\n",
            "  adding: content/best_model/vocab.txt (deflated 45%)\n",
            "  adding: content/best_model/special_tokens_map.json (deflated 42%)\n",
            "  adding: content/best_model/tokenizer.json (deflated 67%)\n",
            "  adding: content/best_model/tokenizer_config.json (deflated 75%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r /content/best_model.zip /content/best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21rl67yjMB6X"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}