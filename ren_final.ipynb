{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bahramzada/az-ner-blur/blob/main/ren_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXsru22SSgBW",
        "outputId": "e49c8bc9-a92e-461a-8a0b-950f92a13e84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NÃ¼munÉ™ cÃ¼mlÉ™lÉ™r:\n",
            "1. TÉ™hlÃ¼kÉ™sizlik É™mÉ™kdaÅŸÄ± 85-NL-695 nÃ¶mrÉ™sini soruÅŸdu.\n",
            "2. MÉ™n 95-MA-819 nÃ¶mrÉ™sini gÃ¶rdÃ¼m.\n",
            "3. Avtomobil 54-GY-552 nÃ¶mrÉ™si qara maÅŸÄ±nda yazÄ±lÄ±b.\n",
            "4. 66-KO-487 nÃ¶mrÉ™li maÅŸÄ±n Ã§ox bÃ¶yÃ¼kdÃ¼r.\n",
            "5. Avtomobil nÃ¶mrÉ™si 85-IZ-670 qeydiyyatdan keÃ§di.\n",
            "6. Bu qÉ™za ilÉ™ baÄŸlÄ± 11-SE-498 nÃ¶mrÉ™li avtomobilin adÄ± hallanÄ±r.\n",
            "7. MÉ™lumat bazasÄ±nda 50-LE-457 nÃ¶mrÉ™si ilÉ™ axtarÄ±ÅŸ aparÄ±ldÄ±.\n",
            "8. O, Ã¶z avtomobilinin nÃ¶mrÉ™sini, 28-TW-112 nÃ¶mrÉ™sini xatÄ±rladÄ±.\n",
            "9. Avtomobilin nÃ¶mrÉ™si 80-EL-861 vÉ™ rÉ™ngi aÄŸdÄ±r.\n",
            "10. DÃ¼nÉ™n 57-MV-270 nÃ¶mrÉ™li avtomobili gÃ¶rdÃ¼m.\n",
            "\n",
            "==================================================\n",
            "CÉ™mi 10000 cÃ¼mlÉ™ yaradÄ±ldÄ±.\n",
            "10000 cÃ¼mlÉ™ car_plate_dataset.txt faylÄ±nda saxlanÄ±ldÄ±.\n",
            "CSV formatÄ±nda da car_plate_dataset.csv faylÄ±nda saxlanÄ±ldÄ±.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "def generate_car_plate():\n",
        "    \"\"\"99-XX-999 formatÄ±nda avtomobil nÃ¶mrÉ™si yaradÄ±r\"\"\"\n",
        "    # Ä°lk iki rÉ™qÉ™m\n",
        "    first_digits = f\"{random.randint(10, 99)}\"\n",
        "\n",
        "    # Ä°ki hÉ™rf\n",
        "    letters = ''.join(random.choices(string.ascii_uppercase, k=2))\n",
        "\n",
        "    # Son Ã¼Ã§ rÉ™qÉ™m\n",
        "    last_digits = f\"{random.randint(100, 999)}\"\n",
        "\n",
        "    return f\"{first_digits}-{letters}-{last_digits}\"\n",
        "\n",
        "def generate_sentences_with_car_plates(num_sentences=1000):\n",
        "    \"\"\"Avtomobil nÃ¶mrÉ™lÉ™ri olan cÃ¼mlÉ™lÉ™r yaradÄ±r\"\"\"\n",
        "\n",
        "    # MÃ¼xtÉ™lif cÃ¼mlÉ™ ÅŸablonlarÄ±\n",
        "    sentence_templates = [\n",
        "        \"Avtomobil nÃ¶mrÉ™si {} olan maÅŸÄ±n yolda gedirdi.\",\n",
        "        \"{} nÃ¶mrÉ™li avtomobil sÃ¼rÉ™tlÉ™ keÃ§di.\",\n",
        "        \"MÉ™n {} nÃ¶mrÉ™sini gÃ¶rdÃ¼m.\",\n",
        "        \"{} nÃ¶mrÉ™li maÅŸÄ±n dayanacaqda idi.\",\n",
        "        \"Polis {} nÃ¶mrÉ™li avtomobili dayandÄ±rdÄ±.\",\n",
        "        \"Bu gÃ¼n {} nÃ¶mrÉ™sini qeyd etdim.\",\n",
        "        \"{} nÃ¶mrÉ™li avtomobil qÄ±rmÄ±zÄ± iÅŸÄ±qda dayandÄ±.\",\n",
        "        \"QonÅŸumun avtomobil nÃ¶mrÉ™si {}dÄ±r.\",\n",
        "        \"{} nÃ¶mrÉ™li maÅŸÄ±n Ã§ox sÃ¼rÉ™tlÉ™ gedirdi.\",\n",
        "        \"Avtomobil {} nÃ¶mrÉ™si ilÉ™ qeydiyyatdan keÃ§ib.\",\n",
        "        \"MÉ™n {} nÃ¶mrÉ™li avtomobili tanÄ±yÄ±ram.\",\n",
        "        \"{} nÃ¶mrÉ™sindÉ™ olan maÅŸÄ±n aÄŸ rÉ™ngdÉ™dir.\",\n",
        "        \"DÃ¼nÉ™n {} nÃ¶mrÉ™li avtomobili gÃ¶rdÃ¼m.\",\n",
        "        \"Bu {} nÃ¶mrÉ™li maÅŸÄ±n kimÉ™ mÉ™xsusdur?\",\n",
        "        \"{} nÃ¶mrÉ™li avtomobil yeni alÄ±nÄ±b.\",\n",
        "        \"Parklama yerindÉ™ {} nÃ¶mrÉ™si var idi.\",\n",
        "        \"{} nÃ¶mrÉ™li maÅŸÄ±n tÉ™mirÉ™ ehtiyacÄ± var.\",\n",
        "        \"Avtomobil nÃ¶mrÉ™si {} olan sÃ¼rÃ¼cÃ¼ tÉ™crÃ¼bÉ™lidir.\",\n",
        "        \"MÉ™nim dostumun avtomobil nÃ¶mrÉ™si {}dÄ±r.\",\n",
        "        \"{} nÃ¶mrÉ™li avtomobil bazarda satÄ±lÄ±r.\",\n",
        "        \"Bu sÉ™hÉ™r {} nÃ¶mrÉ™sini yolda gÃ¶rdÃ¼m.\",\n",
        "        \"{} nÃ¶mrÉ™li maÅŸÄ±n Ã§ox bÃ¶yÃ¼kdÃ¼r.\",\n",
        "        \"Avtomobil {} nÃ¶mrÉ™si qara maÅŸÄ±nda yazÄ±lÄ±b.\",\n",
        "        \"MÉ™n {} nÃ¶mrÉ™sini unutmuÅŸdum.\",\n",
        "        \"{} nÃ¶mrÉ™li avtomobil hÉ™ftÉ™sonu istifadÉ™ olunur.\",\n",
        "        \"Bu {} avtomobil nÃ¶mrÉ™si Ã§ox maraqlÄ±dÄ±r.\",\n",
        "        \"{} nÃ¶mrÉ™li maÅŸÄ±n É™la vÉ™ziyyÉ™tdÉ™dir.\",\n",
        "        \"Avtomobil nÃ¶mrÉ™si {} yaddaÅŸÄ±mda qalÄ±b.\",\n",
        "        \"DÃ¼nÉ™n axÅŸam {} nÃ¶mrÉ™li avtomobil gÉ™ldi.\",\n",
        "        \"{} nÃ¶mrÉ™sini polisÉ™ bildirdim.\",\n",
        "        \"HadisÉ™ yerindÉ™n {} nÃ¶mrÉ™li avtomobil uzaqlaÅŸdÄ±.\",\n",
        "        \"ÅÉ™hÉ™r kameralarÄ± {} nÃ¶mrÉ™li maÅŸÄ±nÄ± qeydÉ™ aldÄ±.\",\n",
        "        \"{} nÃ¶mrÉ™si ilÉ™ icarÉ™yÉ™ gÃ¶tÃ¼rÃ¼lÉ™n avtomobil qaytarÄ±ldÄ±.\",\n",
        "        \"TÉ™hlÃ¼kÉ™sizlik É™mÉ™kdaÅŸÄ± {} nÃ¶mrÉ™sini soruÅŸdu.\",\n",
        "        \"Bu avtomobilin nÃ¶mrÉ™si {} olaraq qeyd edilib.\",\n",
        "        \"Avtomobilin texniki baxÄ±ÅŸÄ± {} nÃ¶mrÉ™sinÉ™ uyÄŸundur.\",\n",
        "        \"{} nÃ¶mrÉ™li maÅŸÄ±n yol kÉ™narÄ±nda saxlanÄ±lÄ±b.\",\n",
        "        \"MÉ™lumat bazasÄ±nda {} nÃ¶mrÉ™si ilÉ™ axtarÄ±ÅŸ aparÄ±ldÄ±.\",\n",
        "        \"O, Ã¶z avtomobilinin nÃ¶mrÉ™sini, {} nÃ¶mrÉ™sini xatÄ±rladÄ±.\",\n",
        "        \"{} nÃ¶mrÉ™li avtomobilin sahibi kimdir?\",\n",
        "        \"KÃ¶mÉ™k Ã¼Ã§Ã¼n {} nÃ¶mrÉ™li maÅŸÄ±n Ã§aÄŸÄ±rÄ±ldÄ±.\",\n",
        "        \"Avtomobil nÃ¶mrÉ™si {} qeydiyyatdan keÃ§di.\",\n",
        "        \"NÉ™qliyyatÄ±n hÉ™rÉ™kÉ™tini izlÉ™mÉ™k Ã¼Ã§Ã¼n {} nÃ¶mrÉ™sindÉ™n istifadÉ™ edildi.\",\n",
        "        \"GÃ¶mrÃ¼kdÉ™ {} nÃ¶mrÉ™li avtomobil yoxlanÄ±ldÄ±.\",\n",
        "        \"Bu qÉ™za ilÉ™ baÄŸlÄ± {} nÃ¶mrÉ™li avtomobilin adÄ± hallanÄ±r.\",\n",
        "        \"{} nÃ¶mrÉ™li maÅŸÄ±nÄ±n tÉ™kÉ™rlÉ™ri dÉ™yiÅŸdirildi.\",\n",
        "        \"Avtomobilin nÃ¶mrÉ™si {} vÉ™ rÉ™ngi aÄŸdÄ±r.\",\n",
        "        \"Bu avtomobilin nÃ¶mrÉ™si {} olaraq dÉ™yiÅŸdirilÉ™cÉ™k.\",\n",
        "        \"TÉ™dbirdÉ™ iÅŸtirak edÉ™n hÉ™r bir avtomobilin nÃ¶mrÉ™si, o cÃ¼mlÉ™dÉ™n {} qeyd olundu.\"\n",
        "    ]\n",
        "    sentences = []\n",
        "\n",
        "    for _ in range(num_sentences):\n",
        "        # TÉ™sadÃ¼fi ÅŸablon seÃ§\n",
        "        template = random.choice(sentence_templates)\n",
        "\n",
        "        # Avtomobil nÃ¶mrÉ™si yarat\n",
        "        car_plate = generate_car_plate()\n",
        "\n",
        "        # CÃ¼mlÉ™ni yaradÄ±n\n",
        "        sentence = template.format(car_plate)\n",
        "        sentences.append(sentence)\n",
        "\n",
        "    return sentences\n",
        "\n",
        "def save_dataset(sentences, filename=\"car_plate_dataset.txt\"):\n",
        "    \"\"\"Dataset-i fayla saxlayÄ±r\"\"\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        for sentence in sentences:\n",
        "            f.write(sentence + '\\n')\n",
        "    print(f\"{len(sentences)} cÃ¼mlÉ™ {filename} faylÄ±nda saxlanÄ±ldÄ±.\")\n",
        "\n",
        "def main():\n",
        "    # 1000 cÃ¼mlÉ™ yaradÄ±n\n",
        "    sentences = generate_sentences_with_car_plates(10000)\n",
        "\n",
        "    # Ä°lk 10 cÃ¼mlÉ™ni gÃ¶stÉ™r\n",
        "    print(\"NÃ¼munÉ™ cÃ¼mlÉ™lÉ™r:\")\n",
        "    for i, sentence in enumerate(sentences[:10], 1):\n",
        "        print(f\"{i}. {sentence}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"CÉ™mi {len(sentences)} cÃ¼mlÉ™ yaradÄ±ldÄ±.\")\n",
        "\n",
        "    # Fayla saxla\n",
        "    save_dataset(sentences)\n",
        "\n",
        "    # CSV formatÄ±nda da saxla\n",
        "    save_dataset_csv(sentences)\n",
        "\n",
        "def save_dataset_csv(sentences, filename=\"car_plate_dataset.csv\"):\n",
        "    \"\"\"Dataset-i CSV formatÄ±nda saxlayÄ±r\"\"\"\n",
        "    import csv\n",
        "    import re\n",
        "\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['sentence', 'car_plate'])\n",
        "\n",
        "        for sentence in sentences:\n",
        "            # Avtomobil nÃ¶mrÉ™sini cÃ¼mlÉ™dan Ã§Ä±xarÄ±n\n",
        "            car_plate_pattern = r'\\d{2}-[A-Z]{2}-\\d{3}'\n",
        "            match = re.search(car_plate_pattern, sentence)\n",
        "            if match:\n",
        "                car_plate = match.group()\n",
        "                writer.writerow([sentence, car_plate])\n",
        "\n",
        "    print(f\"CSV formatÄ±nda da {filename} faylÄ±nda saxlanÄ±ldÄ±.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRCH-RXwSf_V",
        "outputId": "0a87f736-d25f-449f-dba2-96240c0d0f08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV fayl adÄ±nÄ± daxil edin (mÉ™s: sentences.csv): \n",
            "10000 cÃ¼mlÉ™ annotate edildi vÉ™ /content/car_plate_dataset_annotated.json faylÄ±nda saxlanÄ±ldÄ±.\n",
            "\n",
            "NÃ¼munÉ™ 5 annotated cÃ¼mlÉ™:\n",
            "================================================================================\n",
            "1. TÉ™hlÃ¼kÉ™sizlik É™mÉ™kdaÅŸÄ± 85-NL-695 nÃ¶mrÉ™sini soruÅŸdu.\n",
            "   -> TapÄ±lan: '85-NL-695' (mÃ¶vqe: 23-32)\n",
            "----------------------------------------\n",
            "2. MÉ™n 95-MA-819 nÃ¶mrÉ™sini gÃ¶rdÃ¼m.\n",
            "   -> TapÄ±lan: '95-MA-819' (mÃ¶vqe: 4-13)\n",
            "----------------------------------------\n",
            "3. Avtomobil 54-GY-552 nÃ¶mrÉ™si qara maÅŸÄ±nda yazÄ±lÄ±b.\n",
            "   -> TapÄ±lan: '54-GY-552' (mÃ¶vqe: 10-19)\n",
            "----------------------------------------\n",
            "4. 66-KO-487 nÃ¶mrÉ™li maÅŸÄ±n Ã§ox bÃ¶yÃ¼kdÃ¼r.\n",
            "   -> TapÄ±lan: '66-KO-487' (mÃ¶vqe: 0-9)\n",
            "----------------------------------------\n",
            "5. Avtomobil nÃ¶mrÉ™si 85-IZ-670 qeydiyyatdan keÃ§di.\n",
            "   -> TapÄ±lan: '85-IZ-670' (mÃ¶vqe: 18-27)\n",
            "----------------------------------------\n",
            "\n",
            "âœ… UÄŸurla tamamlandÄ±!\n",
            "ğŸ“ Input: /content/car_plate_dataset.csv\n",
            "ğŸ“ Output: /content/car_plate_dataset_annotated.json\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import json\n",
        "import re\n",
        "\n",
        "def find_car_plate_positions(text):\n",
        "    \"\"\"MÉ™tndÉ™ avtomobil nÃ¶mrÉ™lÉ™rinin mÃ¶vqelÉ™rini tapÄ±r\"\"\"\n",
        "    car_plate_pattern = r'\\d{2}-[A-Z]{2}-\\d{3}'\n",
        "    entities = []\n",
        "\n",
        "    for match in re.finditer(car_plate_pattern, text):\n",
        "        start_pos = match.start()\n",
        "        end_pos = match.end()\n",
        "        entities.append([start_pos, end_pos, \"CAR_PLATE\"])\n",
        "\n",
        "    return entities\n",
        "\n",
        "def annotate_csv_file(input_csv_file, output_json_file):\n",
        "    \"\"\"CSV faylÄ±nÄ± oxuyub JSON formatÄ±nda annotate edir\"\"\"\n",
        "    annotated_data = []\n",
        "\n",
        "    # CSV faylÄ±nÄ± oxu\n",
        "    with open(input_csv_file, 'r', encoding='utf-8') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "\n",
        "        # Header-i atla (É™gÉ™r varsa)\n",
        "        try:\n",
        "            first_row = next(reader)\n",
        "            # ÆgÉ™r ilk sÉ™tir header deyilsÉ™, onu da emal et\n",
        "            if not (first_row[0].lower() in ['sentence', 'text', 'cÃ¼mle']):\n",
        "                sentence = first_row[0]\n",
        "                entities = find_car_plate_positions(sentence)\n",
        "                annotation = {\"entities\": entities}\n",
        "                annotated_data.append([sentence, annotation])\n",
        "        except StopIteration:\n",
        "            print(\"CSV faylÄ± boÅŸdur!\")\n",
        "            return\n",
        "\n",
        "        # Qalan sÉ™tirlÉ™ri emal et\n",
        "        for row in reader:\n",
        "            if row:  # BoÅŸ sÉ™tirlÉ™ri atla\n",
        "                sentence = row[0]  # Ä°lk sÃ¼tunu cÃ¼mlÉ™ kimi gÃ¶tÃ¼r\n",
        "                entities = find_car_plate_positions(sentence)\n",
        "                annotation = {\"entities\": entities}\n",
        "                annotated_data.append([sentence, annotation])\n",
        "\n",
        "    # JSON faylÄ±nda saxla\n",
        "    with open(output_json_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(annotated_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"{len(annotated_data)} cÃ¼mlÉ™ annotate edildi vÉ™ {output_json_file} faylÄ±nda saxlanÄ±ldÄ±.\")\n",
        "\n",
        "    return annotated_data\n",
        "\n",
        "def print_sample_results(data, num_samples=5):\n",
        "    \"\"\"NÃ¼munÉ™ nÉ™ticÉ™lÉ™ri gÃ¶stÉ™r\"\"\"\n",
        "    print(f\"\\nNÃ¼munÉ™ {min(num_samples, len(data))} annotated cÃ¼mlÉ™:\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for i, (sentence, annotation) in enumerate(data[:num_samples]):\n",
        "        print(f\"{i+1}. {sentence}\")\n",
        "        if annotation['entities']:\n",
        "            for entity in annotation['entities']:\n",
        "                start, end, label = entity\n",
        "                car_plate = sentence[start:end]\n",
        "                print(f\"   -> TapÄ±lan: '{car_plate}' (mÃ¶vqe: {start}-{end})\")\n",
        "        else:\n",
        "            print(\"   -> Avtomobil nÃ¶mrÉ™si tapÄ±lmadÄ±\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "def main():\n",
        "    # CSV fayl adÄ±nÄ± daxil edin\n",
        "    input_csv = input(\"CSV fayl adÄ±nÄ± daxil edin (mÉ™s: sentences.csv): \").strip()\n",
        "    if not input_csv:\n",
        "        input_csv = \"/content/car_plate_dataset.csv\"\n",
        "\n",
        "    # Output JSON fayl adÄ±\n",
        "    output_json = input_csv.replace('.csv', '_annotated.json')\n",
        "\n",
        "    try:\n",
        "        # CSV-ni annotate et\n",
        "        data = annotate_csv_file(input_csv, output_json)\n",
        "\n",
        "        # NÃ¼munÉ™ nÉ™ticÉ™lÉ™ri gÃ¶stÉ™r\n",
        "        print_sample_results(data)\n",
        "\n",
        "        print(f\"\\nâœ… UÄŸurla tamamlandÄ±!\")\n",
        "        print(f\"ğŸ“ Input: {input_csv}\")\n",
        "        print(f\"ğŸ“ Output: {output_json}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ XÆTA: '{input_csv}' faylÄ± tapÄ±lmadÄ±!\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ XÆTA: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UONzBJzpSf9J"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def random_aze_id():\n",
        "    # AZE + 7 vÉ™ ya 9 rÉ™qÉ™m (random seÃ§im)\n",
        "    digits = ''.join([str(random.randint(0, 9)) for _ in range(9)])\n",
        "    return \"AZE\" + digits\n",
        "\n",
        "def random_aa_id():\n",
        "    # AA + 7 rÉ™qÉ™m\n",
        "    digits = ''.join([str(random.randint(0, 9)) for _ in range(7)])\n",
        "    return \"AA\" + digits\n",
        "\n",
        "templates = [\n",
        "    \"MÉ™nim ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™m {}-dir.\",\n",
        "    \"ÅÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si {}-dir.\",\n",
        "    \"VÉ™siqÉ™ nÃ¶mrÉ™m {}-dir.\",\n",
        "    \"ZÉ™hmÉ™t olmasa, ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™nizi qeyd edin: {}\",\n",
        "    \"AdÄ±: Bahram Zada, VÉ™siqÉ™ nÃ¶mrÉ™si: {}\",\n",
        "    \"ÅÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si: {}\",\n",
        "    \"MÉ™nim {} ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™mdir.\",\n",
        "    \"ÅÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™sini yoxlamaq Ã¼Ã§Ã¼n {} daxil edin.\",\n",
        "    \"VÉ™siqÉ™ nÃ¶mrÉ™si olmadan qeydiyyat mÃ¼mkÃ¼n deyil: {}\",\n",
        "    \"Sizin ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™niz {}-dÃ¼r?\",\n",
        "    \"Qeydiyyat Ã¼Ã§Ã¼n ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si tÉ™lÉ™b olunur: {}\",\n",
        "    \"Ä°stifadÉ™Ã§inin ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si: {}\",\n",
        "    \"AÅŸaÄŸÄ±da gÃ¶stÉ™rilÉ™n vÉ™siqÉ™ nÃ¶mrÉ™sini yoxlayÄ±n: {}\",\n",
        "    \"SistemÉ™ giriÅŸ Ã¼Ã§Ã¼n {} vÉ™siqÉ™ nÃ¶mrÉ™sini daxil edin.\",\n",
        "    \"SÉ™nÉ™d mÉ™lumatlarÄ±: ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si - {}\",\n",
        "    \"{} ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™sini tÉ™sdiqlÉ™yin.\",\n",
        "    \"YuxarÄ±da qeyd olunan {} vÉ™siqÉ™ nÃ¶mrÉ™sidir.\",\n",
        "    \"ÆlavÉ™ mÉ™lumat Ã¼Ã§Ã¼n {} ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™sini istifadÉ™ edin.\",\n",
        "    \"ÅÉ™xsiyyÉ™tinizi tÉ™sdiqlÉ™mÉ™k Ã¼Ã§Ã¼n {} nÃ¶mrÉ™sini yazÄ±n.\",\n",
        "    \"ÆgÉ™r ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™niz {}-dirsÉ™, davam edin.\",\n",
        "    \"Qeydiyyat zamanÄ± istifadÉ™ etdiyiniz vÉ™siqÉ™ nÃ¶mrÉ™si: {}\",\n",
        "    \"ÅÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™niz bir daha tÉ™sdiqlÉ™nir: {}\",\n",
        "    \"Formada yazÄ±lan ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si {}-dir.\",\n",
        "    \"SizdÉ™n tÉ™lÉ™b olunan ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si: {}\",\n",
        "    \"AÅŸaÄŸÄ±da gÃ¶stÉ™rilÉ™n {} nÃ¶mrÉ™si sizin vÉ™siqÉ™nizdir.\",\n",
        "    \"ProfilinizdÉ™ qeyd olunan ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si: {}\",\n",
        "    \"ÅÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si olmadan qeydiyyat mÃ¼mkÃ¼n deyil, nÃ¶mrÉ™niz: {}\",\n",
        "    \"SistemÉ™ daxil olmaq Ã¼Ã§Ã¼n ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™nizi {} daxil edin.\",\n",
        "    \"Qeydiyyat formasÄ±nda {} ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™sini yazÄ±n.\",\n",
        "    \"MÉ™lumat bazasÄ±nda saxlanÄ±lan vÉ™siqÉ™ nÃ¶mrÉ™si: {}\",\n",
        "    \"MÃ¼raciÉ™t Ã¼Ã§Ã¼n {} ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si vacibdir.\",\n",
        "    \"YalnÄ±z {} nÃ¶mrÉ™si olan ÅŸÉ™xs xidmÉ™tdÉ™n yararlana bilÉ™r.\",\n",
        "    \"{} ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™si ilÉ™ É™laqÉ™li sÉ™nÉ™dlÉ™r qÉ™bul edildi.\",\n",
        "    \"Ã–dÉ™niÅŸi etmÉ™k Ã¼Ã§Ã¼n {} ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™nizi gÃ¶stÉ™rin.\",\n",
        "    \"ÆrizÉ™yÉ™ {} vÉ™siqÉ™ nÃ¶mrÉ™si ilÉ™ mÃ¼raciÉ™t edÉ™ bilÉ™rsiniz.\",\n",
        "    \"HÉ™r hansÄ± bir dÉ™yiÅŸiklik Ã¼Ã§Ã¼n {} nÃ¶mrÉ™si tÉ™lÉ™b olunur.\",\n",
        "    \"{} nÃ¶mrÉ™si yoxlandÄ± vÉ™ tÉ™sdiq edildi.\",\n",
        "    \"Bu hesabÄ±n sahibi {} ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™sinÉ™ malikdir.\",\n",
        "    \"VÉ™siqÉ™nin Ã¼zÉ™rindÉ™ {} nÃ¶mrÉ™si qeyd olunub.\",\n",
        "    \"{} nÃ¶mrÉ™si ilÉ™ baÄŸlÄ± bÃ¼tÃ¼n mÉ™lumatlar doÄŸrudur.\",\n",
        "    \"GiriÅŸ Ã¼Ã§Ã¼n {} ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™ nÃ¶mrÉ™sini yenidÉ™n daxil edin.\",\n",
        "    \"LÃ¼tfÉ™n, {} nÃ¶mrÉ™sini É™lavÉ™ edin.\",\n",
        "    \"Bu ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™si nÃ¶mrÉ™si, {} yeni verilmiÅŸdir.\",\n",
        "    \"SÄ±ÄŸorta É™mÉ™liyyatÄ± {} nÃ¶mrÉ™sinÉ™ É™saslanÄ±r.\",\n",
        "    \"TÉ™sdiqlÉ™mÉ™ kodu {} vÉ™siqÉ™ nÃ¶mrÉ™nizÉ™ gÃ¶ndÉ™rildi.\",\n",
        "    \"{} vÉ™siqÉ™ nÃ¶mrÉ™si mÉ™lumat bazasÄ±na daxil edilmiÅŸdir.\",\n",
        "    \"XidmÉ™t haqqÄ±nÄ± Ã¶dÉ™mÉ™k Ã¼Ã§Ã¼n {} nÃ¶mrÉ™si lazÄ±mdÄ±r.\",\n",
        "    \"SistemdÉ™ {} nÃ¶mrÉ™si ilÉ™ baÄŸlÄ± heÃ§ bir qeyd tapÄ±lmadÄ±.\",\n",
        "    \"Ã–dÉ™niÅŸin tÉ™sdiqi Ã¼Ã§Ã¼n {} nÃ¶mrÉ™sini gÃ¶stÉ™rin.\",\n",
        "    \"{} nÃ¶mrÉ™si ilÉ™ baÄŸlÄ± bÃ¼tÃ¼n mÉ™lumatlar qorunur.\"\n",
        "]\n",
        "\n",
        "def generate_sentences(n=1000, aze_ratio=0.7):\n",
        "    sentences = []\n",
        "    for _ in range(n):\n",
        "        template = random.choice(templates)\n",
        "        # 70% AZE formatÄ±, 30% AA formatÄ±\n",
        "        if random.random() < aze_ratio:\n",
        "            id_num = random_aze_id()\n",
        "        else:\n",
        "            id_num = random_aa_id()\n",
        "        sentence = template.format(id_num)\n",
        "        sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sentences = generate_sentences(10000)\n",
        "    with open(\"vesiqe_numune.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for sentence in sentences:\n",
        "            f.write(sentence + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOJvkk6tSf7Q"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "# Regex patternlÉ™ri\n",
        "pattern_aze = r'AZE\\d{9}'         # AZE + 9 rÉ™qÉ™m\n",
        "pattern_aa = r'AA\\d{7}'           # AA + 7 rÉ™qÉ™m\n",
        "\n",
        "label = \"ID_NUMBER\"\n",
        "\n",
        "def label_sentences(sentences):\n",
        "    labeled_data = []\n",
        "    for sentence in sentences:\n",
        "        entities = []\n",
        "        # AZE Ã¼Ã§Ã¼n\n",
        "        for match in re.finditer(pattern_aze, sentence):\n",
        "            start, end = match.start(), match.end()\n",
        "            entities.append((start, end, label))\n",
        "        # AA Ã¼Ã§Ã¼n\n",
        "        for match in re.finditer(pattern_aa, sentence):\n",
        "            start, end = match.start(), match.end()\n",
        "            entities.append((start, end, label))\n",
        "        if entities:\n",
        "            labeled_data.append((sentence, {\"entities\": entities}))\n",
        "    return labeled_data\n",
        "\n",
        "# Fayldan cÃ¼mlÉ™lÉ™ri oxu\n",
        "with open(\"vesiqe_numune.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    sentences = [line.strip() for line in f.readlines()]\n",
        "\n",
        "labeled_data = label_sentences(sentences)\n",
        "\n",
        "# Annotasiya formatlÄ± JSON-a yaz\n",
        "with open(\"vesiqe_annotated.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(labeled_data, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RW8pp1cISf5N"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def random_fin():\n",
        "    # Ä°cazÉ™li hÉ™rflÉ™r: Ä°ngilis É™lifbasÄ±, amma \"I\" vÉ™ \"O\" olmamalÄ±dÄ±r\n",
        "    letters = [chr(c) for c in range(ord('A'), ord('Z')+1) if chr(c) not in ['I', 'O']]\n",
        "    digits = [str(d) for d in range(10)]\n",
        "    allowed = letters + digits\n",
        "    fin = ''.join(random.choice(allowed) for _ in range(7))\n",
        "    return fin\n",
        "\n",
        "fin_templates = [\n",
        "    \"MÉ™nim FÄ°N kodum {}-dur.\",\n",
        "    \"FÄ°N kodu: {}\",\n",
        "    \"ZÉ™hmÉ™t olmasa, FÄ°N kodunuzu daxil edin: {}\",\n",
        "    \"Sizin FÄ°N kodunuz {}-dir?\",\n",
        "    \"Qeydiyyat Ã¼Ã§Ã¼n tÉ™lÉ™b olunan FÄ°N kodu: {}\",\n",
        "    \"Formada yazÄ±lan FÄ°N kodu {}-dir.\",\n",
        "    \"ÆlavÉ™ mÉ™lumat Ã¼Ã§Ã¼n FÄ°N kodu: {}\",\n",
        "    \"ProfilinizdÉ™ qeyd olunan FÄ°N kodu: {}\",\n",
        "    \"FÄ°N kodunuzu yoxlamaq Ã¼Ã§Ã¼n {} daxil edin.\",\n",
        "    \"FÄ°N kodu olmadan qeydiyyat mÃ¼mkÃ¼n deyil: {}\",\n",
        "    \"SistemdÉ™ qeydiyyatdan keÃ§mÉ™k Ã¼Ã§Ã¼n FÄ°N kodunuzu daxil edin: {}\",\n",
        "    \"Sizin ÅŸÉ™xsiyyÉ™tinizi tÉ™sdiqlÉ™yÉ™n FÄ°N kod: {}\",\n",
        "    \"FÄ°N kodunuz bir daha tÉ™sdiqlÉ™nir: {}\",\n",
        "    \"AÅŸaÄŸÄ±da gÃ¶stÉ™rilÉ™n FÄ°N kodunu yoxlayÄ±n: {}\",\n",
        "    \"SÉ™nÉ™d mÉ™lumatlarÄ±: FÄ°N kodu - {}\",\n",
        "    \"Qeydiyyat formasÄ±nda {} FÄ°N kodunu yazÄ±n.\",\n",
        "    \"MÉ™lumat bazasÄ±nda saxlanÄ±lan FÄ°N kodu: {}\",\n",
        "    \"SizdÉ™n tÉ™lÉ™b olunan FÄ°N kodu: {}\",\n",
        "    \"FÄ°N kodu olmadan É™mÉ™liyyat davam etmir: {}\",\n",
        "    \"SistemÉ™ giriÅŸ Ã¼Ã§Ã¼n {} FÄ°N kodunu daxil edin.\",\n",
        "    \"SÉ™nÉ™din Ã¼zÉ™rindÉ™ yazÄ±lmÄ±ÅŸ FÄ°N kodu {}-dir.\",\n",
        "    \"FÄ°N kodunuzu tÉ™sdiqlÉ™yin: {}\",\n",
        "    \"FÄ°N kodu sahÉ™sinÉ™ {} yazÄ±n.\",\n",
        "    \"ÅÉ™xsiyyÉ™ti tÉ™sdiqlÉ™mÉ™k Ã¼Ã§Ã¼n {} FÄ°N kodunu daxil edin.\",\n",
        "    \"Sizin Ã¼Ã§Ã¼n yaradÄ±lmÄ±ÅŸ FÄ°N kodu: {}\",\n",
        "    \"SÉ™nÉ™dÉ™ É™lavÉ™ olunan FÄ°N kodu: {}\",\n",
        "    \"FÄ°N kodu tÉ™lÉ™b olunduqda {} tÉ™qdim edin.\",\n",
        "    \"SistemdÉ™ mÃ¶vcud FÄ°N kodu: {}\",\n",
        "    \"FÄ°N kodunu dÉ™yiÅŸmÉ™k Ã¼Ã§Ã¼n kÃ¶hnÉ™ kod: {}\",\n",
        "    \"ÆgÉ™r FÄ°N kodunuz {}-dirsÉ™, davam edin.\",\n",
        "    \"FÄ°N kodunuzu unutmusunuzsa, yeni kod alÄ±n: {}\",\n",
        "    \"TÉ™sdiqlÉ™nmiÅŸ FÄ°N kodu: {}\",\n",
        "    \"Yeni qeydiyyat Ã¼Ã§Ã¼n FÄ°N kodu: {}\",\n",
        "    \"FÄ°N kodu olmadan qeydiyyat mÃ¼mkÃ¼n deyil, kodunuz: {}\",\n",
        "    \"AÅŸaÄŸÄ±da gÃ¶stÉ™rilÉ™n {} kodu sizin FÄ°N kodunuzdur.\",\n",
        "    \"FÄ°N kodu ilÉ™ baÄŸlÄ± sualÄ±nÄ±z varsa, kod: {}\",\n",
        "    \"MÉ™lumat formasÄ±nda FÄ°N kodu: {}\",\n",
        "    \"SistemÉ™ daxil olmaq Ã¼Ã§Ã¼n FÄ°N kodunuzu {} yazÄ±n.\",\n",
        "    \"FÄ°N kodunuzun dÃ¼zgÃ¼nlÃ¼yÃ¼nÃ¼ yoxlayÄ±n: {}\",\n",
        "    \"ÅÉ™xsiyyÉ™t vÉ™siqÉ™si vÉ™ FÄ°N kodu: {}\",\n",
        "    \"FÄ°N kodu olmadan sÉ™nÉ™d qÉ™bul edilmir: {}\",\n",
        "    \"FÄ°N kodu qutusuna {} yazÄ±n.\",\n",
        "    \"FÄ°N kodu sÉ™hvdirsÉ™, yenisini daxil edin: {}\",\n",
        "    \"HÉ™r hansÄ± É™mÉ™liyyat Ã¼Ã§Ã¼n FÄ°N kodu {} mÃ¼tlÉ™qdir.\",\n",
        "    \"MÉ™lumatlarÄ±nÄ±zÄ±n tÉ™hlÃ¼kÉ™sizliyi Ã¼Ã§Ã¼n FÄ°N kodunuzu {} ilÉ™ tÉ™sdiqlÉ™yin.\",\n",
        "    \"Daxil etdiyiniz FÄ°N kodu {} mÉ™lumatlarÄ±mÄ±zla uyÄŸun gÉ™lmir.\",\n",
        "    \"Bank xidmÉ™tlÉ™rindÉ™n istifadÉ™ Ã¼Ã§Ã¼n FÄ°N kodunuz: {}\",\n",
        "    \"ÅÉ™xsi mÉ™lumatlarÄ±nÄ±zÄ± yoxlamaq Ã¼Ã§Ã¼n {} FÄ°N kodunuzu daxil edin.\",\n",
        "    \"FÄ°N kodu {} ilÉ™ baÄŸlÄ± bÃ¼tÃ¼n sÉ™nÉ™dlÉ™r qÉ™bul edildi.\",\n",
        "    \"Ä°stifadÉ™Ã§inin FÄ°N kodu {} olaraq qeyd olundu.\",\n",
        "    \"Bu sÉ™nÉ™dÉ™ uyÄŸun FÄ°N kodu {}-dir.\",\n",
        "    \"Yeni bank kartÄ± almaq Ã¼Ã§Ã¼n {} FÄ°N kodu tÉ™lÉ™b olunur.\",\n",
        "    \"FÄ°N kodu {} olan ÅŸÉ™xs sistemÉ™ daxil oldu.\",\n",
        "    \"SistemdÉ™ {} FÄ°N kodu ilÉ™ axtarÄ±ÅŸ aparÄ±ldÄ±.\",\n",
        "    \"{} FÄ°N kodu ilÉ™ Ã¶dÉ™niÅŸ uÄŸurla tamamlandÄ±.\",\n",
        "    \"Vergi Ã¶dÉ™niÅŸi Ã¼Ã§Ã¼n FÄ°N kodunuzu {} yazÄ±n.\",\n",
        "    \"MaliyyÉ™ É™mÉ™liyyatlarÄ±nÄ± yoxlamaq Ã¼Ã§Ã¼n {} FÄ°N kodunu tÉ™qdim edin.\",\n",
        "]\n",
        "\n",
        "def generate_fin_sentences(n=500):\n",
        "    sentences = []\n",
        "    for _ in range(n):\n",
        "        template = random.choice(fin_templates)\n",
        "        fin_code = random_fin()\n",
        "        sentence = template.format(fin_code)\n",
        "        sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sentences = generate_fin_sentences(10000)\n",
        "    with open(\"fin_numune.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for sentence in sentences:\n",
        "            f.write(sentence + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsdxqkonSf3R"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "# FÄ°N kodu: 7 simvol, bÃ¶yÃ¼k hÉ™rf vÉ™ rÉ™qÉ™m, \"I\" vÉ™ \"O\" istisna\n",
        "# Pattern: [A-HJ-NP-Z0-9]{7} (A-Z, amma I vÉ™ O olmadan, 0-9)\n",
        "pattern_fin = r'\\b[A-HJ-NP-Z0-9]{7}\\b'\n",
        "label = \"FIN_CODE\"\n",
        "\n",
        "def label_sentences(sentences):\n",
        "    labeled_data = []\n",
        "    for sentence in sentences:\n",
        "        entities = []\n",
        "        for match in re.finditer(pattern_fin, sentence):\n",
        "            start, end = match.start(), match.end()\n",
        "            entities.append((start, end, label))\n",
        "        if entities:\n",
        "            labeled_data.append((sentence, {\"entities\": entities}))\n",
        "    return labeled_data\n",
        "\n",
        "# Fayldan cÃ¼mlÉ™lÉ™ri oxu\n",
        "with open(\"fin_numune.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    sentences = [line.strip() for line in f.readlines()]\n",
        "\n",
        "labeled_data = label_sentences(sentences)\n",
        "\n",
        "# AnnotasiyanÄ± JSON-a yaz\n",
        "with open(\"fin_annotated.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(labeled_data, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVf6b1vMSf0l"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/sample_data /content/car_plate_dataset.csv /content/car_plate_dataset.txt /content/fin_numune.txt /content/vesiqe_numune.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpTc0oZURgJG",
        "outputId": "e8b69167-e20c-432b-e003-1a09b9992ec5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… BÃ¼tÃ¼n kitabxanalar yÃ¼klÉ™ndi\n",
            "ğŸ”¥ PyTorch versiyasÄ±: 2.8.0+cu126\n",
            "ğŸš€ CUDA mÃ¶vcuddur: True\n",
            "ğŸ’» Ä°stifadÉ™ edilÉ™cÉ™k device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Import-lar\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from typing import List, Tuple, Dict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForTokenClassification\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ… BÃ¼tÃ¼n kitabxanalar yÃ¼klÉ™ndi\")\n",
        "print(f\"ğŸ”¥ PyTorch versiyasÄ±: {torch.__version__}\")\n",
        "print(f\"ğŸš€ CUDA mÃ¶vcuddur: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Device ayarla\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ğŸ’» Ä°stifadÉ™ edilÉ™cÉ™k device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiVTTom2RyAN",
        "outputId": "263e9dae-8468-44c8-875f-314a7943a507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“‹ Konfiqurasiya:\n",
            "Model: bert-base-multilingual-cased\n",
            "Max Length: 128\n",
            "Batch Size: 16\n",
            "Epochs: 3\n",
            "Learning Rate: 2e-05\n",
            "Labels: ['O', 'B-PLATE', 'I-PLATE', 'B-FIN', 'I-FIN', 'B-ID', 'I-ID']\n"
          ]
        }
      ],
      "source": [
        "# Model vÉ™ training konfiqurasiyasÄ±\n",
        "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 3\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "# Label sistemi - datasetinizdÉ™n Ã§Ä±xan nÉ™ticÉ™lÉ™rÉ™ gÃ¶rÉ™\n",
        "LABELS = [\n",
        "    'O',           # Outside\n",
        "    'B-PLATE',     # Beginning of Car Plate\n",
        "    'I-PLATE',     # Inside Car Plate\n",
        "    'B-FIN',       # Beginning of FIN Code\n",
        "    'I-FIN',       # Inside FIN Code\n",
        "    'B-ID',        # Beginning of ID Number\n",
        "    'I-ID'         # Inside ID Number\n",
        "]\n",
        "\n",
        "# Label mapping\n",
        "label2id = {label: i for i, label in enumerate(LABELS)}\n",
        "id2label = {i: label for i, label in enumerate(LABELS)}\n",
        "\n",
        "print(\"ğŸ“‹ Konfiqurasiya:\")\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Max Length: {MAX_LEN}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"Labels: {LABELS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnbGL2lSR0Jl",
        "outputId": "3a054e81-4273-4de6-a836-4ac6f70d44e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“ Entity Format QaydalarÄ±:\n",
            "ğŸ”¹ FIN kod: 7 simvol (hÉ™rf+rÉ™qÉ™m) - mÉ™sÉ™lÉ™n: AZEDF12\n",
            "ğŸ”¹ ID/AA: 9 simvol (AA + 7 rÉ™qÉ™m) - mÉ™sÉ™lÉ™n: AA1234567\n",
            "ğŸ”¹ ID/AZE: 12 simvol (AZE + 9 rÉ™qÉ™m) - mÉ™sÉ™lÉ™n: AZE123456789\n",
            "ğŸ”¹ Avtomobil: XX-YY-ZZZ formatÄ± - mÉ™sÉ™lÉ™n: 90-AB-123\n"
          ]
        }
      ],
      "source": [
        "def validate_entity_format(text: str, entity_type: str) -> bool:\n",
        "    \"\"\"\n",
        "    Entity-nin formatÄ±nÄ± doÄŸrulayÄ±r\n",
        "    Format qaydalarÄ±:\n",
        "    - FIN kod: 7 simvol (hÉ™rf+rÉ™qÉ™m)\n",
        "    - ID/AA: AA + 7 rÉ™qÉ™m (9 simvol)\n",
        "    - ID/AZE: AZE + 9 rÉ™qÉ™m (12 simvol)\n",
        "    - Avtomobil: XX-YY-ZZZ formatÄ±\n",
        "    \"\"\"\n",
        "    if entity_type == 'FIN':\n",
        "        # FIN kod 7 simvol olmalÄ±dÄ±r (hÉ™rf vÉ™ rÉ™qÉ™m qarÄ±ÅŸÄ±ÄŸÄ±)\n",
        "        return len(text) == 7 and re.match(r'^[A-Z0-9]{7}$', text)\n",
        "\n",
        "    elif entity_type == 'ID':\n",
        "        # Seriya nÃ¶mrÉ™si qaydalarÄ±\n",
        "        if text.startswith('AA'):\n",
        "            # AA ilÉ™ baÅŸlayÄ±rsa 9 simvol (AA + 7 rÉ™qÉ™m)\n",
        "            return len(text) == 9 and re.match(r'^AA\\d{7}$', text)\n",
        "        elif text.startswith('AZE'):\n",
        "            # AZE ilÉ™ baÅŸlayÄ±rsa 12 simvol (AZE + 9 rÉ™qÉ™m)\n",
        "            return len(text) == 12 and re.match(r'^AZE\\d{9}$', text)\n",
        "        else:\n",
        "            # DigÉ™r seriya nÃ¶mrÉ™lÉ™ri Ã¼Ã§Ã¼n Ã¼mumi qayda\n",
        "            return 8 <= len(text) <= 12 and re.match(r'^[A-Z]{2,3}\\d{6,9}$', text)\n",
        "\n",
        "    elif entity_type == 'PLATE':\n",
        "        # Avtomobil nÃ¶mrÉ™si XX-YY-ZZZ formatÄ±nda\n",
        "        return len(text) == 9 and re.match(r'^\\d{2}-[A-Z]{2}-\\d{3}$', text)\n",
        "\n",
        "    return False\n",
        "\n",
        "def show_validation_rules():\n",
        "    \"\"\"Format qaydalarÄ±nÄ± gÃ¶stÉ™rir\"\"\"\n",
        "    print(\"ğŸ“ Entity Format QaydalarÄ±:\")\n",
        "    print(\"ğŸ”¹ FIN kod: 7 simvol (hÉ™rf+rÉ™qÉ™m) - mÉ™sÉ™lÉ™n: AZEDF12\")\n",
        "    print(\"ğŸ”¹ ID/AA: 9 simvol (AA + 7 rÉ™qÉ™m) - mÉ™sÉ™lÉ™n: AA1234567\")\n",
        "    print(\"ğŸ”¹ ID/AZE: 12 simvol (AZE + 9 rÉ™qÉ™m) - mÉ™sÉ™lÉ™n: AZE123456789\")\n",
        "    print(\"ğŸ”¹ Avtomobil: XX-YY-ZZZ formatÄ± - mÉ™sÉ™lÉ™n: 90-AB-123\")\n",
        "\n",
        "show_validation_rules()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiHQQQSnR3PF",
        "outputId": "c0e53640-0925-4769-8d59-7e46adba2181"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Dataset yÃ¼klÉ™mÉ™ funksiyalarÄ± hazÄ±rlandÄ±\n"
          ]
        }
      ],
      "source": [
        "def convert_spacy_to_bio(text: str, entities: list) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    spaCy formatÄ±ndan BIO formatÄ±na Ã§evirmÉ™\n",
        "    \"\"\"\n",
        "    tokens = text.split()\n",
        "    labels = ['O'] * len(tokens)\n",
        "\n",
        "    # Token-larÄ±n mÃ¶vqelÉ™rini hesablayÄ±rÄ±q\n",
        "    token_positions = []\n",
        "    current_pos = 0\n",
        "\n",
        "    for token in tokens:\n",
        "        start_pos = text.find(token, current_pos)\n",
        "        end_pos = start_pos + len(token)\n",
        "        token_positions.append((start_pos, end_pos))\n",
        "        current_pos = end_pos\n",
        "\n",
        "    # Entity-lÉ™ri BIO formatÄ±na Ã§eviririk\n",
        "    for start_char, end_char, entity_type in entities:\n",
        "        # Entity tipini bizim formatÄ±mÄ±za uyÄŸunlaÅŸdÄ±rÄ±rÄ±q\n",
        "        if entity_type == \"CAR_PLATE\":\n",
        "            bio_label = \"PLATE\"\n",
        "        elif entity_type == \"FIN_CODE\":\n",
        "            bio_label = \"FIN\"\n",
        "        elif entity_type == \"ID_NUMBER\":\n",
        "            bio_label = \"ID\"\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        # HansÄ± token-larÄ±n entity-yÉ™ aid olduÄŸunu tapÄ±rÄ±q\n",
        "        entity_tokens = []\n",
        "        for i, (token_start, token_end) in enumerate(token_positions):\n",
        "            if token_start < end_char and token_end > start_char:\n",
        "                entity_tokens.append(i)\n",
        "\n",
        "        # BIO etiketlÉ™ri tÉ™yin edirik\n",
        "        for i, token_idx in enumerate(entity_tokens):\n",
        "            if i == 0:\n",
        "                labels[token_idx] = f\"B-{bio_label}\"\n",
        "            else:\n",
        "                labels[token_idx] = f\"I-{bio_label}\"\n",
        "\n",
        "    return tokens, labels\n",
        "\n",
        "def load_spacy_json_from_path(file_path: str):\n",
        "    \"\"\"\n",
        "    VerilÉ™n fayl yolundan spaCy formatÄ±nda JSON dataseti yÃ¼klÉ™yir\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        texts = []\n",
        "        labels = []\n",
        "\n",
        "        for item in data:\n",
        "            text = item[0]  # CÃ¼mlÉ™\n",
        "            annotations = item[1]  # Annotasiyalar\n",
        "            entities = annotations.get('entities', [])\n",
        "\n",
        "            tokens, bio_labels = convert_spacy_to_bio(text, entities)\n",
        "            texts.append(tokens)\n",
        "            labels.append(bio_labels)\n",
        "\n",
        "        print(f\"âœ… {file_path} uÄŸurla yÃ¼klÉ™ndi: {len(texts)} nÃ¼munÉ™\")\n",
        "\n",
        "        return texts, labels\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ Fayl tapÄ±lmadÄ±: {file_path}\")\n",
        "        return [], []\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ XÉ™ta baÅŸ verdi {file_path}: {e}\")\n",
        "        return [], []\n",
        "\n",
        "def load_datasets_from_paths(file_paths: List[str]):\n",
        "    \"\"\"\n",
        "    VerilÉ™n fayl yollarÄ±ndan datasetlÉ™ri yÃ¼klÉ™yir\n",
        "    \"\"\"\n",
        "    datasets = []\n",
        "\n",
        "    for i, file_path in enumerate(file_paths, 1):\n",
        "        print(f\"\\nğŸ“ Dataset {i} yÃ¼klÉ™nir: {file_path}\")\n",
        "        texts, labels = load_spacy_json_from_path(file_path)\n",
        "\n",
        "        if texts:\n",
        "            datasets.append((texts, labels))\n",
        "            print(f\"âœ… Dataset {i} yÃ¼klÉ™ndi: {len(texts)} nÃ¼munÉ™\")\n",
        "        else:\n",
        "            print(f\"âŒ Dataset {i} yÃ¼klÉ™nmÉ™di\")\n",
        "\n",
        "    return datasets\n",
        "\n",
        "def combine_datasets(datasets: List[Tuple[List[List[str]], List[List[str]]]]):\n",
        "    \"\"\"\n",
        "    DatasetlÉ™ri birlÉ™ÅŸdirir\n",
        "    \"\"\"\n",
        "    all_texts = []\n",
        "    all_labels = []\n",
        "\n",
        "    for i, (texts, labels) in enumerate(datasets, 1):\n",
        "        print(f\"Dataset {i}: {len(texts)} nÃ¼munÉ™\")\n",
        "        all_texts.extend(texts)\n",
        "        all_labels.extend(labels)\n",
        "\n",
        "    print(f\"ğŸ“Š Ãœmumi: {len(all_texts)} nÃ¼munÉ™\")\n",
        "    return all_texts, all_labels\n",
        "\n",
        "print(\"âœ… Dataset yÃ¼klÉ™mÉ™ funksiyalarÄ± hazÄ±rlandÄ±\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNMViva_R6lZ",
        "outputId": "df9b01bc-08df-42ad-c7e9-1f013f2afe40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Dataset analiz funksiyalarÄ± hazÄ±rlandÄ±\n"
          ]
        }
      ],
      "source": [
        "def analyze_dataset(texts: List[List[str]], labels: List[List[str]]):\n",
        "    \"\"\"\n",
        "    Dataset analizi vÉ™ statistika\n",
        "    \"\"\"\n",
        "    label_counts = {}\n",
        "    total_tokens = 0\n",
        "    entity_examples = {}\n",
        "\n",
        "    for text_tokens, label_seq in zip(texts, labels):\n",
        "        total_tokens += len(label_seq)\n",
        "\n",
        "        # Entity nÃ¼munÉ™lÉ™rini topla\n",
        "        i = 0\n",
        "        while i < len(text_tokens):\n",
        "            label = label_seq[i] if i < len(label_seq) else 'O'\n",
        "\n",
        "            if label.startswith('B-'):\n",
        "                entity_type = label[2:]\n",
        "                entity_tokens = [text_tokens[i]]\n",
        "                j = i + 1\n",
        "\n",
        "                while (j < len(text_tokens) and\n",
        "                       j < len(label_seq) and\n",
        "                       label_seq[j] == f'I-{entity_type}'):\n",
        "                    entity_tokens.append(text_tokens[j])\n",
        "                    j += 1\n",
        "\n",
        "                entity_text = ''.join(entity_tokens)\n",
        "                if entity_type not in entity_examples:\n",
        "                    entity_examples[entity_type] = []\n",
        "                if len(entity_examples[entity_type]) < 3:\n",
        "                    entity_examples[entity_type].append(entity_text)\n",
        "                i = j\n",
        "            else:\n",
        "                i += 1\n",
        "\n",
        "        # Label sayÄ±nÄ± hesabla\n",
        "        for label in label_seq:\n",
        "            label_counts[label] = label_counts.get(label, 0) + 1\n",
        "\n",
        "    print(\"\\nğŸ“ˆ Dataset StatistikasÄ±:\")\n",
        "    print(f\"Ãœmumi cÃ¼mlÉ™ sayÄ±: {len(texts)}\")\n",
        "    print(f\"Ãœmumi token sayÄ±: {total_tokens}\")\n",
        "    print(\"\\nğŸ·ï¸ Label paylanmasÄ±:\")\n",
        "\n",
        "    for label, count in sorted(label_counts.items()):\n",
        "        percentage = (count / total_tokens) * 100\n",
        "        print(f\"{label}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    print(\"\\nğŸ“ Entity nÃ¼munÉ™lÉ™ri:\")\n",
        "    for entity_type, examples in entity_examples.items():\n",
        "        print(f\"{entity_type}: {', '.join(examples[:3])}\")\n",
        "\n",
        "def validate_dataset_entities(texts: List[List[str]], labels: List[List[str]]):\n",
        "    \"\"\"\n",
        "    Dataset-dÉ™ olan entity-lÉ™ri validation qaydalarÄ±na gÃ¶rÉ™ yoxlayÄ±r\n",
        "    \"\"\"\n",
        "    valid_entities = {'FIN': 0, 'ID': 0, 'PLATE': 0}\n",
        "    invalid_entities = {'FIN': 0, 'ID': 0, 'PLATE': 0}\n",
        "\n",
        "    for text_tokens, label_seq in zip(texts, labels):\n",
        "        i = 0\n",
        "        while i < len(text_tokens):\n",
        "            label = label_seq[i] if i < len(label_seq) else 'O'\n",
        "\n",
        "            if label.startswith('B-'):\n",
        "                entity_type = label[2:]\n",
        "                entity_tokens = [text_tokens[i]]\n",
        "                j = i + 1\n",
        "\n",
        "                while (j < len(text_tokens) and\n",
        "                       j < len(label_seq) and\n",
        "                       label_seq[j] == f'I-{entity_type}'):\n",
        "                    entity_tokens.append(text_tokens[j])\n",
        "                    j += 1\n",
        "\n",
        "                entity_text = ''.join(entity_tokens)\n",
        "\n",
        "                if validate_entity_format(entity_text, entity_type):\n",
        "                    valid_entities[entity_type] += 1\n",
        "                else:\n",
        "                    invalid_entities[entity_type] += 1\n",
        "\n",
        "                i = j\n",
        "            else:\n",
        "                i += 1\n",
        "\n",
        "    print(\"\\nâœ… Validation NÉ™ticÉ™lÉ™ri:\")\n",
        "    for entity_type in valid_entities:\n",
        "        total = valid_entities[entity_type] + invalid_entities[entity_type]\n",
        "        if total > 0:\n",
        "            valid_percent = (valid_entities[entity_type] / total) * 100\n",
        "            print(f\"{entity_type}: {valid_entities[entity_type]}/{total} valid ({valid_percent:.1f}%)\")\n",
        "\n",
        "print(\"âœ… Dataset analiz funksiyalarÄ± hazÄ±rlandÄ±\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLsLl2t3R_km",
        "outputId": "a24f438d-1a4c-493b-d26e-5b87b7f52cc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“‚ DatasetlÉ™r yÃ¼klÉ™nir...\n",
            "Fayl yollarÄ±:\n",
            "  1. /content/car_plate_dataset_annotated.json\n",
            "  2. /content/fin_annotated.json\n",
            "  3. /content/vesiqe_annotated.json\n",
            "\n",
            "ğŸ“ Dataset 1 yÃ¼klÉ™nir: /content/car_plate_dataset_annotated.json\n",
            "âœ… /content/car_plate_dataset_annotated.json uÄŸurla yÃ¼klÉ™ndi: 10000 nÃ¼munÉ™\n",
            "âœ… Dataset 1 yÃ¼klÉ™ndi: 10000 nÃ¼munÉ™\n",
            "\n",
            "ğŸ“ Dataset 2 yÃ¼klÉ™nir: /content/fin_annotated.json\n",
            "âœ… /content/fin_annotated.json uÄŸurla yÃ¼klÉ™ndi: 10000 nÃ¼munÉ™\n",
            "âœ… Dataset 2 yÃ¼klÉ™ndi: 10000 nÃ¼munÉ™\n",
            "\n",
            "ğŸ“ Dataset 3 yÃ¼klÉ™nir: /content/vesiqe_annotated.json\n",
            "âœ… /content/vesiqe_annotated.json uÄŸurla yÃ¼klÉ™ndi: 10000 nÃ¼munÉ™\n",
            "âœ… Dataset 3 yÃ¼klÉ™ndi: 10000 nÃ¼munÉ™\n",
            "Dataset 1: 10000 nÃ¼munÉ™\n",
            "Dataset 2: 10000 nÃ¼munÉ™\n",
            "Dataset 3: 10000 nÃ¼munÉ™\n",
            "ğŸ“Š Ãœmumi: 30000 nÃ¼munÉ™\n",
            "\n",
            "ğŸ“ˆ Dataset StatistikasÄ±:\n",
            "Ãœmumi cÃ¼mlÉ™ sayÄ±: 30000\n",
            "Ãœmumi token sayÄ±: 192258\n",
            "\n",
            "ğŸ·ï¸ Label paylanmasÄ±:\n",
            "B-FIN: 10000 (5.2%)\n",
            "B-ID: 10000 (5.2%)\n",
            "B-PLATE: 10000 (5.2%)\n",
            "O: 162258 (84.4%)\n",
            "\n",
            "ğŸ“ Entity nÃ¼munÉ™lÉ™ri:\n",
            "PLATE: 85-NL-695, 95-MA-819, 54-GY-552\n",
            "FIN: BML6CZ9, ZWUH9EL, U9H9PXW\n",
            "ID: AZE017114779, AZE879177352, AA3042819\n",
            "\n",
            "âœ… Validation NÉ™ticÉ™lÉ™ri:\n",
            "FIN: 8928/10000 valid (89.3%)\n",
            "ID: 8850/10000 valid (88.5%)\n",
            "PLATE: 9608/10000 valid (96.1%)\n",
            "\n",
            "ğŸ“‹ Data Split:\n",
            "Train: 24000 nÃ¼munÉ™\n",
            "Validation: 6000 nÃ¼munÉ™\n",
            "âœ… DatasetlÉ™r uÄŸurla hazÄ±rlandÄ±!\n"
          ]
        }
      ],
      "source": [
        "# SÄ°Z BU HÄ°SSÆNÄ° DÆYÄ°ÅDÄ°RÄ°N - Ã–Z FAYL YOLLARINIZI VERÄ°N\n",
        "dataset_paths = [\n",
        "    \"/content/car_plate_dataset_annotated.json\",    # 1-ci dataset\n",
        "    \"/content/fin_annotated.json\",                  # 2-ci dataset\n",
        "    \"/content/vesiqe_annotated.json\"                # 3-cÃ¼ dataset\n",
        "]\n",
        "\n",
        "print(\"ğŸ“‚ DatasetlÉ™r yÃ¼klÉ™nir...\")\n",
        "print(\"Fayl yollarÄ±:\")\n",
        "for i, path in enumerate(dataset_paths, 1):\n",
        "    print(f\"  {i}. {path}\")\n",
        "\n",
        "# DatasetlÉ™ri yÃ¼klÉ™\n",
        "datasets = load_datasets_from_paths(dataset_paths)\n",
        "\n",
        "if datasets:\n",
        "    # DatasetlÉ™ri birlÉ™ÅŸdir\n",
        "    all_texts, all_labels = combine_datasets(datasets)\n",
        "\n",
        "    # Dataset analizini et\n",
        "    analyze_dataset(all_texts, all_labels)\n",
        "\n",
        "    # Validation yoxlamasÄ±\n",
        "    validate_dataset_entities(all_texts, all_labels)\n",
        "\n",
        "    # Train-validation split\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "        all_texts, all_labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"\\nğŸ“‹ Data Split:\")\n",
        "    print(f\"Train: {len(train_texts)} nÃ¼munÉ™\")\n",
        "    print(f\"Validation: {len(val_texts)} nÃ¼munÉ™\")\n",
        "\n",
        "    print(\"âœ… DatasetlÉ™r uÄŸurla hazÄ±rlandÄ±!\")\n",
        "else:\n",
        "    print(\"âŒ HeÃ§ bir dataset yÃ¼klÉ™nmÉ™di!\")\n",
        "    print(\"Fayl yollarÄ±nÄ± yoxlayÄ±n vÉ™ yenidÉ™n cÉ™hd edin.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181,
          "referenced_widgets": [
            "5562f2ef16344cadb184f93a065d2696",
            "55d11845ff5c4e0dba39a6b7ae554649",
            "170b73c382ba4a8ca2432526f4dc5952",
            "c2152a9a197c4ede8bf10fa5c136804e",
            "8530b259f40048d9a52187268a9fecd8",
            "c1e5f4385ff14b5f88152efe143e6787",
            "59101551208b4d3b93352c7caa8ce6f9",
            "e9d9d8b49b474530a69c42411c080882",
            "a8dcc80c961a42e7b363dec816004d38",
            "09708c7c0b9e4cd0a4ef183bf30a0be7",
            "d6eabdfe77854b72bb87f9a125a06bfb",
            "a4b67e734e2b4f0ebec95a1dbd754f15",
            "0eb22280c7ce4774bc2c63326c3ca7e4",
            "0af231b8b0ca4c4289f51f7d95b3349c",
            "d4f58c773d604547958cab90c6ce7379",
            "b608c9954d7e4978ade0093665813810",
            "0f85083e933d43728b822086a57f5fb3",
            "86ac301200f44445b3ea6d8aaa4bb5cc",
            "0c94d80308e641feae3ac5c7b8133e51",
            "a7ef542a5d47478a8e1d830c21d0df87",
            "8f0ab0d1e5bd4e278014fdfbd3f8e003",
            "bc974ee6e9914c66b2b380e5a816ae07",
            "6f7a7327d84746338f15535d8d7aaee9",
            "5884c1378b0844378c21e4cd5d75ba43",
            "34c697bc1fb94a33a80b4ca93e2b42fd",
            "4c70f338b98f49c993345353042273d5",
            "b3d64369119c420592531c1bc034858a",
            "457f04538b9b422cafa050ef74417236",
            "07a68a9621434e579662f209cbdf07f3",
            "42d9711f885e4cf19ffdd1d482185877",
            "42ae2f211273449db35154e4209446aa",
            "d85653158cff4046aeca106889e1671c",
            "05019c7d08904109a4e37e46004de499",
            "0073c7fa482e40bdb2f05a6cd11d7bb2",
            "c666c6e4e96e4e8e922344777c6458f9",
            "ec21d6f181cd461bb64348ccdac73b85",
            "2ce2482153854279a2694a37efe8a6a9",
            "e939d9baf53640e79a62f4582d2bd286",
            "c74af87461c44618bb6f359911997633",
            "5adb7debbc4e4ac5a1f95a1b5c4382b2",
            "e1279f3b20fd47caaf9d76a746d69618",
            "8351bdb2ab704ed1a6f4da3be128d886",
            "8df9ce03fa814103b5b0775f7ab8ff9f",
            "9f7a666fa28840e9964bb9fe7d2d56f0"
          ]
        },
        "id": "E65uuHX3SIoK",
        "outputId": "7e1e0087-96cd-4a3f-f445-71f99d139cd0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5562f2ef16344cadb184f93a065d2696",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4b67e734e2b4f0ebec95a1dbd754f15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f7a7327d84746338f15535d8d7aaee9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0073c7fa482e40bdb2f05a6cd11d7bb2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Tokenizer yÃ¼klÉ™ndi: bert-base-multilingual-cased\n",
            "âœ… NERDataset sinifi hazÄ±rlandÄ±\n"
          ]
        }
      ],
      "source": [
        "# Tokenizer vÉ™ model yÃ¼klÉ™mÉ™\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "print(f\"âœ… Tokenizer yÃ¼klÉ™ndi: {MODEL_NAME}\")\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, texts: List[List[str]], labels: List[List[str]], tokenizer, max_len: int):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        labels = self.labels[idx]\n",
        "\n",
        "        # TokenlÉ™ÅŸdirmÉ™\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            is_split_into_words=True,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Label-larÄ± token-lara uyÄŸunlaÅŸdÄ±rma\n",
        "        word_ids = encoding.word_ids()\n",
        "        label_ids = []\n",
        "\n",
        "        for word_id in word_ids:\n",
        "            if word_id is None:\n",
        "                label_ids.append(-100)  # Special tokens\n",
        "            elif word_id < len(labels):\n",
        "                label_ids.append(label2id[labels[word_id]])\n",
        "            else:\n",
        "                label_ids.append(label2id['O'])\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label_ids, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "print(\"âœ… NERDataset sinifi hazÄ±rlandÄ±\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247,
          "referenced_widgets": [
            "88aa1838f579462bb7598de5e2f49a23",
            "c9beb6c2396943dc94fa97dfd7ede179",
            "cd63c5d7c2e248289314b33d78ac49e0",
            "b7895f9e0f2d419db600a68f28ca5e03",
            "61b65922d9174c66a09fe4b34f8a87b5",
            "6f6be16bcb5e46478a63593c0525eccf",
            "7eb8732f59704972bae589db6cc19bf8",
            "df20009ea2ef4a949455091f458205f6",
            "786cac18f08443cd8cd260bf17cd5551",
            "aeb8a472c3424ee2bdbd53131eabc84f",
            "3c42cffc80214d1aa57054ec2dca733d"
          ]
        },
        "id": "BkfUbs4aSLWz",
        "outputId": "22b7df97-f25b-468e-9eef-67deec00c676"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88aa1838f579462bb7598de5e2f49a23",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Model yÃ¼klÉ™ndi: bert-base-multilingual-cased\n",
            "ğŸ“Š Label sayÄ±: 7\n",
            "Max steps: 300\n",
            "Warmup steps: 50\n",
            "Logging every: 25 steps\n",
            "Evaluation every: 50 steps\n",
            "Save every: 50 steps\n",
            "âœ… Training konfiqurasiyasÄ± hazÄ±rlandÄ±\n"
          ]
        }
      ],
      "source": [
        "# Model yÃ¼klÉ™\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(LABELS),\n",
        "    label2id=label2id,\n",
        "    id2label=id2label\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "print(f\"âœ… Model yÃ¼klÉ™ndi: {MODEL_NAME}\")\n",
        "print(f\"ğŸ“Š Label sayÄ±: {len(LABELS)}\")\n",
        "\n",
        "# Metrics hesablama funksiyasÄ±\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Training zamanÄ± metriklÉ™r hesablamaq Ã¼Ã§Ã¼n\n",
        "    \"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # -100 olan labellarÄ± Ã§Ä±xar (special tokens)\n",
        "    true_predictions = [\n",
        "        [id2label[p] for p, l in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [id2label[l] for p, l in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    # Flatten etmÉ™k\n",
        "    flat_true_labels = [label for sublist in true_labels for label in sublist]\n",
        "    flat_predictions = [pred for sublist in true_predictions for pred in sublist]\n",
        "\n",
        "    # Metrics hesablamaq\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        flat_true_labels, flat_predictions, average='weighted', zero_division=0\n",
        "    )\n",
        "    accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Training argumentlÉ™ri - 300 step Ã¼Ã§Ã¼n\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    max_steps=300,                    # 300 step mÉ™hdudiyyÉ™ti\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    warmup_steps=50,                  # Daha az warmup (300 step-in 1/6-sÄ±)\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=25,                 # Daha tez-tez log (300/12)\n",
        "    eval_strategy=\"steps\",      # eval_strategy É™vÉ™zinÉ™ evaluation_strategy\n",
        "    eval_steps=50,                    # Daha tez-tez eval (300/6)\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,                    # Daha tez-tez save\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    report_to=None,\n",
        "    learning_rate=LEARNING_RATE,\n",
        ")\n",
        "\n",
        "print(f\"Max steps: 300\")\n",
        "print(f\"Warmup steps: 50\")\n",
        "print(f\"Logging every: 25 steps\")\n",
        "print(f\"Evaluation every: 50 steps\")\n",
        "print(f\"Save every: 50 steps\")\n",
        "\n",
        "print(\"âœ… Training konfiqurasiyasÄ± hazÄ±rlandÄ±\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "id": "2ohdYzfbSMsp",
        "outputId": "1b4c5827-1644-4711-f241-dc6a68743e71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Train dataset: 24000 nÃ¼munÉ™\n",
            "âœ… Validation dataset: 6000 nÃ¼munÉ™\n",
            "âœ… Trainer hazÄ±rlandÄ±\n",
            "\n",
            "ğŸš€ Training baÅŸlayÄ±r...\n",
            "ğŸ“Š Total training samples: 24000\n",
            "ğŸ“Š Total validation samples: 6000\n",
            "ğŸ”¥ Device: cuda\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbahramzada\u001b[0m (\u001b[33mbahramzada-unec-business-school\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "creating run (0.0s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250910_204936-jrib0ssv</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bahramzada-unec-business-school/huggingface/runs/jrib0ssv' target=\"_blank\">pleasant-energy-20</a></strong> to <a href='https://wandb.ai/bahramzada-unec-business-school/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/bahramzada-unec-business-school/huggingface' target=\"_blank\">https://wandb.ai/bahramzada-unec-business-school/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/bahramzada-unec-business-school/huggingface/runs/jrib0ssv' target=\"_blank\">https://wandb.ai/bahramzada-unec-business-school/huggingface/runs/jrib0ssv</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [300/300 09:56, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.159500</td>\n",
              "      <td>0.005118</td>\n",
              "      <td>0.999606</td>\n",
              "      <td>0.999606</td>\n",
              "      <td>0.999607</td>\n",
              "      <td>0.999606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.000624</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.000402</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.000478</td>\n",
              "      <td>0.999964</td>\n",
              "      <td>0.999964</td>\n",
              "      <td>0.999964</td>\n",
              "      <td>0.999964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.000307</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.000290</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Training tamamlandÄ±!\n",
            "\n",
            "ğŸ“Š Model qiymÉ™tlÉ™ndirilir...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [375/375 00:43]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¯ Evaluation nÉ™ticÉ™lÉ™ri:\n",
            "eval_loss: 0.0006\n",
            "eval_accuracy: 1.0000\n",
            "eval_f1: 1.0000\n",
            "eval_precision: 1.0000\n",
            "eval_recall: 1.0000\n",
            "eval_runtime: 45.0148\n",
            "eval_samples_per_second: 133.2890\n",
            "eval_steps_per_second: 8.3310\n",
            "epoch: 0.2000\n",
            "âœ… Model ./best_model qovluÄŸunda saxlanÄ±ldÄ±\n"
          ]
        }
      ],
      "source": [
        "# Dataset obyektlÉ™rini yarat (É™gÉ™r datasetlÉ™r yÃ¼klÉ™nmiÅŸsÉ™)\n",
        "if 'train_texts' in locals() and len(train_texts) > 0:\n",
        "    train_dataset = NERDataset(\n",
        "        texts=train_texts,\n",
        "        labels=train_labels,\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=MAX_LEN\n",
        "    )\n",
        "\n",
        "    val_dataset = NERDataset(\n",
        "        texts=val_texts,\n",
        "        labels=val_labels,\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=MAX_LEN\n",
        "    )\n",
        "\n",
        "    # Data collator\n",
        "    data_collator = DataCollatorForTokenClassification(\n",
        "        tokenizer=tokenizer,\n",
        "        padding=True,\n",
        "        max_length=MAX_LEN,\n",
        "        pad_to_multiple_of=None,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Trainer obyektini yarat\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    print(f\"âœ… Train dataset: {len(train_dataset)} nÃ¼munÉ™\")\n",
        "    print(f\"âœ… Validation dataset: {len(val_dataset)} nÃ¼munÉ™\")\n",
        "    print(\"âœ… Trainer hazÄ±rlandÄ±\")\n",
        "\n",
        "    # Training baÅŸlat\n",
        "    print(\"\\nğŸš€ Training baÅŸlayÄ±r...\")\n",
        "    print(f\"ğŸ“Š Total training samples: {len(train_dataset)}\")\n",
        "    print(f\"ğŸ“Š Total validation samples: {len(val_dataset)}\")\n",
        "    print(f\"ğŸ”¥ Device: {device}\")\n",
        "\n",
        "    try:\n",
        "        trainer.train()\n",
        "        print(\"âœ… Training tamamlandÄ±!\")\n",
        "\n",
        "        # Model qiymÉ™tlÉ™ndirmÉ™\n",
        "        print(\"\\nğŸ“Š Model qiymÉ™tlÉ™ndirilir...\")\n",
        "        eval_results = trainer.evaluate()\n",
        "\n",
        "        print(\"ğŸ¯ Evaluation nÉ™ticÉ™lÉ™ri:\")\n",
        "        for key, value in eval_results.items():\n",
        "            if isinstance(value, float):\n",
        "                print(f\"{key}: {value:.4f}\")\n",
        "            else:\n",
        "                print(f\"{key}: {value}\")\n",
        "\n",
        "        # En yaxÅŸÄ± modeli saxla\n",
        "        trainer.save_model(\"./best_model\")\n",
        "        tokenizer.save_pretrained(\"./best_model\")\n",
        "        print(\"âœ… Model ./best_model qovluÄŸunda saxlanÄ±ldÄ±\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Training zamanÄ± xÉ™ta: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ DatasetlÉ™r hazÄ±rlanmadÄ±, training edilÉ™ bilmÉ™z!\")\n",
        "    print(\"ÆvvÉ™lki hissÉ™lÉ™rdÉ™ datasetlÉ™ri dÃ¼zgÃ¼n yÃ¼klÉ™yin.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFcRr3miSTDr",
        "outputId": "7f4bfc5a-eac3-48e0-8e7b-b0578c13d6a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… TÉ™kmillÉ™ÅŸdirilmiÅŸ inference funksiyalarÄ± hazÄ±rlandÄ±\n"
          ]
        }
      ],
      "source": [
        "def predict_entities_with_model(text: str, model, tokenizer, device):\n",
        "    \"\"\"\n",
        "    Modeli istifadÉ™ edÉ™rÉ™k entity-lÉ™ri predict edir\n",
        "    \"\"\"\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Tokenizer ilÉ™ encode et\n",
        "    inputs = tokenizer(\n",
        "        tokens,\n",
        "        is_split_into_words=True,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=MAX_LEN\n",
        "    )\n",
        "\n",
        "    # GPU-ya gÃ¶ndÉ™r\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Model prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=2)\n",
        "\n",
        "    # Word IDs ilÉ™ token-word mapping\n",
        "    word_ids = inputs.word_ids() if hasattr(inputs, 'word_ids') else None\n",
        "    predicted_labels = []\n",
        "\n",
        "    if word_ids is None:\n",
        "        # Manual word mapping\n",
        "        tokenized = tokenizer.tokenize(' '.join(tokens))\n",
        "        pred_idx = 1  # CLS token-dan sonra baÅŸla\n",
        "\n",
        "        for token in tokens:\n",
        "            if pred_idx < len(predictions[0]):\n",
        "                label_id = predictions[0][pred_idx].item()\n",
        "                if label_id < len(id2label):\n",
        "                    predicted_labels.append(id2label[label_id])\n",
        "                else:\n",
        "                    predicted_labels.append('O')\n",
        "\n",
        "                # Token neÃ§É™ subword-É™ bÃ¶lÃ¼nÃ¼b onu hesabla\n",
        "                token_subwords = tokenizer.tokenize(token)\n",
        "                pred_idx += len(token_subwords)\n",
        "            else:\n",
        "                predicted_labels.append('O')\n",
        "    else:\n",
        "        # Word IDs istifadÉ™ et\n",
        "        for i in range(len(tokens)):\n",
        "            word_positions = [j for j, wid in enumerate(word_ids[0]) if wid == i]\n",
        "            if word_positions:\n",
        "                label_id = predictions[0][word_positions[0]].item()\n",
        "                if label_id < len(id2label):\n",
        "                    predicted_labels.append(id2label[label_id])\n",
        "                else:\n",
        "                    predicted_labels.append('O')\n",
        "            else:\n",
        "                predicted_labels.append('O')\n",
        "\n",
        "    # Label sayÄ±nÄ± dÃ¼zÉ™lt\n",
        "    while len(predicted_labels) < len(tokens):\n",
        "        predicted_labels.append('O')\n",
        "    predicted_labels = predicted_labels[:len(tokens)]\n",
        "\n",
        "    return tokens, predicted_labels\n",
        "\n",
        "def enhanced_predict_with_validation(text: str, model, tokenizer, device):\n",
        "    \"\"\"\n",
        "    Validation qaydalarÄ± ilÉ™ tÉ™kmillÉ™ÅŸdirilmiÅŸ entity prediction\n",
        "    \"\"\"\n",
        "    tokens, predicted_labels = predict_entities_with_model(text, model, tokenizer, device)\n",
        "\n",
        "    validated_labels = []\n",
        "    i = 0\n",
        "\n",
        "    while i < len(tokens):\n",
        "        current_label = predicted_labels[i] if i < len(predicted_labels) else 'O'\n",
        "\n",
        "        if current_label.startswith('B-'):\n",
        "            entity_type = current_label[2:]\n",
        "            entity_tokens = [tokens[i]]\n",
        "\n",
        "            # Entity-nin tam mÉ™tnini topla\n",
        "            j = i + 1\n",
        "            while (j < len(tokens) and\n",
        "                   j < len(predicted_labels) and\n",
        "                   predicted_labels[j] == f'I-{entity_type}'):\n",
        "                entity_tokens.append(tokens[j])\n",
        "                j += 1\n",
        "\n",
        "            entity_text = ''.join(entity_tokens)\n",
        "\n",
        "            # Format validation\n",
        "            if validate_entity_format(entity_text, entity_type):\n",
        "                # Valid entity - saxla\n",
        "                validated_labels.append(current_label)\n",
        "                for k in range(i + 1, j):\n",
        "                    if k < len(predicted_labels):\n",
        "                        validated_labels.append(predicted_labels[k])\n",
        "                    else:\n",
        "                        validated_labels.append('O')\n",
        "            else:\n",
        "                # Invalid entity - O etiketi ver\n",
        "                for k in range(i, j):\n",
        "                    validated_labels.append('O')\n",
        "\n",
        "            i = j\n",
        "        else:\n",
        "            validated_labels.append(current_label)\n",
        "            i += 1\n",
        "\n",
        "    # Label sayÄ±nÄ± dÃ¼zÉ™lt\n",
        "    while len(validated_labels) < len(tokens):\n",
        "        validated_labels.append('O')\n",
        "    validated_labels = validated_labels[:len(tokens)]\n",
        "\n",
        "    return tokens, validated_labels\n",
        "\n",
        "def blur_with_model_and_validation(text: str, model, tokenizer, device):\n",
        "    \"\"\"\n",
        "    Model + validation ilÉ™ blurring edir\n",
        "    \"\"\"\n",
        "    tokens, validated_labels = enhanced_predict_with_validation(text, model, tokenizer, device)\n",
        "\n",
        "    blurred_tokens = []\n",
        "    entities_found = []\n",
        "    i = 0\n",
        "\n",
        "    while i < len(tokens):\n",
        "        current_label = validated_labels[i] if i < len(validated_labels) else 'O'\n",
        "\n",
        "        if current_label.startswith('B-'):\n",
        "            entity_type = current_label[2:]\n",
        "            entity_tokens = [tokens[i]]\n",
        "            entity_start = i\n",
        "\n",
        "            # I- etiketli davamÄ±nÄ± tap\n",
        "            j = i + 1\n",
        "            while (j < len(tokens) and\n",
        "                   j < len(validated_labels) and\n",
        "                   validated_labels[j] == f'I-{entity_type}'):\n",
        "                entity_tokens.append(tokens[j])\n",
        "                j += 1\n",
        "\n",
        "            entity_text = ''.join(entity_tokens)\n",
        "\n",
        "            entities_found.append({\n",
        "                'text': entity_text,\n",
        "                'type': entity_type,\n",
        "                'start': entity_start,\n",
        "                'end': j-1,\n",
        "                'tokens': entity_tokens\n",
        "            })\n",
        "\n",
        "            blurred_tokens.append('[BLURRED]')\n",
        "            i = j\n",
        "        else:\n",
        "            blurred_tokens.append(tokens[i])\n",
        "            i += 1\n",
        "\n",
        "    return ' '.join(blurred_tokens), entities_found\n",
        "\n",
        "print(\"âœ… TÉ™kmillÉ™ÅŸdirilmiÅŸ inference funksiyalarÄ± hazÄ±rlandÄ±\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujVNFXhHSaVe",
        "outputId": "3ff4dfb9-ec9b-45f9-927d-688f7b3647f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¯ Model test edilir...\n",
            "ğŸ§ª Model + Validation Test...\n",
            "\n",
            "ğŸ“ Test 1:\n",
            "Orijinal: MÉ™nim fin kodum AZEDF12 olan kartÄ±m var\n",
            "Blurred:  MÉ™nim fin kodum [BLURRED] olan kartÄ±m var\n",
            "âœ… TapÄ±lan valid entity-lÉ™r:\n",
            "  âœ“ Valid - 'AZEDF12' (FIN)\n",
            "----------------------------------------------------------------------\n",
            "ğŸ“ Test 2:\n",
            "Orijinal: FIN kod AB12345 mÃ¶vcuddur\n",
            "Blurred:  FIN kod AB12345 mÃ¶vcuddur\n",
            "âŒ HeÃ§ bir valid entity tapÄ±lmadÄ±\n",
            "----------------------------------------------------------------------\n",
            "ğŸ“ Test 3:\n",
            "Orijinal: Bu 90-AB-123 nÃ¶mrÉ™li avtomobil dostumundur\n",
            "Blurred:  Bu [BLURRED] nÃ¶mrÉ™li avtomobil dostumundur\n",
            "âœ… TapÄ±lan valid entity-lÉ™r:\n",
            "  âœ“ Valid - '90-AB-123' (PLATE)\n",
            "----------------------------------------------------------------------\n",
            "ğŸ“ Test 4:\n",
            "Orijinal: ÅÉ™xsiyyÉ™t vÉ™siqÉ™ AA1234567 dir\n",
            "Blurred:  ÅÉ™xsiyyÉ™t vÉ™siqÉ™ [BLURRED] dir\n",
            "âœ… TapÄ±lan valid entity-lÉ™r:\n",
            "  âœ“ Valid - 'AA1234567' (ID)\n",
            "----------------------------------------------------------------------\n",
            "ğŸ“ Test 5:\n",
            "Orijinal: SÉ™nÉ™d nÃ¶mrÉ™si AZE123456789 tÉ™qdim edilmÉ™lidir\n",
            "Blurred:  SÉ™nÉ™d nÃ¶mrÉ™si [BLURRED] tÉ™qdim edilmÉ™lidir\n",
            "âœ… TapÄ±lan valid entity-lÉ™r:\n",
            "  âœ“ Valid - 'AZE123456789' (ID)\n",
            "----------------------------------------------------------------------\n",
            "ğŸ“ Test 6:\n",
            "Orijinal: SÉ™hv format AA12345 vÉ™ ya AZE12345 var\n",
            "Blurred:  SÉ™hv format AA12345 vÉ™ ya AZE12345 var\n",
            "âŒ HeÃ§ bir valid entity tapÄ±lmadÄ±\n",
            "----------------------------------------------------------------------\n",
            "ğŸ“ Test 7:\n",
            "Orijinal: Bu adi cÃ¼mlÉ™ dir heÃ§ nÉ™ yoxdur\n",
            "Blurred:  Bu adi cÃ¼mlÉ™ dir heÃ§ nÉ™ yoxdur\n",
            "âŒ HeÃ§ bir valid entity tapÄ±lmadÄ±\n",
            "----------------------------------------------------------------------\n",
            "ğŸ“ Test 8:\n",
            "Orijinal: FÄ°N MM4NS3L vÉ™ maÅŸÄ±n 77-KM-596 qeydiyyatÄ± var\n",
            "Blurred:  FÄ°N [BLURRED] vÉ™ maÅŸÄ±n [BLURRED] qeydiyyatÄ± var\n",
            "âœ… TapÄ±lan valid entity-lÉ™r:\n",
            "  âœ“ Valid - 'MM4NS3L' (FIN)\n",
            "  âœ“ Valid - '77-KM-596' (PLATE)\n",
            "----------------------------------------------------------------------\n",
            "ğŸ“ Test 9:\n",
            "Orijinal: Qeydiyyat Ã¼Ã§Ã¼n AZE987654321 vÉ™ 10-AB-123 lazÄ±m\n",
            "Blurred:  Qeydiyyat Ã¼Ã§Ã¼n [BLURRED] vÉ™ 10-AB-123 lazÄ±m\n",
            "âœ… TapÄ±lan valid entity-lÉ™r:\n",
            "  âœ“ Valid - 'AZE987654321' (ID)\n",
            "----------------------------------------------------------------------\n",
            "ğŸ“ Test 10:\n",
            "Orijinal: Qeydiyyat prosesindÉ™ ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™si nÃ¶mrÉ™si AZE123456789 vÉ™ FÄ°N kod MM4NS3L tÉ™qdim edilmÉ™lidir. Avtomobilin nÃ¶mrÉ™si 77-KM-596 vÉ™ 10-AB-123 ilÉ™ baÄŸlÄ± mÉ™lumatlar da sistemÉ™ daxil edilmÉ™lidir. ÆlavÉ™ olaraq, yeni qeydiyyatda tÉ™lÉ™b olunan ikinci ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™si nÃ¶mrÉ™si AZE987654321 vÉ™ FÄ°N kodu QA9WT2K dÉ™ yoxlanÄ±lacaq. Qeyd: hÉ™r hansÄ± bir kod sÉ™hv daxil edilÉ™rsÉ™, sistem xÉ™bÉ™rdarlÄ±q edÉ™cÉ™k.\n",
            "Blurred:  Qeydiyyat prosesindÉ™ ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™si nÃ¶mrÉ™si [BLURRED] vÉ™ FÄ°N kod MM4NS3L tÉ™qdim edilmÉ™lidir. Avtomobilin nÃ¶mrÉ™si [BLURRED] vÉ™ [BLURRED] ilÉ™ baÄŸlÄ± mÉ™lumatlar da sistemÉ™ daxil edilmÉ™lidir. ÆlavÉ™ olaraq, yeni qeydiyyatda tÉ™lÉ™b olunan ikinci ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™si nÃ¶mrÉ™si [BLURRED] vÉ™ FÄ°N kodu [BLURRED] dÉ™ yoxlanÄ±lacaq. Qeyd: hÉ™r hansÄ± bir kod sÉ™hv daxil edilÉ™rsÉ™, sistem xÉ™bÉ™rdarlÄ±q edÉ™cÉ™k.\n",
            "âœ… TapÄ±lan valid entity-lÉ™r:\n",
            "  âœ“ Valid - 'AZE123456789' (ID)\n",
            "  âœ“ Valid - '77-KM-596' (PLATE)\n",
            "  âœ“ Valid - '10-AB-123' (PLATE)\n",
            "  âœ“ Valid - 'AZE987654321' (ID)\n",
            "  âœ“ Valid - 'QA9WT2K' (FIN)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "======================================================================\n",
            "Ä°nteraktiv sistemÉ™ keÃ§mÉ™k istÉ™yirsinizmi? (y/n)\n"
          ]
        }
      ],
      "source": [
        "def test_enhanced_model():\n",
        "    \"\"\"\n",
        "    Validation qaydalarÄ± ilÉ™ modeli test edir\n",
        "    \"\"\"\n",
        "    test_sentences = [\n",
        "        \"MÉ™nim fin kodum AZEDF12 olan kartÄ±m var\",          # Valid FIN (7 simvol)\n",
        "        \"FIN kod AB12345 mÃ¶vcuddur\",                        # Valid FIN (7 simvol)\n",
        "        \"Bu 90-AB-123 nÃ¶mrÉ™li avtomobil dostumundur\",       # Valid PLATE\n",
        "        \"ÅÉ™xsiyyÉ™t vÉ™siqÉ™ AA1234567 dir\",                   # Valid ID (AA + 7 rÉ™qÉ™m)\n",
        "        \"SÉ™nÉ™d nÃ¶mrÉ™si AZE123456789 tÉ™qdim edilmÉ™lidir\",    # Valid ID (AZE + 9 rÉ™qÉ™m)\n",
        "        \"SÉ™hv format AA12345 vÉ™ ya AZE12345 var\",           # Invalid formats\n",
        "        \"Bu adi cÃ¼mlÉ™ dir heÃ§ nÉ™ yoxdur\",                   # HeÃ§ nÉ™ yox\n",
        "        \"FÄ°N MM4NS3L vÉ™ maÅŸÄ±n 77-KM-596 qeydiyyatÄ± var\",   # Valid entities\n",
        "        \"Qeydiyyat Ã¼Ã§Ã¼n AZE987654321 vÉ™ 10-AB-123 lazÄ±m\",   # Multiple valid entities\n",
        "        'Qeydiyyat prosesindÉ™ ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™si nÃ¶mrÉ™si AZE123456789 vÉ™ FÄ°N kod MM4NS3L tÉ™qdim edilmÉ™lidir. Avtomobilin nÃ¶mrÉ™si 77-KM-596 vÉ™ 10-AB-123 ilÉ™ baÄŸlÄ± mÉ™lumatlar da sistemÉ™ daxil edilmÉ™lidir. ÆlavÉ™ olaraq, yeni qeydiyyatda tÉ™lÉ™b olunan ikinci ÅŸÉ™xsiyyÉ™t vÉ™siqÉ™si nÃ¶mrÉ™si AZE987654321 vÉ™ FÄ°N kodu QA9WT2K dÉ™ yoxlanÄ±lacaq. Qeyd: hÉ™r hansÄ± bir kod sÉ™hv daxil edilÉ™rsÉ™, sistem xÉ™bÉ™rdarlÄ±q edÉ™cÉ™k.' #custom\n",
        "    ]\n",
        "\n",
        "    print(\"ğŸ§ª Model + Validation Test...\\n\")\n",
        "\n",
        "    for i, sentence in enumerate(test_sentences, 1):\n",
        "        print(f\"ğŸ“ Test {i}:\")\n",
        "        print(f\"Orijinal: {sentence}\")\n",
        "\n",
        "        try:\n",
        "            blurred_text, entities = blur_with_model_and_validation(sentence, model, tokenizer, device)\n",
        "            print(f\"Blurred:  {blurred_text}\")\n",
        "\n",
        "            if entities:\n",
        "                print(\"âœ… TapÄ±lan valid entity-lÉ™r:\")\n",
        "                for entity in entities:\n",
        "                    validation_result = \"âœ“ Valid\" if validate_entity_format(entity['text'], entity['type']) else \"âœ— Invalid\"\n",
        "                    print(f\"  {validation_result} - '{entity['text']}' ({entity['type']})\")\n",
        "            else:\n",
        "                print(\"âŒ HeÃ§ bir valid entity tapÄ±lmadÄ±\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ XÉ™ta: {e}\")\n",
        "\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "def detailed_analysis_demo(text: str):\n",
        "    \"\"\"\n",
        "    Detailed token-level analiz\n",
        "    \"\"\"\n",
        "    print(f\"\\nğŸ” Detailed Analiz: '{text}'\")\n",
        "\n",
        "    try:\n",
        "        tokens, model_labels = predict_entities_with_model(text, model, tokenizer, device)\n",
        "        _, validated_labels = enhanced_predict_with_validation(text, model, tokenizer, device)\n",
        "\n",
        "        print(\"\\nToken-by-token analiz:\")\n",
        "        print(\"Token\".ljust(15) + \"Model\".ljust(10) + \"Validated\".ljust(12) + \"Valid?\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for i, token in enumerate(tokens):\n",
        "            model_label = model_labels[i] if i < len(model_labels) else 'O'\n",
        "            validated_label = validated_labels[i] if i < len(validated_labels) else 'O'\n",
        "\n",
        "            # Entity-nin validation statusunu yoxla\n",
        "            if model_label.startswith('B-'):\n",
        "                entity_type = model_label[2:]\n",
        "                # Entity-nin tam mÉ™tnini tap\n",
        "                entity_tokens = [token]\n",
        "                j = i + 1\n",
        "                while (j < len(tokens) and\n",
        "                       j < len(model_labels) and\n",
        "                       model_labels[j] == f'I-{entity_type}'):\n",
        "                    entity_tokens.append(tokens[j])\n",
        "                    j += 1\n",
        "                entity_text = ''.join(entity_tokens)\n",
        "                is_valid = validate_entity_format(entity_text, entity_type)\n",
        "                valid_status = \"âœ“\" if is_valid else \"âœ—\"\n",
        "            else:\n",
        "                valid_status = \"-\"\n",
        "\n",
        "            print(f\"{token:<15} {model_label:<10} {validated_label:<12} {valid_status}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Analiz xÉ™tasÄ±: {e}\")\n",
        "\n",
        "def interactive_blur_system():\n",
        "    \"\"\"\n",
        "    Ä°nteraktiv blurring sistemi\n",
        "    \"\"\"\n",
        "    print(\"ğŸ¤– AzÉ™rbaycan NER Blur Sistemi\")\n",
        "    print(\"Model + Validation qaydalarÄ± istifadÉ™ edilir\")\n",
        "    print(\"Format qaydalarÄ±:\")\n",
        "    print(\"  â€¢ FIN: 7 simvol (hÉ™rf+rÉ™qÉ™m)\")\n",
        "    print(\"  â€¢ ID/AA: 9 simvol (AA + 7 rÉ™qÉ™m)\")\n",
        "    print(\"  â€¢ ID/AZE: 12 simvol (AZE + 9 rÉ™qÉ™m)\")\n",
        "    print(\"  â€¢ Avtomobil: XX-YY-ZZZ\")\n",
        "    print(\"\\nÃ‡Ä±xmaq Ã¼Ã§Ã¼n 'exit' yazÄ±n\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"ğŸ“ MÉ™tn daxil edin: \")\n",
        "\n",
        "        if user_input.lower() in ['exit', 'Ã§Ä±x', 'quit', 'q']:\n",
        "            print(\"ğŸ‘‹ GÃ¶rÃ¼ÅŸÉ™nÉ™dÉ™k!\")\n",
        "            break\n",
        "\n",
        "        if user_input.strip():\n",
        "            try:\n",
        "                blurred_text, entities = blur_with_model_and_validation(user_input, model, tokenizer, device)\n",
        "                print(f\"ğŸ”’ Blurred: {blurred_text}\")\n",
        "\n",
        "                if entities:\n",
        "                    print(\"ğŸ“ TapÄ±lan entity-lÉ™r:\")\n",
        "                    for entity in entities:\n",
        "                        print(f\"  â€¢ '{entity['text']}' â†’ {entity['type']}\")\n",
        "                else:\n",
        "                    print(\"ğŸ“ HeÃ§ bir entity tapÄ±lmadÄ±\")\n",
        "\n",
        "                # Detailed analizi gÃ¶stÉ™r\n",
        "                detailed_analysis_demo(user_input)\n",
        "                print()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ XÉ™ta: {e}\\n\")\n",
        "        else:\n",
        "            print(\"âŒ BoÅŸ mÉ™tn daxil etdiniz\\n\")\n",
        "\n",
        "# ÆgÉ™r model Ã¶yrÉ™dilib vÉ™ mÃ¶vcuddursa test et\n",
        "if 'model' in locals():\n",
        "    print(\"ğŸ¯ Model test edilir...\")\n",
        "    test_enhanced_model()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Ä°nteraktiv sistemÉ™ keÃ§mÉ™k istÉ™yirsinizmi? (y/n)\")\n",
        "    # interactive_blur_system()\n",
        "else:\n",
        "    print(\"âŒ Model É™vvÉ™lcÉ™ Ã¶yrÉ™dilmÉ™lidir!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueDDhD7oVvue",
        "outputId": "4884c3c8-da95-45f1-f61e-7158831078a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "zip error: Nothing to do! (/content/best_model.zip)\n"
          ]
        }
      ],
      "source": [
        "!zip -r /content/best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Okj6QqP5cBh5",
        "outputId": "bc8b4a03-036d-4544-fca2-445691fa92ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/best_model/ (stored 0%)\n",
            "  adding: content/best_model/model.safetensors (deflated 7%)\n",
            "  adding: content/best_model/config.json (deflated 55%)\n",
            "  adding: content/best_model/training_args.bin (deflated 53%)\n",
            "  adding: content/best_model/vocab.txt (deflated 45%)\n",
            "  adding: content/best_model/special_tokens_map.json (deflated 42%)\n",
            "  adding: content/best_model/tokenizer.json (deflated 67%)\n",
            "  adding: content/best_model/tokenizer_config.json (deflated 75%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r /content/best_model.zip /content/best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqXE37AXcN-2",
        "outputId": "f2a229ad-b1f9-47a4-a2d1-c279e474dc37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 659003257 Sep 10 21:04 /content/best_model.zip\n"
          ]
        }
      ],
      "source": [
        "!ls -l /content/best_model.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ua6xAasrccax"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}