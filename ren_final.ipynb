{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bahramzada/az-ner-blur/blob/main/ren_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXsru22SSgBW",
        "outputId": "e49c8bc9-a92e-461a-8a0b-950f92a13e84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nümunə cümlələr:\n",
            "1. Təhlükəsizlik əməkdaşı 85-NL-695 nömrəsini soruşdu.\n",
            "2. Mən 95-MA-819 nömrəsini gördüm.\n",
            "3. Avtomobil 54-GY-552 nömrəsi qara maşında yazılıb.\n",
            "4. 66-KO-487 nömrəli maşın çox böyükdür.\n",
            "5. Avtomobil nömrəsi 85-IZ-670 qeydiyyatdan keçdi.\n",
            "6. Bu qəza ilə bağlı 11-SE-498 nömrəli avtomobilin adı hallanır.\n",
            "7. Məlumat bazasında 50-LE-457 nömrəsi ilə axtarış aparıldı.\n",
            "8. O, öz avtomobilinin nömrəsini, 28-TW-112 nömrəsini xatırladı.\n",
            "9. Avtomobilin nömrəsi 80-EL-861 və rəngi ağdır.\n",
            "10. Dünən 57-MV-270 nömrəli avtomobili gördüm.\n",
            "\n",
            "==================================================\n",
            "Cəmi 10000 cümlə yaradıldı.\n",
            "10000 cümlə car_plate_dataset.txt faylında saxlanıldı.\n",
            "CSV formatında da car_plate_dataset.csv faylında saxlanıldı.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "def generate_car_plate():\n",
        "    \"\"\"99-XX-999 formatında avtomobil nömrəsi yaradır\"\"\"\n",
        "    # İlk iki rəqəm\n",
        "    first_digits = f\"{random.randint(10, 99)}\"\n",
        "\n",
        "    # İki hərf\n",
        "    letters = ''.join(random.choices(string.ascii_uppercase, k=2))\n",
        "\n",
        "    # Son üç rəqəm\n",
        "    last_digits = f\"{random.randint(100, 999)}\"\n",
        "\n",
        "    return f\"{first_digits}-{letters}-{last_digits}\"\n",
        "\n",
        "def generate_sentences_with_car_plates(num_sentences=1000):\n",
        "    \"\"\"Avtomobil nömrələri olan cümlələr yaradır\"\"\"\n",
        "\n",
        "    # Müxtəlif cümlə şablonları\n",
        "    sentence_templates = [\n",
        "        \"Avtomobil nömrəsi {} olan maşın yolda gedirdi.\",\n",
        "        \"{} nömrəli avtomobil sürətlə keçdi.\",\n",
        "        \"Mən {} nömrəsini gördüm.\",\n",
        "        \"{} nömrəli maşın dayanacaqda idi.\",\n",
        "        \"Polis {} nömrəli avtomobili dayandırdı.\",\n",
        "        \"Bu gün {} nömrəsini qeyd etdim.\",\n",
        "        \"{} nömrəli avtomobil qırmızı işıqda dayandı.\",\n",
        "        \"Qonşumun avtomobil nömrəsi {}dır.\",\n",
        "        \"{} nömrəli maşın çox sürətlə gedirdi.\",\n",
        "        \"Avtomobil {} nömrəsi ilə qeydiyyatdan keçib.\",\n",
        "        \"Mən {} nömrəli avtomobili tanıyıram.\",\n",
        "        \"{} nömrəsində olan maşın ağ rəngdədir.\",\n",
        "        \"Dünən {} nömrəli avtomobili gördüm.\",\n",
        "        \"Bu {} nömrəli maşın kimə məxsusdur?\",\n",
        "        \"{} nömrəli avtomobil yeni alınıb.\",\n",
        "        \"Parklama yerində {} nömrəsi var idi.\",\n",
        "        \"{} nömrəli maşın təmirə ehtiyacı var.\",\n",
        "        \"Avtomobil nömrəsi {} olan sürücü təcrübəlidir.\",\n",
        "        \"Mənim dostumun avtomobil nömrəsi {}dır.\",\n",
        "        \"{} nömrəli avtomobil bazarda satılır.\",\n",
        "        \"Bu səhər {} nömrəsini yolda gördüm.\",\n",
        "        \"{} nömrəli maşın çox böyükdür.\",\n",
        "        \"Avtomobil {} nömrəsi qara maşında yazılıb.\",\n",
        "        \"Mən {} nömrəsini unutmuşdum.\",\n",
        "        \"{} nömrəli avtomobil həftəsonu istifadə olunur.\",\n",
        "        \"Bu {} avtomobil nömrəsi çox maraqlıdır.\",\n",
        "        \"{} nömrəli maşın əla vəziyyətdədir.\",\n",
        "        \"Avtomobil nömrəsi {} yaddaşımda qalıb.\",\n",
        "        \"Dünən axşam {} nömrəli avtomobil gəldi.\",\n",
        "        \"{} nömrəsini polisə bildirdim.\",\n",
        "        \"Hadisə yerindən {} nömrəli avtomobil uzaqlaşdı.\",\n",
        "        \"Şəhər kameraları {} nömrəli maşını qeydə aldı.\",\n",
        "        \"{} nömrəsi ilə icarəyə götürülən avtomobil qaytarıldı.\",\n",
        "        \"Təhlükəsizlik əməkdaşı {} nömrəsini soruşdu.\",\n",
        "        \"Bu avtomobilin nömrəsi {} olaraq qeyd edilib.\",\n",
        "        \"Avtomobilin texniki baxışı {} nömrəsinə uyğundur.\",\n",
        "        \"{} nömrəli maşın yol kənarında saxlanılıb.\",\n",
        "        \"Məlumat bazasında {} nömrəsi ilə axtarış aparıldı.\",\n",
        "        \"O, öz avtomobilinin nömrəsini, {} nömrəsini xatırladı.\",\n",
        "        \"{} nömrəli avtomobilin sahibi kimdir?\",\n",
        "        \"Kömək üçün {} nömrəli maşın çağırıldı.\",\n",
        "        \"Avtomobil nömrəsi {} qeydiyyatdan keçdi.\",\n",
        "        \"Nəqliyyatın hərəkətini izləmək üçün {} nömrəsindən istifadə edildi.\",\n",
        "        \"Gömrükdə {} nömrəli avtomobil yoxlanıldı.\",\n",
        "        \"Bu qəza ilə bağlı {} nömrəli avtomobilin adı hallanır.\",\n",
        "        \"{} nömrəli maşının təkərləri dəyişdirildi.\",\n",
        "        \"Avtomobilin nömrəsi {} və rəngi ağdır.\",\n",
        "        \"Bu avtomobilin nömrəsi {} olaraq dəyişdiriləcək.\",\n",
        "        \"Tədbirdə iştirak edən hər bir avtomobilin nömrəsi, o cümlədən {} qeyd olundu.\"\n",
        "    ]\n",
        "    sentences = []\n",
        "\n",
        "    for _ in range(num_sentences):\n",
        "        # Təsadüfi şablon seç\n",
        "        template = random.choice(sentence_templates)\n",
        "\n",
        "        # Avtomobil nömrəsi yarat\n",
        "        car_plate = generate_car_plate()\n",
        "\n",
        "        # Cümləni yaradın\n",
        "        sentence = template.format(car_plate)\n",
        "        sentences.append(sentence)\n",
        "\n",
        "    return sentences\n",
        "\n",
        "def save_dataset(sentences, filename=\"car_plate_dataset.txt\"):\n",
        "    \"\"\"Dataset-i fayla saxlayır\"\"\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        for sentence in sentences:\n",
        "            f.write(sentence + '\\n')\n",
        "    print(f\"{len(sentences)} cümlə {filename} faylında saxlanıldı.\")\n",
        "\n",
        "def main():\n",
        "    # 1000 cümlə yaradın\n",
        "    sentences = generate_sentences_with_car_plates(10000)\n",
        "\n",
        "    # İlk 10 cümləni göstər\n",
        "    print(\"Nümunə cümlələr:\")\n",
        "    for i, sentence in enumerate(sentences[:10], 1):\n",
        "        print(f\"{i}. {sentence}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"Cəmi {len(sentences)} cümlə yaradıldı.\")\n",
        "\n",
        "    # Fayla saxla\n",
        "    save_dataset(sentences)\n",
        "\n",
        "    # CSV formatında da saxla\n",
        "    save_dataset_csv(sentences)\n",
        "\n",
        "def save_dataset_csv(sentences, filename=\"car_plate_dataset.csv\"):\n",
        "    \"\"\"Dataset-i CSV formatında saxlayır\"\"\"\n",
        "    import csv\n",
        "    import re\n",
        "\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['sentence', 'car_plate'])\n",
        "\n",
        "        for sentence in sentences:\n",
        "            # Avtomobil nömrəsini cümlədan çıxarın\n",
        "            car_plate_pattern = r'\\d{2}-[A-Z]{2}-\\d{3}'\n",
        "            match = re.search(car_plate_pattern, sentence)\n",
        "            if match:\n",
        "                car_plate = match.group()\n",
        "                writer.writerow([sentence, car_plate])\n",
        "\n",
        "    print(f\"CSV formatında da {filename} faylında saxlanıldı.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRCH-RXwSf_V",
        "outputId": "0a87f736-d25f-449f-dba2-96240c0d0f08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV fayl adını daxil edin (məs: sentences.csv): \n",
            "10000 cümlə annotate edildi və /content/car_plate_dataset_annotated.json faylında saxlanıldı.\n",
            "\n",
            "Nümunə 5 annotated cümlə:\n",
            "================================================================================\n",
            "1. Təhlükəsizlik əməkdaşı 85-NL-695 nömrəsini soruşdu.\n",
            "   -> Tapılan: '85-NL-695' (mövqe: 23-32)\n",
            "----------------------------------------\n",
            "2. Mən 95-MA-819 nömrəsini gördüm.\n",
            "   -> Tapılan: '95-MA-819' (mövqe: 4-13)\n",
            "----------------------------------------\n",
            "3. Avtomobil 54-GY-552 nömrəsi qara maşında yazılıb.\n",
            "   -> Tapılan: '54-GY-552' (mövqe: 10-19)\n",
            "----------------------------------------\n",
            "4. 66-KO-487 nömrəli maşın çox böyükdür.\n",
            "   -> Tapılan: '66-KO-487' (mövqe: 0-9)\n",
            "----------------------------------------\n",
            "5. Avtomobil nömrəsi 85-IZ-670 qeydiyyatdan keçdi.\n",
            "   -> Tapılan: '85-IZ-670' (mövqe: 18-27)\n",
            "----------------------------------------\n",
            "\n",
            "✅ Uğurla tamamlandı!\n",
            "📁 Input: /content/car_plate_dataset.csv\n",
            "📁 Output: /content/car_plate_dataset_annotated.json\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import json\n",
        "import re\n",
        "\n",
        "def find_car_plate_positions(text):\n",
        "    \"\"\"Mətndə avtomobil nömrələrinin mövqelərini tapır\"\"\"\n",
        "    car_plate_pattern = r'\\d{2}-[A-Z]{2}-\\d{3}'\n",
        "    entities = []\n",
        "\n",
        "    for match in re.finditer(car_plate_pattern, text):\n",
        "        start_pos = match.start()\n",
        "        end_pos = match.end()\n",
        "        entities.append([start_pos, end_pos, \"CAR_PLATE\"])\n",
        "\n",
        "    return entities\n",
        "\n",
        "def annotate_csv_file(input_csv_file, output_json_file):\n",
        "    \"\"\"CSV faylını oxuyub JSON formatında annotate edir\"\"\"\n",
        "    annotated_data = []\n",
        "\n",
        "    # CSV faylını oxu\n",
        "    with open(input_csv_file, 'r', encoding='utf-8') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "\n",
        "        # Header-i atla (əgər varsa)\n",
        "        try:\n",
        "            first_row = next(reader)\n",
        "            # Əgər ilk sətir header deyilsə, onu da emal et\n",
        "            if not (first_row[0].lower() in ['sentence', 'text', 'cümle']):\n",
        "                sentence = first_row[0]\n",
        "                entities = find_car_plate_positions(sentence)\n",
        "                annotation = {\"entities\": entities}\n",
        "                annotated_data.append([sentence, annotation])\n",
        "        except StopIteration:\n",
        "            print(\"CSV faylı boşdur!\")\n",
        "            return\n",
        "\n",
        "        # Qalan sətirləri emal et\n",
        "        for row in reader:\n",
        "            if row:  # Boş sətirləri atla\n",
        "                sentence = row[0]  # İlk sütunu cümlə kimi götür\n",
        "                entities = find_car_plate_positions(sentence)\n",
        "                annotation = {\"entities\": entities}\n",
        "                annotated_data.append([sentence, annotation])\n",
        "\n",
        "    # JSON faylında saxla\n",
        "    with open(output_json_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(annotated_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"{len(annotated_data)} cümlə annotate edildi və {output_json_file} faylında saxlanıldı.\")\n",
        "\n",
        "    return annotated_data\n",
        "\n",
        "def print_sample_results(data, num_samples=5):\n",
        "    \"\"\"Nümunə nəticələri göstər\"\"\"\n",
        "    print(f\"\\nNümunə {min(num_samples, len(data))} annotated cümlə:\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for i, (sentence, annotation) in enumerate(data[:num_samples]):\n",
        "        print(f\"{i+1}. {sentence}\")\n",
        "        if annotation['entities']:\n",
        "            for entity in annotation['entities']:\n",
        "                start, end, label = entity\n",
        "                car_plate = sentence[start:end]\n",
        "                print(f\"   -> Tapılan: '{car_plate}' (mövqe: {start}-{end})\")\n",
        "        else:\n",
        "            print(\"   -> Avtomobil nömrəsi tapılmadı\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "def main():\n",
        "    # CSV fayl adını daxil edin\n",
        "    input_csv = input(\"CSV fayl adını daxil edin (məs: sentences.csv): \").strip()\n",
        "    if not input_csv:\n",
        "        input_csv = \"/content/car_plate_dataset.csv\"\n",
        "\n",
        "    # Output JSON fayl adı\n",
        "    output_json = input_csv.replace('.csv', '_annotated.json')\n",
        "\n",
        "    try:\n",
        "        # CSV-ni annotate et\n",
        "        data = annotate_csv_file(input_csv, output_json)\n",
        "\n",
        "        # Nümunə nəticələri göstər\n",
        "        print_sample_results(data)\n",
        "\n",
        "        print(f\"\\n✅ Uğurla tamamlandı!\")\n",
        "        print(f\"📁 Input: {input_csv}\")\n",
        "        print(f\"📁 Output: {output_json}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ XƏTA: '{input_csv}' faylı tapılmadı!\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ XƏTA: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UONzBJzpSf9J"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def random_aze_id():\n",
        "    # AZE + 7 və ya 9 rəqəm (random seçim)\n",
        "    digits = ''.join([str(random.randint(0, 9)) for _ in range(9)])\n",
        "    return \"AZE\" + digits\n",
        "\n",
        "def random_aa_id():\n",
        "    # AA + 7 rəqəm\n",
        "    digits = ''.join([str(random.randint(0, 9)) for _ in range(7)])\n",
        "    return \"AA\" + digits\n",
        "\n",
        "templates = [\n",
        "    \"Mənim şəxsiyyət vəsiqə nömrəm {}-dir.\",\n",
        "    \"Şəxsiyyət vəsiqə nömrəsi {}-dir.\",\n",
        "    \"Vəsiqə nömrəm {}-dir.\",\n",
        "    \"Zəhmət olmasa, şəxsiyyət vəsiqə nömrənizi qeyd edin: {}\",\n",
        "    \"Adı: Bahram Zada, Vəsiqə nömrəsi: {}\",\n",
        "    \"Şəxsiyyət vəsiqə nömrəsi: {}\",\n",
        "    \"Mənim {} şəxsiyyət vəsiqə nömrəmdir.\",\n",
        "    \"Şəxsiyyət vəsiqə nömrəsini yoxlamaq üçün {} daxil edin.\",\n",
        "    \"Vəsiqə nömrəsi olmadan qeydiyyat mümkün deyil: {}\",\n",
        "    \"Sizin şəxsiyyət vəsiqə nömrəniz {}-dür?\",\n",
        "    \"Qeydiyyat üçün şəxsiyyət vəsiqə nömrəsi tələb olunur: {}\",\n",
        "    \"İstifadəçinin şəxsiyyət vəsiqə nömrəsi: {}\",\n",
        "    \"Aşağıda göstərilən vəsiqə nömrəsini yoxlayın: {}\",\n",
        "    \"Sistemə giriş üçün {} vəsiqə nömrəsini daxil edin.\",\n",
        "    \"Sənəd məlumatları: şəxsiyyət vəsiqə nömrəsi - {}\",\n",
        "    \"{} şəxsiyyət vəsiqə nömrəsini təsdiqləyin.\",\n",
        "    \"Yuxarıda qeyd olunan {} vəsiqə nömrəsidir.\",\n",
        "    \"Əlavə məlumat üçün {} şəxsiyyət vəsiqə nömrəsini istifadə edin.\",\n",
        "    \"Şəxsiyyətinizi təsdiqləmək üçün {} nömrəsini yazın.\",\n",
        "    \"Əgər şəxsiyyət vəsiqə nömrəniz {}-dirsə, davam edin.\",\n",
        "    \"Qeydiyyat zamanı istifadə etdiyiniz vəsiqə nömrəsi: {}\",\n",
        "    \"Şəxsiyyət vəsiqə nömrəniz bir daha təsdiqlənir: {}\",\n",
        "    \"Formada yazılan şəxsiyyət vəsiqə nömrəsi {}-dir.\",\n",
        "    \"Sizdən tələb olunan şəxsiyyət vəsiqə nömrəsi: {}\",\n",
        "    \"Aşağıda göstərilən {} nömrəsi sizin vəsiqənizdir.\",\n",
        "    \"Profilinizdə qeyd olunan şəxsiyyət vəsiqə nömrəsi: {}\",\n",
        "    \"Şəxsiyyət vəsiqə nömrəsi olmadan qeydiyyat mümkün deyil, nömrəniz: {}\",\n",
        "    \"Sistemə daxil olmaq üçün şəxsiyyət vəsiqə nömrənizi {} daxil edin.\",\n",
        "    \"Qeydiyyat formasında {} şəxsiyyət vəsiqə nömrəsini yazın.\",\n",
        "    \"Məlumat bazasında saxlanılan vəsiqə nömrəsi: {}\",\n",
        "    \"Müraciət üçün {} şəxsiyyət vəsiqə nömrəsi vacibdir.\",\n",
        "    \"Yalnız {} nömrəsi olan şəxs xidmətdən yararlana bilər.\",\n",
        "    \"{} şəxsiyyət vəsiqə nömrəsi ilə əlaqəli sənədlər qəbul edildi.\",\n",
        "    \"Ödənişi etmək üçün {} şəxsiyyət vəsiqə nömrənizi göstərin.\",\n",
        "    \"Ərizəyə {} vəsiqə nömrəsi ilə müraciət edə bilərsiniz.\",\n",
        "    \"Hər hansı bir dəyişiklik üçün {} nömrəsi tələb olunur.\",\n",
        "    \"{} nömrəsi yoxlandı və təsdiq edildi.\",\n",
        "    \"Bu hesabın sahibi {} şəxsiyyət vəsiqə nömrəsinə malikdir.\",\n",
        "    \"Vəsiqənin üzərində {} nömrəsi qeyd olunub.\",\n",
        "    \"{} nömrəsi ilə bağlı bütün məlumatlar doğrudur.\",\n",
        "    \"Giriş üçün {} şəxsiyyət vəsiqə nömrəsini yenidən daxil edin.\",\n",
        "    \"Lütfən, {} nömrəsini əlavə edin.\",\n",
        "    \"Bu şəxsiyyət vəsiqəsi nömrəsi, {} yeni verilmişdir.\",\n",
        "    \"Sığorta əməliyyatı {} nömrəsinə əsaslanır.\",\n",
        "    \"Təsdiqləmə kodu {} vəsiqə nömrənizə göndərildi.\",\n",
        "    \"{} vəsiqə nömrəsi məlumat bazasına daxil edilmişdir.\",\n",
        "    \"Xidmət haqqını ödəmək üçün {} nömrəsi lazımdır.\",\n",
        "    \"Sistemdə {} nömrəsi ilə bağlı heç bir qeyd tapılmadı.\",\n",
        "    \"Ödənişin təsdiqi üçün {} nömrəsini göstərin.\",\n",
        "    \"{} nömrəsi ilə bağlı bütün məlumatlar qorunur.\"\n",
        "]\n",
        "\n",
        "def generate_sentences(n=1000, aze_ratio=0.7):\n",
        "    sentences = []\n",
        "    for _ in range(n):\n",
        "        template = random.choice(templates)\n",
        "        # 70% AZE formatı, 30% AA formatı\n",
        "        if random.random() < aze_ratio:\n",
        "            id_num = random_aze_id()\n",
        "        else:\n",
        "            id_num = random_aa_id()\n",
        "        sentence = template.format(id_num)\n",
        "        sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sentences = generate_sentences(10000)\n",
        "    with open(\"vesiqe_numune.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for sentence in sentences:\n",
        "            f.write(sentence + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOJvkk6tSf7Q"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "# Regex patternləri\n",
        "pattern_aze = r'AZE\\d{9}'         # AZE + 9 rəqəm\n",
        "pattern_aa = r'AA\\d{7}'           # AA + 7 rəqəm\n",
        "\n",
        "label = \"ID_NUMBER\"\n",
        "\n",
        "def label_sentences(sentences):\n",
        "    labeled_data = []\n",
        "    for sentence in sentences:\n",
        "        entities = []\n",
        "        # AZE üçün\n",
        "        for match in re.finditer(pattern_aze, sentence):\n",
        "            start, end = match.start(), match.end()\n",
        "            entities.append((start, end, label))\n",
        "        # AA üçün\n",
        "        for match in re.finditer(pattern_aa, sentence):\n",
        "            start, end = match.start(), match.end()\n",
        "            entities.append((start, end, label))\n",
        "        if entities:\n",
        "            labeled_data.append((sentence, {\"entities\": entities}))\n",
        "    return labeled_data\n",
        "\n",
        "# Fayldan cümlələri oxu\n",
        "with open(\"vesiqe_numune.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    sentences = [line.strip() for line in f.readlines()]\n",
        "\n",
        "labeled_data = label_sentences(sentences)\n",
        "\n",
        "# Annotasiya formatlı JSON-a yaz\n",
        "with open(\"vesiqe_annotated.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(labeled_data, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RW8pp1cISf5N"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def random_fin():\n",
        "    # İcazəli hərflər: İngilis əlifbası, amma \"I\" və \"O\" olmamalıdır\n",
        "    letters = [chr(c) for c in range(ord('A'), ord('Z')+1) if chr(c) not in ['I', 'O']]\n",
        "    digits = [str(d) for d in range(10)]\n",
        "    allowed = letters + digits\n",
        "    fin = ''.join(random.choice(allowed) for _ in range(7))\n",
        "    return fin\n",
        "\n",
        "fin_templates = [\n",
        "    \"Mənim FİN kodum {}-dur.\",\n",
        "    \"FİN kodu: {}\",\n",
        "    \"Zəhmət olmasa, FİN kodunuzu daxil edin: {}\",\n",
        "    \"Sizin FİN kodunuz {}-dir?\",\n",
        "    \"Qeydiyyat üçün tələb olunan FİN kodu: {}\",\n",
        "    \"Formada yazılan FİN kodu {}-dir.\",\n",
        "    \"Əlavə məlumat üçün FİN kodu: {}\",\n",
        "    \"Profilinizdə qeyd olunan FİN kodu: {}\",\n",
        "    \"FİN kodunuzu yoxlamaq üçün {} daxil edin.\",\n",
        "    \"FİN kodu olmadan qeydiyyat mümkün deyil: {}\",\n",
        "    \"Sistemdə qeydiyyatdan keçmək üçün FİN kodunuzu daxil edin: {}\",\n",
        "    \"Sizin şəxsiyyətinizi təsdiqləyən FİN kod: {}\",\n",
        "    \"FİN kodunuz bir daha təsdiqlənir: {}\",\n",
        "    \"Aşağıda göstərilən FİN kodunu yoxlayın: {}\",\n",
        "    \"Sənəd məlumatları: FİN kodu - {}\",\n",
        "    \"Qeydiyyat formasında {} FİN kodunu yazın.\",\n",
        "    \"Məlumat bazasında saxlanılan FİN kodu: {}\",\n",
        "    \"Sizdən tələb olunan FİN kodu: {}\",\n",
        "    \"FİN kodu olmadan əməliyyat davam etmir: {}\",\n",
        "    \"Sistemə giriş üçün {} FİN kodunu daxil edin.\",\n",
        "    \"Sənədin üzərində yazılmış FİN kodu {}-dir.\",\n",
        "    \"FİN kodunuzu təsdiqləyin: {}\",\n",
        "    \"FİN kodu sahəsinə {} yazın.\",\n",
        "    \"Şəxsiyyəti təsdiqləmək üçün {} FİN kodunu daxil edin.\",\n",
        "    \"Sizin üçün yaradılmış FİN kodu: {}\",\n",
        "    \"Sənədə əlavə olunan FİN kodu: {}\",\n",
        "    \"FİN kodu tələb olunduqda {} təqdim edin.\",\n",
        "    \"Sistemdə mövcud FİN kodu: {}\",\n",
        "    \"FİN kodunu dəyişmək üçün köhnə kod: {}\",\n",
        "    \"Əgər FİN kodunuz {}-dirsə, davam edin.\",\n",
        "    \"FİN kodunuzu unutmusunuzsa, yeni kod alın: {}\",\n",
        "    \"Təsdiqlənmiş FİN kodu: {}\",\n",
        "    \"Yeni qeydiyyat üçün FİN kodu: {}\",\n",
        "    \"FİN kodu olmadan qeydiyyat mümkün deyil, kodunuz: {}\",\n",
        "    \"Aşağıda göstərilən {} kodu sizin FİN kodunuzdur.\",\n",
        "    \"FİN kodu ilə bağlı sualınız varsa, kod: {}\",\n",
        "    \"Məlumat formasında FİN kodu: {}\",\n",
        "    \"Sistemə daxil olmaq üçün FİN kodunuzu {} yazın.\",\n",
        "    \"FİN kodunuzun düzgünlüyünü yoxlayın: {}\",\n",
        "    \"Şəxsiyyət vəsiqəsi və FİN kodu: {}\",\n",
        "    \"FİN kodu olmadan sənəd qəbul edilmir: {}\",\n",
        "    \"FİN kodu qutusuna {} yazın.\",\n",
        "    \"FİN kodu səhvdirsə, yenisini daxil edin: {}\",\n",
        "    \"Hər hansı əməliyyat üçün FİN kodu {} mütləqdir.\",\n",
        "    \"Məlumatlarınızın təhlükəsizliyi üçün FİN kodunuzu {} ilə təsdiqləyin.\",\n",
        "    \"Daxil etdiyiniz FİN kodu {} məlumatlarımızla uyğun gəlmir.\",\n",
        "    \"Bank xidmətlərindən istifadə üçün FİN kodunuz: {}\",\n",
        "    \"Şəxsi məlumatlarınızı yoxlamaq üçün {} FİN kodunuzu daxil edin.\",\n",
        "    \"FİN kodu {} ilə bağlı bütün sənədlər qəbul edildi.\",\n",
        "    \"İstifadəçinin FİN kodu {} olaraq qeyd olundu.\",\n",
        "    \"Bu sənədə uyğun FİN kodu {}-dir.\",\n",
        "    \"Yeni bank kartı almaq üçün {} FİN kodu tələb olunur.\",\n",
        "    \"FİN kodu {} olan şəxs sistemə daxil oldu.\",\n",
        "    \"Sistemdə {} FİN kodu ilə axtarış aparıldı.\",\n",
        "    \"{} FİN kodu ilə ödəniş uğurla tamamlandı.\",\n",
        "    \"Vergi ödənişi üçün FİN kodunuzu {} yazın.\",\n",
        "    \"Maliyyə əməliyyatlarını yoxlamaq üçün {} FİN kodunu təqdim edin.\",\n",
        "]\n",
        "\n",
        "def generate_fin_sentences(n=500):\n",
        "    sentences = []\n",
        "    for _ in range(n):\n",
        "        template = random.choice(fin_templates)\n",
        "        fin_code = random_fin()\n",
        "        sentence = template.format(fin_code)\n",
        "        sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sentences = generate_fin_sentences(10000)\n",
        "    with open(\"fin_numune.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for sentence in sentences:\n",
        "            f.write(sentence + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsdxqkonSf3R"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "# FİN kodu: 7 simvol, böyük hərf və rəqəm, \"I\" və \"O\" istisna\n",
        "# Pattern: [A-HJ-NP-Z0-9]{7} (A-Z, amma I və O olmadan, 0-9)\n",
        "pattern_fin = r'\\b[A-HJ-NP-Z0-9]{7}\\b'\n",
        "label = \"FIN_CODE\"\n",
        "\n",
        "def label_sentences(sentences):\n",
        "    labeled_data = []\n",
        "    for sentence in sentences:\n",
        "        entities = []\n",
        "        for match in re.finditer(pattern_fin, sentence):\n",
        "            start, end = match.start(), match.end()\n",
        "            entities.append((start, end, label))\n",
        "        if entities:\n",
        "            labeled_data.append((sentence, {\"entities\": entities}))\n",
        "    return labeled_data\n",
        "\n",
        "# Fayldan cümlələri oxu\n",
        "with open(\"fin_numune.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    sentences = [line.strip() for line in f.readlines()]\n",
        "\n",
        "labeled_data = label_sentences(sentences)\n",
        "\n",
        "# Annotasiyanı JSON-a yaz\n",
        "with open(\"fin_annotated.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(labeled_data, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVf6b1vMSf0l"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/sample_data /content/car_plate_dataset.csv /content/car_plate_dataset.txt /content/fin_numune.txt /content/vesiqe_numune.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpTc0oZURgJG",
        "outputId": "e8b69167-e20c-432b-e003-1a09b9992ec5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Bütün kitabxanalar yükləndi\n",
            "🔥 PyTorch versiyası: 2.8.0+cu126\n",
            "🚀 CUDA mövcuddur: True\n",
            "💻 İstifadə ediləcək device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Import-lar\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from typing import List, Tuple, Dict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForTokenClassification\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✅ Bütün kitabxanalar yükləndi\")\n",
        "print(f\"🔥 PyTorch versiyası: {torch.__version__}\")\n",
        "print(f\"🚀 CUDA mövcuddur: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Device ayarla\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"💻 İstifadə ediləcək device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiVTTom2RyAN",
        "outputId": "263e9dae-8468-44c8-875f-314a7943a507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📋 Konfiqurasiya:\n",
            "Model: bert-base-multilingual-cased\n",
            "Max Length: 128\n",
            "Batch Size: 16\n",
            "Epochs: 3\n",
            "Learning Rate: 2e-05\n",
            "Labels: ['O', 'B-PLATE', 'I-PLATE', 'B-FIN', 'I-FIN', 'B-ID', 'I-ID']\n"
          ]
        }
      ],
      "source": [
        "# Model və training konfiqurasiyası\n",
        "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 3\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "# Label sistemi - datasetinizdən çıxan nəticələrə görə\n",
        "LABELS = [\n",
        "    'O',           # Outside\n",
        "    'B-PLATE',     # Beginning of Car Plate\n",
        "    'I-PLATE',     # Inside Car Plate\n",
        "    'B-FIN',       # Beginning of FIN Code\n",
        "    'I-FIN',       # Inside FIN Code\n",
        "    'B-ID',        # Beginning of ID Number\n",
        "    'I-ID'         # Inside ID Number\n",
        "]\n",
        "\n",
        "# Label mapping\n",
        "label2id = {label: i for i, label in enumerate(LABELS)}\n",
        "id2label = {i: label for i, label in enumerate(LABELS)}\n",
        "\n",
        "print(\"📋 Konfiqurasiya:\")\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Max Length: {MAX_LEN}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"Labels: {LABELS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnbGL2lSR0Jl",
        "outputId": "3a054e81-4273-4de6-a836-4ac6f70d44e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📏 Entity Format Qaydaları:\n",
            "🔹 FIN kod: 7 simvol (hərf+rəqəm) - məsələn: AZEDF12\n",
            "🔹 ID/AA: 9 simvol (AA + 7 rəqəm) - məsələn: AA1234567\n",
            "🔹 ID/AZE: 12 simvol (AZE + 9 rəqəm) - məsələn: AZE123456789\n",
            "🔹 Avtomobil: XX-YY-ZZZ formatı - məsələn: 90-AB-123\n"
          ]
        }
      ],
      "source": [
        "def validate_entity_format(text: str, entity_type: str) -> bool:\n",
        "    \"\"\"\n",
        "    Entity-nin formatını doğrulayır\n",
        "    Format qaydaları:\n",
        "    - FIN kod: 7 simvol (hərf+rəqəm)\n",
        "    - ID/AA: AA + 7 rəqəm (9 simvol)\n",
        "    - ID/AZE: AZE + 9 rəqəm (12 simvol)\n",
        "    - Avtomobil: XX-YY-ZZZ formatı\n",
        "    \"\"\"\n",
        "    if entity_type == 'FIN':\n",
        "        # FIN kod 7 simvol olmalıdır (hərf və rəqəm qarışığı)\n",
        "        return len(text) == 7 and re.match(r'^[A-Z0-9]{7}$', text)\n",
        "\n",
        "    elif entity_type == 'ID':\n",
        "        # Seriya nömrəsi qaydaları\n",
        "        if text.startswith('AA'):\n",
        "            # AA ilə başlayırsa 9 simvol (AA + 7 rəqəm)\n",
        "            return len(text) == 9 and re.match(r'^AA\\d{7}$', text)\n",
        "        elif text.startswith('AZE'):\n",
        "            # AZE ilə başlayırsa 12 simvol (AZE + 9 rəqəm)\n",
        "            return len(text) == 12 and re.match(r'^AZE\\d{9}$', text)\n",
        "        else:\n",
        "            # Digər seriya nömrələri üçün ümumi qayda\n",
        "            return 8 <= len(text) <= 12 and re.match(r'^[A-Z]{2,3}\\d{6,9}$', text)\n",
        "\n",
        "    elif entity_type == 'PLATE':\n",
        "        # Avtomobil nömrəsi XX-YY-ZZZ formatında\n",
        "        return len(text) == 9 and re.match(r'^\\d{2}-[A-Z]{2}-\\d{3}$', text)\n",
        "\n",
        "    return False\n",
        "\n",
        "def show_validation_rules():\n",
        "    \"\"\"Format qaydalarını göstərir\"\"\"\n",
        "    print(\"📏 Entity Format Qaydaları:\")\n",
        "    print(\"🔹 FIN kod: 7 simvol (hərf+rəqəm) - məsələn: AZEDF12\")\n",
        "    print(\"🔹 ID/AA: 9 simvol (AA + 7 rəqəm) - məsələn: AA1234567\")\n",
        "    print(\"🔹 ID/AZE: 12 simvol (AZE + 9 rəqəm) - məsələn: AZE123456789\")\n",
        "    print(\"🔹 Avtomobil: XX-YY-ZZZ formatı - məsələn: 90-AB-123\")\n",
        "\n",
        "show_validation_rules()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiHQQQSnR3PF",
        "outputId": "c0e53640-0925-4769-8d59-7e46adba2181"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Dataset yükləmə funksiyaları hazırlandı\n"
          ]
        }
      ],
      "source": [
        "def convert_spacy_to_bio(text: str, entities: list) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    spaCy formatından BIO formatına çevirmə\n",
        "    \"\"\"\n",
        "    tokens = text.split()\n",
        "    labels = ['O'] * len(tokens)\n",
        "\n",
        "    # Token-ların mövqelərini hesablayırıq\n",
        "    token_positions = []\n",
        "    current_pos = 0\n",
        "\n",
        "    for token in tokens:\n",
        "        start_pos = text.find(token, current_pos)\n",
        "        end_pos = start_pos + len(token)\n",
        "        token_positions.append((start_pos, end_pos))\n",
        "        current_pos = end_pos\n",
        "\n",
        "    # Entity-ləri BIO formatına çeviririk\n",
        "    for start_char, end_char, entity_type in entities:\n",
        "        # Entity tipini bizim formatımıza uyğunlaşdırırıq\n",
        "        if entity_type == \"CAR_PLATE\":\n",
        "            bio_label = \"PLATE\"\n",
        "        elif entity_type == \"FIN_CODE\":\n",
        "            bio_label = \"FIN\"\n",
        "        elif entity_type == \"ID_NUMBER\":\n",
        "            bio_label = \"ID\"\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        # Hansı token-ların entity-yə aid olduğunu tapırıq\n",
        "        entity_tokens = []\n",
        "        for i, (token_start, token_end) in enumerate(token_positions):\n",
        "            if token_start < end_char and token_end > start_char:\n",
        "                entity_tokens.append(i)\n",
        "\n",
        "        # BIO etiketləri təyin edirik\n",
        "        for i, token_idx in enumerate(entity_tokens):\n",
        "            if i == 0:\n",
        "                labels[token_idx] = f\"B-{bio_label}\"\n",
        "            else:\n",
        "                labels[token_idx] = f\"I-{bio_label}\"\n",
        "\n",
        "    return tokens, labels\n",
        "\n",
        "def load_spacy_json_from_path(file_path: str):\n",
        "    \"\"\"\n",
        "    Verilən fayl yolundan spaCy formatında JSON dataseti yükləyir\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        texts = []\n",
        "        labels = []\n",
        "\n",
        "        for item in data:\n",
        "            text = item[0]  # Cümlə\n",
        "            annotations = item[1]  # Annotasiyalar\n",
        "            entities = annotations.get('entities', [])\n",
        "\n",
        "            tokens, bio_labels = convert_spacy_to_bio(text, entities)\n",
        "            texts.append(tokens)\n",
        "            labels.append(bio_labels)\n",
        "\n",
        "        print(f\"✅ {file_path} uğurla yükləndi: {len(texts)} nümunə\")\n",
        "\n",
        "        return texts, labels\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ Fayl tapılmadı: {file_path}\")\n",
        "        return [], []\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Xəta baş verdi {file_path}: {e}\")\n",
        "        return [], []\n",
        "\n",
        "def load_datasets_from_paths(file_paths: List[str]):\n",
        "    \"\"\"\n",
        "    Verilən fayl yollarından datasetləri yükləyir\n",
        "    \"\"\"\n",
        "    datasets = []\n",
        "\n",
        "    for i, file_path in enumerate(file_paths, 1):\n",
        "        print(f\"\\n📁 Dataset {i} yüklənir: {file_path}\")\n",
        "        texts, labels = load_spacy_json_from_path(file_path)\n",
        "\n",
        "        if texts:\n",
        "            datasets.append((texts, labels))\n",
        "            print(f\"✅ Dataset {i} yükləndi: {len(texts)} nümunə\")\n",
        "        else:\n",
        "            print(f\"❌ Dataset {i} yüklənmədi\")\n",
        "\n",
        "    return datasets\n",
        "\n",
        "def combine_datasets(datasets: List[Tuple[List[List[str]], List[List[str]]]]):\n",
        "    \"\"\"\n",
        "    Datasetləri birləşdirir\n",
        "    \"\"\"\n",
        "    all_texts = []\n",
        "    all_labels = []\n",
        "\n",
        "    for i, (texts, labels) in enumerate(datasets, 1):\n",
        "        print(f\"Dataset {i}: {len(texts)} nümunə\")\n",
        "        all_texts.extend(texts)\n",
        "        all_labels.extend(labels)\n",
        "\n",
        "    print(f\"📊 Ümumi: {len(all_texts)} nümunə\")\n",
        "    return all_texts, all_labels\n",
        "\n",
        "print(\"✅ Dataset yükləmə funksiyaları hazırlandı\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNMViva_R6lZ",
        "outputId": "df9b01bc-08df-42ad-c7e9-1f013f2afe40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Dataset analiz funksiyaları hazırlandı\n"
          ]
        }
      ],
      "source": [
        "def analyze_dataset(texts: List[List[str]], labels: List[List[str]]):\n",
        "    \"\"\"\n",
        "    Dataset analizi və statistika\n",
        "    \"\"\"\n",
        "    label_counts = {}\n",
        "    total_tokens = 0\n",
        "    entity_examples = {}\n",
        "\n",
        "    for text_tokens, label_seq in zip(texts, labels):\n",
        "        total_tokens += len(label_seq)\n",
        "\n",
        "        # Entity nümunələrini topla\n",
        "        i = 0\n",
        "        while i < len(text_tokens):\n",
        "            label = label_seq[i] if i < len(label_seq) else 'O'\n",
        "\n",
        "            if label.startswith('B-'):\n",
        "                entity_type = label[2:]\n",
        "                entity_tokens = [text_tokens[i]]\n",
        "                j = i + 1\n",
        "\n",
        "                while (j < len(text_tokens) and\n",
        "                       j < len(label_seq) and\n",
        "                       label_seq[j] == f'I-{entity_type}'):\n",
        "                    entity_tokens.append(text_tokens[j])\n",
        "                    j += 1\n",
        "\n",
        "                entity_text = ''.join(entity_tokens)\n",
        "                if entity_type not in entity_examples:\n",
        "                    entity_examples[entity_type] = []\n",
        "                if len(entity_examples[entity_type]) < 3:\n",
        "                    entity_examples[entity_type].append(entity_text)\n",
        "                i = j\n",
        "            else:\n",
        "                i += 1\n",
        "\n",
        "        # Label sayını hesabla\n",
        "        for label in label_seq:\n",
        "            label_counts[label] = label_counts.get(label, 0) + 1\n",
        "\n",
        "    print(\"\\n📈 Dataset Statistikası:\")\n",
        "    print(f\"Ümumi cümlə sayı: {len(texts)}\")\n",
        "    print(f\"Ümumi token sayı: {total_tokens}\")\n",
        "    print(\"\\n🏷️ Label paylanması:\")\n",
        "\n",
        "    for label, count in sorted(label_counts.items()):\n",
        "        percentage = (count / total_tokens) * 100\n",
        "        print(f\"{label}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    print(\"\\n📝 Entity nümunələri:\")\n",
        "    for entity_type, examples in entity_examples.items():\n",
        "        print(f\"{entity_type}: {', '.join(examples[:3])}\")\n",
        "\n",
        "def validate_dataset_entities(texts: List[List[str]], labels: List[List[str]]):\n",
        "    \"\"\"\n",
        "    Dataset-də olan entity-ləri validation qaydalarına görə yoxlayır\n",
        "    \"\"\"\n",
        "    valid_entities = {'FIN': 0, 'ID': 0, 'PLATE': 0}\n",
        "    invalid_entities = {'FIN': 0, 'ID': 0, 'PLATE': 0}\n",
        "\n",
        "    for text_tokens, label_seq in zip(texts, labels):\n",
        "        i = 0\n",
        "        while i < len(text_tokens):\n",
        "            label = label_seq[i] if i < len(label_seq) else 'O'\n",
        "\n",
        "            if label.startswith('B-'):\n",
        "                entity_type = label[2:]\n",
        "                entity_tokens = [text_tokens[i]]\n",
        "                j = i + 1\n",
        "\n",
        "                while (j < len(text_tokens) and\n",
        "                       j < len(label_seq) and\n",
        "                       label_seq[j] == f'I-{entity_type}'):\n",
        "                    entity_tokens.append(text_tokens[j])\n",
        "                    j += 1\n",
        "\n",
        "                entity_text = ''.join(entity_tokens)\n",
        "\n",
        "                if validate_entity_format(entity_text, entity_type):\n",
        "                    valid_entities[entity_type] += 1\n",
        "                else:\n",
        "                    invalid_entities[entity_type] += 1\n",
        "\n",
        "                i = j\n",
        "            else:\n",
        "                i += 1\n",
        "\n",
        "    print(\"\\n✅ Validation Nəticələri:\")\n",
        "    for entity_type in valid_entities:\n",
        "        total = valid_entities[entity_type] + invalid_entities[entity_type]\n",
        "        if total > 0:\n",
        "            valid_percent = (valid_entities[entity_type] / total) * 100\n",
        "            print(f\"{entity_type}: {valid_entities[entity_type]}/{total} valid ({valid_percent:.1f}%)\")\n",
        "\n",
        "print(\"✅ Dataset analiz funksiyaları hazırlandı\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLsLl2t3R_km",
        "outputId": "a24f438d-1a4c-493b-d26e-5b87b7f52cc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📂 Datasetlər yüklənir...\n",
            "Fayl yolları:\n",
            "  1. /content/car_plate_dataset_annotated.json\n",
            "  2. /content/fin_annotated.json\n",
            "  3. /content/vesiqe_annotated.json\n",
            "\n",
            "📁 Dataset 1 yüklənir: /content/car_plate_dataset_annotated.json\n",
            "✅ /content/car_plate_dataset_annotated.json uğurla yükləndi: 10000 nümunə\n",
            "✅ Dataset 1 yükləndi: 10000 nümunə\n",
            "\n",
            "📁 Dataset 2 yüklənir: /content/fin_annotated.json\n",
            "✅ /content/fin_annotated.json uğurla yükləndi: 10000 nümunə\n",
            "✅ Dataset 2 yükləndi: 10000 nümunə\n",
            "\n",
            "📁 Dataset 3 yüklənir: /content/vesiqe_annotated.json\n",
            "✅ /content/vesiqe_annotated.json uğurla yükləndi: 10000 nümunə\n",
            "✅ Dataset 3 yükləndi: 10000 nümunə\n",
            "Dataset 1: 10000 nümunə\n",
            "Dataset 2: 10000 nümunə\n",
            "Dataset 3: 10000 nümunə\n",
            "📊 Ümumi: 30000 nümunə\n",
            "\n",
            "📈 Dataset Statistikası:\n",
            "Ümumi cümlə sayı: 30000\n",
            "Ümumi token sayı: 192258\n",
            "\n",
            "🏷️ Label paylanması:\n",
            "B-FIN: 10000 (5.2%)\n",
            "B-ID: 10000 (5.2%)\n",
            "B-PLATE: 10000 (5.2%)\n",
            "O: 162258 (84.4%)\n",
            "\n",
            "📝 Entity nümunələri:\n",
            "PLATE: 85-NL-695, 95-MA-819, 54-GY-552\n",
            "FIN: BML6CZ9, ZWUH9EL, U9H9PXW\n",
            "ID: AZE017114779, AZE879177352, AA3042819\n",
            "\n",
            "✅ Validation Nəticələri:\n",
            "FIN: 8928/10000 valid (89.3%)\n",
            "ID: 8850/10000 valid (88.5%)\n",
            "PLATE: 9608/10000 valid (96.1%)\n",
            "\n",
            "📋 Data Split:\n",
            "Train: 24000 nümunə\n",
            "Validation: 6000 nümunə\n",
            "✅ Datasetlər uğurla hazırlandı!\n"
          ]
        }
      ],
      "source": [
        "# SİZ BU HİSSƏNİ DƏYİŞDİRİN - ÖZ FAYL YOLLARINIZI VERİN\n",
        "dataset_paths = [\n",
        "    \"/content/car_plate_dataset_annotated.json\",    # 1-ci dataset\n",
        "    \"/content/fin_annotated.json\",                  # 2-ci dataset\n",
        "    \"/content/vesiqe_annotated.json\"                # 3-cü dataset\n",
        "]\n",
        "\n",
        "print(\"📂 Datasetlər yüklənir...\")\n",
        "print(\"Fayl yolları:\")\n",
        "for i, path in enumerate(dataset_paths, 1):\n",
        "    print(f\"  {i}. {path}\")\n",
        "\n",
        "# Datasetləri yüklə\n",
        "datasets = load_datasets_from_paths(dataset_paths)\n",
        "\n",
        "if datasets:\n",
        "    # Datasetləri birləşdir\n",
        "    all_texts, all_labels = combine_datasets(datasets)\n",
        "\n",
        "    # Dataset analizini et\n",
        "    analyze_dataset(all_texts, all_labels)\n",
        "\n",
        "    # Validation yoxlaması\n",
        "    validate_dataset_entities(all_texts, all_labels)\n",
        "\n",
        "    # Train-validation split\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "        all_texts, all_labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"\\n📋 Data Split:\")\n",
        "    print(f\"Train: {len(train_texts)} nümunə\")\n",
        "    print(f\"Validation: {len(val_texts)} nümunə\")\n",
        "\n",
        "    print(\"✅ Datasetlər uğurla hazırlandı!\")\n",
        "else:\n",
        "    print(\"❌ Heç bir dataset yüklənmədi!\")\n",
        "    print(\"Fayl yollarını yoxlayın və yenidən cəhd edin.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181,
          "referenced_widgets": [
            "5562f2ef16344cadb184f93a065d2696",
            "55d11845ff5c4e0dba39a6b7ae554649",
            "170b73c382ba4a8ca2432526f4dc5952",
            "c2152a9a197c4ede8bf10fa5c136804e",
            "8530b259f40048d9a52187268a9fecd8",
            "c1e5f4385ff14b5f88152efe143e6787",
            "59101551208b4d3b93352c7caa8ce6f9",
            "e9d9d8b49b474530a69c42411c080882",
            "a8dcc80c961a42e7b363dec816004d38",
            "09708c7c0b9e4cd0a4ef183bf30a0be7",
            "d6eabdfe77854b72bb87f9a125a06bfb",
            "a4b67e734e2b4f0ebec95a1dbd754f15",
            "0eb22280c7ce4774bc2c63326c3ca7e4",
            "0af231b8b0ca4c4289f51f7d95b3349c",
            "d4f58c773d604547958cab90c6ce7379",
            "b608c9954d7e4978ade0093665813810",
            "0f85083e933d43728b822086a57f5fb3",
            "86ac301200f44445b3ea6d8aaa4bb5cc",
            "0c94d80308e641feae3ac5c7b8133e51",
            "a7ef542a5d47478a8e1d830c21d0df87",
            "8f0ab0d1e5bd4e278014fdfbd3f8e003",
            "bc974ee6e9914c66b2b380e5a816ae07",
            "6f7a7327d84746338f15535d8d7aaee9",
            "5884c1378b0844378c21e4cd5d75ba43",
            "34c697bc1fb94a33a80b4ca93e2b42fd",
            "4c70f338b98f49c993345353042273d5",
            "b3d64369119c420592531c1bc034858a",
            "457f04538b9b422cafa050ef74417236",
            "07a68a9621434e579662f209cbdf07f3",
            "42d9711f885e4cf19ffdd1d482185877",
            "42ae2f211273449db35154e4209446aa",
            "d85653158cff4046aeca106889e1671c",
            "05019c7d08904109a4e37e46004de499",
            "0073c7fa482e40bdb2f05a6cd11d7bb2",
            "c666c6e4e96e4e8e922344777c6458f9",
            "ec21d6f181cd461bb64348ccdac73b85",
            "2ce2482153854279a2694a37efe8a6a9",
            "e939d9baf53640e79a62f4582d2bd286",
            "c74af87461c44618bb6f359911997633",
            "5adb7debbc4e4ac5a1f95a1b5c4382b2",
            "e1279f3b20fd47caaf9d76a746d69618",
            "8351bdb2ab704ed1a6f4da3be128d886",
            "8df9ce03fa814103b5b0775f7ab8ff9f",
            "9f7a666fa28840e9964bb9fe7d2d56f0"
          ]
        },
        "id": "E65uuHX3SIoK",
        "outputId": "7e1e0087-96cd-4a3f-f445-71f99d139cd0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5562f2ef16344cadb184f93a065d2696",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4b67e734e2b4f0ebec95a1dbd754f15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f7a7327d84746338f15535d8d7aaee9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0073c7fa482e40bdb2f05a6cd11d7bb2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Tokenizer yükləndi: bert-base-multilingual-cased\n",
            "✅ NERDataset sinifi hazırlandı\n"
          ]
        }
      ],
      "source": [
        "# Tokenizer və model yükləmə\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "print(f\"✅ Tokenizer yükləndi: {MODEL_NAME}\")\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, texts: List[List[str]], labels: List[List[str]], tokenizer, max_len: int):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        labels = self.labels[idx]\n",
        "\n",
        "        # Tokenləşdirmə\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            is_split_into_words=True,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Label-ları token-lara uyğunlaşdırma\n",
        "        word_ids = encoding.word_ids()\n",
        "        label_ids = []\n",
        "\n",
        "        for word_id in word_ids:\n",
        "            if word_id is None:\n",
        "                label_ids.append(-100)  # Special tokens\n",
        "            elif word_id < len(labels):\n",
        "                label_ids.append(label2id[labels[word_id]])\n",
        "            else:\n",
        "                label_ids.append(label2id['O'])\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label_ids, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "print(\"✅ NERDataset sinifi hazırlandı\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247,
          "referenced_widgets": [
            "88aa1838f579462bb7598de5e2f49a23",
            "c9beb6c2396943dc94fa97dfd7ede179",
            "cd63c5d7c2e248289314b33d78ac49e0",
            "b7895f9e0f2d419db600a68f28ca5e03",
            "61b65922d9174c66a09fe4b34f8a87b5",
            "6f6be16bcb5e46478a63593c0525eccf",
            "7eb8732f59704972bae589db6cc19bf8",
            "df20009ea2ef4a949455091f458205f6",
            "786cac18f08443cd8cd260bf17cd5551",
            "aeb8a472c3424ee2bdbd53131eabc84f",
            "3c42cffc80214d1aa57054ec2dca733d"
          ]
        },
        "id": "BkfUbs4aSLWz",
        "outputId": "22b7df97-f25b-468e-9eef-67deec00c676"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88aa1838f579462bb7598de5e2f49a23",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Model yükləndi: bert-base-multilingual-cased\n",
            "📊 Label sayı: 7\n",
            "Max steps: 300\n",
            "Warmup steps: 50\n",
            "Logging every: 25 steps\n",
            "Evaluation every: 50 steps\n",
            "Save every: 50 steps\n",
            "✅ Training konfiqurasiyası hazırlandı\n"
          ]
        }
      ],
      "source": [
        "# Model yüklə\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(LABELS),\n",
        "    label2id=label2id,\n",
        "    id2label=id2label\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "print(f\"✅ Model yükləndi: {MODEL_NAME}\")\n",
        "print(f\"📊 Label sayı: {len(LABELS)}\")\n",
        "\n",
        "# Metrics hesablama funksiyası\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Training zamanı metriklər hesablamaq üçün\n",
        "    \"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # -100 olan labelları çıxar (special tokens)\n",
        "    true_predictions = [\n",
        "        [id2label[p] for p, l in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [id2label[l] for p, l in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    # Flatten etmək\n",
        "    flat_true_labels = [label for sublist in true_labels for label in sublist]\n",
        "    flat_predictions = [pred for sublist in true_predictions for pred in sublist]\n",
        "\n",
        "    # Metrics hesablamaq\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        flat_true_labels, flat_predictions, average='weighted', zero_division=0\n",
        "    )\n",
        "    accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Training argumentləri - 300 step üçün\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    max_steps=300,                    # 300 step məhdudiyyəti\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    warmup_steps=50,                  # Daha az warmup (300 step-in 1/6-sı)\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=25,                 # Daha tez-tez log (300/12)\n",
        "    eval_strategy=\"steps\",      # eval_strategy əvəzinə evaluation_strategy\n",
        "    eval_steps=50,                    # Daha tez-tez eval (300/6)\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,                    # Daha tez-tez save\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    report_to=None,\n",
        "    learning_rate=LEARNING_RATE,\n",
        ")\n",
        "\n",
        "print(f\"Max steps: 300\")\n",
        "print(f\"Warmup steps: 50\")\n",
        "print(f\"Logging every: 25 steps\")\n",
        "print(f\"Evaluation every: 50 steps\")\n",
        "print(f\"Save every: 50 steps\")\n",
        "\n",
        "print(\"✅ Training konfiqurasiyası hazırlandı\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "id": "2ohdYzfbSMsp",
        "outputId": "1b4c5827-1644-4711-f241-dc6a68743e71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Train dataset: 24000 nümunə\n",
            "✅ Validation dataset: 6000 nümunə\n",
            "✅ Trainer hazırlandı\n",
            "\n",
            "🚀 Training başlayır...\n",
            "📊 Total training samples: 24000\n",
            "📊 Total validation samples: 6000\n",
            "🔥 Device: cuda\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbahramzada\u001b[0m (\u001b[33mbahramzada-unec-business-school\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "creating run (0.0s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250910_204936-jrib0ssv</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bahramzada-unec-business-school/huggingface/runs/jrib0ssv' target=\"_blank\">pleasant-energy-20</a></strong> to <a href='https://wandb.ai/bahramzada-unec-business-school/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/bahramzada-unec-business-school/huggingface' target=\"_blank\">https://wandb.ai/bahramzada-unec-business-school/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/bahramzada-unec-business-school/huggingface/runs/jrib0ssv' target=\"_blank\">https://wandb.ai/bahramzada-unec-business-school/huggingface/runs/jrib0ssv</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [300/300 09:56, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.159500</td>\n",
              "      <td>0.005118</td>\n",
              "      <td>0.999606</td>\n",
              "      <td>0.999606</td>\n",
              "      <td>0.999607</td>\n",
              "      <td>0.999606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.000624</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.000402</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.000478</td>\n",
              "      <td>0.999964</td>\n",
              "      <td>0.999964</td>\n",
              "      <td>0.999964</td>\n",
              "      <td>0.999964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.000307</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.000290</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Training tamamlandı!\n",
            "\n",
            "📊 Model qiymətləndirilir...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [375/375 00:43]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 Evaluation nəticələri:\n",
            "eval_loss: 0.0006\n",
            "eval_accuracy: 1.0000\n",
            "eval_f1: 1.0000\n",
            "eval_precision: 1.0000\n",
            "eval_recall: 1.0000\n",
            "eval_runtime: 45.0148\n",
            "eval_samples_per_second: 133.2890\n",
            "eval_steps_per_second: 8.3310\n",
            "epoch: 0.2000\n",
            "✅ Model ./best_model qovluğunda saxlanıldı\n"
          ]
        }
      ],
      "source": [
        "# Dataset obyektlərini yarat (əgər datasetlər yüklənmişsə)\n",
        "if 'train_texts' in locals() and len(train_texts) > 0:\n",
        "    train_dataset = NERDataset(\n",
        "        texts=train_texts,\n",
        "        labels=train_labels,\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=MAX_LEN\n",
        "    )\n",
        "\n",
        "    val_dataset = NERDataset(\n",
        "        texts=val_texts,\n",
        "        labels=val_labels,\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=MAX_LEN\n",
        "    )\n",
        "\n",
        "    # Data collator\n",
        "    data_collator = DataCollatorForTokenClassification(\n",
        "        tokenizer=tokenizer,\n",
        "        padding=True,\n",
        "        max_length=MAX_LEN,\n",
        "        pad_to_multiple_of=None,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Trainer obyektini yarat\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    print(f\"✅ Train dataset: {len(train_dataset)} nümunə\")\n",
        "    print(f\"✅ Validation dataset: {len(val_dataset)} nümunə\")\n",
        "    print(\"✅ Trainer hazırlandı\")\n",
        "\n",
        "    # Training başlat\n",
        "    print(\"\\n🚀 Training başlayır...\")\n",
        "    print(f\"📊 Total training samples: {len(train_dataset)}\")\n",
        "    print(f\"📊 Total validation samples: {len(val_dataset)}\")\n",
        "    print(f\"🔥 Device: {device}\")\n",
        "\n",
        "    try:\n",
        "        trainer.train()\n",
        "        print(\"✅ Training tamamlandı!\")\n",
        "\n",
        "        # Model qiymətləndirmə\n",
        "        print(\"\\n📊 Model qiymətləndirilir...\")\n",
        "        eval_results = trainer.evaluate()\n",
        "\n",
        "        print(\"🎯 Evaluation nəticələri:\")\n",
        "        for key, value in eval_results.items():\n",
        "            if isinstance(value, float):\n",
        "                print(f\"{key}: {value:.4f}\")\n",
        "            else:\n",
        "                print(f\"{key}: {value}\")\n",
        "\n",
        "        # En yaxşı modeli saxla\n",
        "        trainer.save_model(\"./best_model\")\n",
        "        tokenizer.save_pretrained(\"./best_model\")\n",
        "        print(\"✅ Model ./best_model qovluğunda saxlanıldı\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Training zamanı xəta: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ Datasetlər hazırlanmadı, training edilə bilməz!\")\n",
        "    print(\"Əvvəlki hissələrdə datasetləri düzgün yükləyin.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFcRr3miSTDr",
        "outputId": "7f4bfc5a-eac3-48e0-8e7b-b0578c13d6a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Təkmilləşdirilmiş inference funksiyaları hazırlandı\n"
          ]
        }
      ],
      "source": [
        "def predict_entities_with_model(text: str, model, tokenizer, device):\n",
        "    \"\"\"\n",
        "    Modeli istifadə edərək entity-ləri predict edir\n",
        "    \"\"\"\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Tokenizer ilə encode et\n",
        "    inputs = tokenizer(\n",
        "        tokens,\n",
        "        is_split_into_words=True,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=MAX_LEN\n",
        "    )\n",
        "\n",
        "    # GPU-ya göndər\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Model prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=2)\n",
        "\n",
        "    # Word IDs ilə token-word mapping\n",
        "    word_ids = inputs.word_ids() if hasattr(inputs, 'word_ids') else None\n",
        "    predicted_labels = []\n",
        "\n",
        "    if word_ids is None:\n",
        "        # Manual word mapping\n",
        "        tokenized = tokenizer.tokenize(' '.join(tokens))\n",
        "        pred_idx = 1  # CLS token-dan sonra başla\n",
        "\n",
        "        for token in tokens:\n",
        "            if pred_idx < len(predictions[0]):\n",
        "                label_id = predictions[0][pred_idx].item()\n",
        "                if label_id < len(id2label):\n",
        "                    predicted_labels.append(id2label[label_id])\n",
        "                else:\n",
        "                    predicted_labels.append('O')\n",
        "\n",
        "                # Token neçə subword-ə bölünüb onu hesabla\n",
        "                token_subwords = tokenizer.tokenize(token)\n",
        "                pred_idx += len(token_subwords)\n",
        "            else:\n",
        "                predicted_labels.append('O')\n",
        "    else:\n",
        "        # Word IDs istifadə et\n",
        "        for i in range(len(tokens)):\n",
        "            word_positions = [j for j, wid in enumerate(word_ids[0]) if wid == i]\n",
        "            if word_positions:\n",
        "                label_id = predictions[0][word_positions[0]].item()\n",
        "                if label_id < len(id2label):\n",
        "                    predicted_labels.append(id2label[label_id])\n",
        "                else:\n",
        "                    predicted_labels.append('O')\n",
        "            else:\n",
        "                predicted_labels.append('O')\n",
        "\n",
        "    # Label sayını düzəlt\n",
        "    while len(predicted_labels) < len(tokens):\n",
        "        predicted_labels.append('O')\n",
        "    predicted_labels = predicted_labels[:len(tokens)]\n",
        "\n",
        "    return tokens, predicted_labels\n",
        "\n",
        "def enhanced_predict_with_validation(text: str, model, tokenizer, device):\n",
        "    \"\"\"\n",
        "    Validation qaydaları ilə təkmilləşdirilmiş entity prediction\n",
        "    \"\"\"\n",
        "    tokens, predicted_labels = predict_entities_with_model(text, model, tokenizer, device)\n",
        "\n",
        "    validated_labels = []\n",
        "    i = 0\n",
        "\n",
        "    while i < len(tokens):\n",
        "        current_label = predicted_labels[i] if i < len(predicted_labels) else 'O'\n",
        "\n",
        "        if current_label.startswith('B-'):\n",
        "            entity_type = current_label[2:]\n",
        "            entity_tokens = [tokens[i]]\n",
        "\n",
        "            # Entity-nin tam mətnini topla\n",
        "            j = i + 1\n",
        "            while (j < len(tokens) and\n",
        "                   j < len(predicted_labels) and\n",
        "                   predicted_labels[j] == f'I-{entity_type}'):\n",
        "                entity_tokens.append(tokens[j])\n",
        "                j += 1\n",
        "\n",
        "            entity_text = ''.join(entity_tokens)\n",
        "\n",
        "            # Format validation\n",
        "            if validate_entity_format(entity_text, entity_type):\n",
        "                # Valid entity - saxla\n",
        "                validated_labels.append(current_label)\n",
        "                for k in range(i + 1, j):\n",
        "                    if k < len(predicted_labels):\n",
        "                        validated_labels.append(predicted_labels[k])\n",
        "                    else:\n",
        "                        validated_labels.append('O')\n",
        "            else:\n",
        "                # Invalid entity - O etiketi ver\n",
        "                for k in range(i, j):\n",
        "                    validated_labels.append('O')\n",
        "\n",
        "            i = j\n",
        "        else:\n",
        "            validated_labels.append(current_label)\n",
        "            i += 1\n",
        "\n",
        "    # Label sayını düzəlt\n",
        "    while len(validated_labels) < len(tokens):\n",
        "        validated_labels.append('O')\n",
        "    validated_labels = validated_labels[:len(tokens)]\n",
        "\n",
        "    return tokens, validated_labels\n",
        "\n",
        "def blur_with_model_and_validation(text: str, model, tokenizer, device):\n",
        "    \"\"\"\n",
        "    Model + validation ilə blurring edir\n",
        "    \"\"\"\n",
        "    tokens, validated_labels = enhanced_predict_with_validation(text, model, tokenizer, device)\n",
        "\n",
        "    blurred_tokens = []\n",
        "    entities_found = []\n",
        "    i = 0\n",
        "\n",
        "    while i < len(tokens):\n",
        "        current_label = validated_labels[i] if i < len(validated_labels) else 'O'\n",
        "\n",
        "        if current_label.startswith('B-'):\n",
        "            entity_type = current_label[2:]\n",
        "            entity_tokens = [tokens[i]]\n",
        "            entity_start = i\n",
        "\n",
        "            # I- etiketli davamını tap\n",
        "            j = i + 1\n",
        "            while (j < len(tokens) and\n",
        "                   j < len(validated_labels) and\n",
        "                   validated_labels[j] == f'I-{entity_type}'):\n",
        "                entity_tokens.append(tokens[j])\n",
        "                j += 1\n",
        "\n",
        "            entity_text = ''.join(entity_tokens)\n",
        "\n",
        "            entities_found.append({\n",
        "                'text': entity_text,\n",
        "                'type': entity_type,\n",
        "                'start': entity_start,\n",
        "                'end': j-1,\n",
        "                'tokens': entity_tokens\n",
        "            })\n",
        "\n",
        "            blurred_tokens.append('[BLURRED]')\n",
        "            i = j\n",
        "        else:\n",
        "            blurred_tokens.append(tokens[i])\n",
        "            i += 1\n",
        "\n",
        "    return ' '.join(blurred_tokens), entities_found\n",
        "\n",
        "print(\"✅ Təkmilləşdirilmiş inference funksiyaları hazırlandı\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujVNFXhHSaVe",
        "outputId": "3ff4dfb9-ec9b-45f9-927d-688f7b3647f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 Model test edilir...\n",
            "🧪 Model + Validation Test...\n",
            "\n",
            "📝 Test 1:\n",
            "Orijinal: Mənim fin kodum AZEDF12 olan kartım var\n",
            "Blurred:  Mənim fin kodum [BLURRED] olan kartım var\n",
            "✅ Tapılan valid entity-lər:\n",
            "  ✓ Valid - 'AZEDF12' (FIN)\n",
            "----------------------------------------------------------------------\n",
            "📝 Test 2:\n",
            "Orijinal: FIN kod AB12345 mövcuddur\n",
            "Blurred:  FIN kod AB12345 mövcuddur\n",
            "❌ Heç bir valid entity tapılmadı\n",
            "----------------------------------------------------------------------\n",
            "📝 Test 3:\n",
            "Orijinal: Bu 90-AB-123 nömrəli avtomobil dostumundur\n",
            "Blurred:  Bu [BLURRED] nömrəli avtomobil dostumundur\n",
            "✅ Tapılan valid entity-lər:\n",
            "  ✓ Valid - '90-AB-123' (PLATE)\n",
            "----------------------------------------------------------------------\n",
            "📝 Test 4:\n",
            "Orijinal: Şəxsiyyət vəsiqə AA1234567 dir\n",
            "Blurred:  Şəxsiyyət vəsiqə [BLURRED] dir\n",
            "✅ Tapılan valid entity-lər:\n",
            "  ✓ Valid - 'AA1234567' (ID)\n",
            "----------------------------------------------------------------------\n",
            "📝 Test 5:\n",
            "Orijinal: Sənəd nömrəsi AZE123456789 təqdim edilməlidir\n",
            "Blurred:  Sənəd nömrəsi [BLURRED] təqdim edilməlidir\n",
            "✅ Tapılan valid entity-lər:\n",
            "  ✓ Valid - 'AZE123456789' (ID)\n",
            "----------------------------------------------------------------------\n",
            "📝 Test 6:\n",
            "Orijinal: Səhv format AA12345 və ya AZE12345 var\n",
            "Blurred:  Səhv format AA12345 və ya AZE12345 var\n",
            "❌ Heç bir valid entity tapılmadı\n",
            "----------------------------------------------------------------------\n",
            "📝 Test 7:\n",
            "Orijinal: Bu adi cümlə dir heç nə yoxdur\n",
            "Blurred:  Bu adi cümlə dir heç nə yoxdur\n",
            "❌ Heç bir valid entity tapılmadı\n",
            "----------------------------------------------------------------------\n",
            "📝 Test 8:\n",
            "Orijinal: FİN MM4NS3L və maşın 77-KM-596 qeydiyyatı var\n",
            "Blurred:  FİN [BLURRED] və maşın [BLURRED] qeydiyyatı var\n",
            "✅ Tapılan valid entity-lər:\n",
            "  ✓ Valid - 'MM4NS3L' (FIN)\n",
            "  ✓ Valid - '77-KM-596' (PLATE)\n",
            "----------------------------------------------------------------------\n",
            "📝 Test 9:\n",
            "Orijinal: Qeydiyyat üçün AZE987654321 və 10-AB-123 lazım\n",
            "Blurred:  Qeydiyyat üçün [BLURRED] və 10-AB-123 lazım\n",
            "✅ Tapılan valid entity-lər:\n",
            "  ✓ Valid - 'AZE987654321' (ID)\n",
            "----------------------------------------------------------------------\n",
            "📝 Test 10:\n",
            "Orijinal: Qeydiyyat prosesində şəxsiyyət vəsiqəsi nömrəsi AZE123456789 və FİN kod MM4NS3L təqdim edilməlidir. Avtomobilin nömrəsi 77-KM-596 və 10-AB-123 ilə bağlı məlumatlar da sistemə daxil edilməlidir. Əlavə olaraq, yeni qeydiyyatda tələb olunan ikinci şəxsiyyət vəsiqəsi nömrəsi AZE987654321 və FİN kodu QA9WT2K də yoxlanılacaq. Qeyd: hər hansı bir kod səhv daxil edilərsə, sistem xəbərdarlıq edəcək.\n",
            "Blurred:  Qeydiyyat prosesində şəxsiyyət vəsiqəsi nömrəsi [BLURRED] və FİN kod MM4NS3L təqdim edilməlidir. Avtomobilin nömrəsi [BLURRED] və [BLURRED] ilə bağlı məlumatlar da sistemə daxil edilməlidir. Əlavə olaraq, yeni qeydiyyatda tələb olunan ikinci şəxsiyyət vəsiqəsi nömrəsi [BLURRED] və FİN kodu [BLURRED] də yoxlanılacaq. Qeyd: hər hansı bir kod səhv daxil edilərsə, sistem xəbərdarlıq edəcək.\n",
            "✅ Tapılan valid entity-lər:\n",
            "  ✓ Valid - 'AZE123456789' (ID)\n",
            "  ✓ Valid - '77-KM-596' (PLATE)\n",
            "  ✓ Valid - '10-AB-123' (PLATE)\n",
            "  ✓ Valid - 'AZE987654321' (ID)\n",
            "  ✓ Valid - 'QA9WT2K' (FIN)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "======================================================================\n",
            "İnteraktiv sistemə keçmək istəyirsinizmi? (y/n)\n"
          ]
        }
      ],
      "source": [
        "def test_enhanced_model():\n",
        "    \"\"\"\n",
        "    Validation qaydaları ilə modeli test edir\n",
        "    \"\"\"\n",
        "    test_sentences = [\n",
        "        \"Mənim fin kodum AZEDF12 olan kartım var\",          # Valid FIN (7 simvol)\n",
        "        \"FIN kod AB12345 mövcuddur\",                        # Valid FIN (7 simvol)\n",
        "        \"Bu 90-AB-123 nömrəli avtomobil dostumundur\",       # Valid PLATE\n",
        "        \"Şəxsiyyət vəsiqə AA1234567 dir\",                   # Valid ID (AA + 7 rəqəm)\n",
        "        \"Sənəd nömrəsi AZE123456789 təqdim edilməlidir\",    # Valid ID (AZE + 9 rəqəm)\n",
        "        \"Səhv format AA12345 və ya AZE12345 var\",           # Invalid formats\n",
        "        \"Bu adi cümlə dir heç nə yoxdur\",                   # Heç nə yox\n",
        "        \"FİN MM4NS3L və maşın 77-KM-596 qeydiyyatı var\",   # Valid entities\n",
        "        \"Qeydiyyat üçün AZE987654321 və 10-AB-123 lazım\",   # Multiple valid entities\n",
        "        'Qeydiyyat prosesində şəxsiyyət vəsiqəsi nömrəsi AZE123456789 və FİN kod MM4NS3L təqdim edilməlidir. Avtomobilin nömrəsi 77-KM-596 və 10-AB-123 ilə bağlı məlumatlar da sistemə daxil edilməlidir. Əlavə olaraq, yeni qeydiyyatda tələb olunan ikinci şəxsiyyət vəsiqəsi nömrəsi AZE987654321 və FİN kodu QA9WT2K də yoxlanılacaq. Qeyd: hər hansı bir kod səhv daxil edilərsə, sistem xəbərdarlıq edəcək.' #custom\n",
        "    ]\n",
        "\n",
        "    print(\"🧪 Model + Validation Test...\\n\")\n",
        "\n",
        "    for i, sentence in enumerate(test_sentences, 1):\n",
        "        print(f\"📝 Test {i}:\")\n",
        "        print(f\"Orijinal: {sentence}\")\n",
        "\n",
        "        try:\n",
        "            blurred_text, entities = blur_with_model_and_validation(sentence, model, tokenizer, device)\n",
        "            print(f\"Blurred:  {blurred_text}\")\n",
        "\n",
        "            if entities:\n",
        "                print(\"✅ Tapılan valid entity-lər:\")\n",
        "                for entity in entities:\n",
        "                    validation_result = \"✓ Valid\" if validate_entity_format(entity['text'], entity['type']) else \"✗ Invalid\"\n",
        "                    print(f\"  {validation_result} - '{entity['text']}' ({entity['type']})\")\n",
        "            else:\n",
        "                print(\"❌ Heç bir valid entity tapılmadı\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Xəta: {e}\")\n",
        "\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "def detailed_analysis_demo(text: str):\n",
        "    \"\"\"\n",
        "    Detailed token-level analiz\n",
        "    \"\"\"\n",
        "    print(f\"\\n🔍 Detailed Analiz: '{text}'\")\n",
        "\n",
        "    try:\n",
        "        tokens, model_labels = predict_entities_with_model(text, model, tokenizer, device)\n",
        "        _, validated_labels = enhanced_predict_with_validation(text, model, tokenizer, device)\n",
        "\n",
        "        print(\"\\nToken-by-token analiz:\")\n",
        "        print(\"Token\".ljust(15) + \"Model\".ljust(10) + \"Validated\".ljust(12) + \"Valid?\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for i, token in enumerate(tokens):\n",
        "            model_label = model_labels[i] if i < len(model_labels) else 'O'\n",
        "            validated_label = validated_labels[i] if i < len(validated_labels) else 'O'\n",
        "\n",
        "            # Entity-nin validation statusunu yoxla\n",
        "            if model_label.startswith('B-'):\n",
        "                entity_type = model_label[2:]\n",
        "                # Entity-nin tam mətnini tap\n",
        "                entity_tokens = [token]\n",
        "                j = i + 1\n",
        "                while (j < len(tokens) and\n",
        "                       j < len(model_labels) and\n",
        "                       model_labels[j] == f'I-{entity_type}'):\n",
        "                    entity_tokens.append(tokens[j])\n",
        "                    j += 1\n",
        "                entity_text = ''.join(entity_tokens)\n",
        "                is_valid = validate_entity_format(entity_text, entity_type)\n",
        "                valid_status = \"✓\" if is_valid else \"✗\"\n",
        "            else:\n",
        "                valid_status = \"-\"\n",
        "\n",
        "            print(f\"{token:<15} {model_label:<10} {validated_label:<12} {valid_status}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Analiz xətası: {e}\")\n",
        "\n",
        "def interactive_blur_system():\n",
        "    \"\"\"\n",
        "    İnteraktiv blurring sistemi\n",
        "    \"\"\"\n",
        "    print(\"🤖 Azərbaycan NER Blur Sistemi\")\n",
        "    print(\"Model + Validation qaydaları istifadə edilir\")\n",
        "    print(\"Format qaydaları:\")\n",
        "    print(\"  • FIN: 7 simvol (hərf+rəqəm)\")\n",
        "    print(\"  • ID/AA: 9 simvol (AA + 7 rəqəm)\")\n",
        "    print(\"  • ID/AZE: 12 simvol (AZE + 9 rəqəm)\")\n",
        "    print(\"  • Avtomobil: XX-YY-ZZZ\")\n",
        "    print(\"\\nÇıxmaq üçün 'exit' yazın\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"📝 Mətn daxil edin: \")\n",
        "\n",
        "        if user_input.lower() in ['exit', 'çıx', 'quit', 'q']:\n",
        "            print(\"👋 Görüşənədək!\")\n",
        "            break\n",
        "\n",
        "        if user_input.strip():\n",
        "            try:\n",
        "                blurred_text, entities = blur_with_model_and_validation(user_input, model, tokenizer, device)\n",
        "                print(f\"🔒 Blurred: {blurred_text}\")\n",
        "\n",
        "                if entities:\n",
        "                    print(\"📍 Tapılan entity-lər:\")\n",
        "                    for entity in entities:\n",
        "                        print(f\"  • '{entity['text']}' → {entity['type']}\")\n",
        "                else:\n",
        "                    print(\"📍 Heç bir entity tapılmadı\")\n",
        "\n",
        "                # Detailed analizi göstər\n",
        "                detailed_analysis_demo(user_input)\n",
        "                print()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Xəta: {e}\\n\")\n",
        "        else:\n",
        "            print(\"❌ Boş mətn daxil etdiniz\\n\")\n",
        "\n",
        "# Əgər model öyrədilib və mövcuddursa test et\n",
        "if 'model' in locals():\n",
        "    print(\"🎯 Model test edilir...\")\n",
        "    test_enhanced_model()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"İnteraktiv sistemə keçmək istəyirsinizmi? (y/n)\")\n",
        "    # interactive_blur_system()\n",
        "else:\n",
        "    print(\"❌ Model əvvəlcə öyrədilməlidir!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueDDhD7oVvue",
        "outputId": "4884c3c8-da95-45f1-f61e-7158831078a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "zip error: Nothing to do! (/content/best_model.zip)\n"
          ]
        }
      ],
      "source": [
        "!zip -r /content/best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Okj6QqP5cBh5",
        "outputId": "bc8b4a03-036d-4544-fca2-445691fa92ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/best_model/ (stored 0%)\n",
            "  adding: content/best_model/model.safetensors (deflated 7%)\n",
            "  adding: content/best_model/config.json (deflated 55%)\n",
            "  adding: content/best_model/training_args.bin (deflated 53%)\n",
            "  adding: content/best_model/vocab.txt (deflated 45%)\n",
            "  adding: content/best_model/special_tokens_map.json (deflated 42%)\n",
            "  adding: content/best_model/tokenizer.json (deflated 67%)\n",
            "  adding: content/best_model/tokenizer_config.json (deflated 75%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r /content/best_model.zip /content/best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqXE37AXcN-2",
        "outputId": "f2a229ad-b1f9-47a4-a2d1-c279e474dc37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 659003257 Sep 10 21:04 /content/best_model.zip\n"
          ]
        }
      ],
      "source": [
        "!ls -l /content/best_model.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ua6xAasrccax"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}